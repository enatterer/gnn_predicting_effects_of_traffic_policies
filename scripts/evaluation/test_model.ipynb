{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ABSTRACT -->\n",
    "\n",
    "The goal of this script is to check how well the model performs on the test set. For this, we will look at the overall test set, as well as some specific cases, that we will visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Add the 'scripts' directory to Python Path\n",
    "scripts_path=os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "\n",
    "import evaluation.help_functions as hf\n",
    "import evaluation.plot_functions as pf\n",
    "\n",
    "import gnn.gnn_io as gio\n",
    "from gnn.models.point_net_transf_gat import PointNetTransfGAT\n",
    "from training.help_functions import seed_worker, normalize_x_features_with_scaler\n",
    "from data_preprocessing.help_functions import highway_mapping, get_link_geometries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the absolute path to the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "# Paths, adjust as needed\n",
    "run_path = os.path.join(project_root, \"data\", \"runs_01_2025\", \"wannabe_best_6\")\n",
    "districts = gpd.read_file(os.path.join(project_root, \"data\", \"visualisation\", \"districts_paris.geojson\"))\n",
    "base_case_path = os.path.join(project_root, \"data\", \"links_and_stats\", \"pop_1pct_basecase_average_output_links.geojson\")\n",
    "result_path = 'results/'\n",
    "\n",
    "\n",
    "# GNN Parameters\n",
    "point_net_conv_layer_structure_local_mlp=\"256\"\n",
    "point_net_conv_layer_structure_global_mlp = \"512\"\n",
    "gat_conv_layer_structure = \"128,256,512\"\n",
    "dropout = 0.3\n",
    "use_dropout = False\n",
    "predict_mode_stats = False\n",
    "in_channels = 5\n",
    "out_channels = 1\n",
    "\n",
    "links_base_case = gpd.read_file(base_case_path, crs=\"EPSG:4326\")\n",
    "data_created_during_training = os.path.join(run_path, 'data_created_during_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "### Load test data from the run itself! ###\n",
    "###########################################\n",
    "\n",
    "# Load scalers\n",
    "scaler_x = joblib.load(os.path.join(data_created_during_training, 'test_x_scaler.pkl'))\n",
    "scaler_pos = joblib.load(os.path.join(data_created_during_training, 'test_pos_scaler.pkl'))\n",
    "\n",
    "# Load the test dataset created during training\n",
    "test_set_dl = torch.load(os.path.join(data_created_during_training, 'test_dl.pt'))\n",
    "\n",
    "# Load the DataLoader parameters\n",
    "with open(os.path.join(data_created_during_training, 'test_loader_params.json'), 'r') as f:\n",
    "    test_set_dl_loader_params = json.load(f)\n",
    "    \n",
    "# Remove or correct collate_fn if it is incorrectly specified\n",
    "if 'collate_fn' in test_set_dl_loader_params and isinstance(test_set_dl_loader_params['collate_fn'], str):\n",
    "    del test_set_dl_loader_params['collate_fn']  # Remove it to use the default collate function\n",
    "    \n",
    "test_set_loader = torch.utils.data.DataLoader(test_set_dl, **test_set_dl_loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "### Load separate test data! ###\n",
    "################################\n",
    "\n",
    "dataset_path = os.path.join(project_root, \"data\", \"test_data\", \"pst_roads_in_inner_districts\")\n",
    "\n",
    "datalist = []\n",
    "batch_num = 1\n",
    "while True:\n",
    "    print(f\"Processing batch number: {batch_num}\")\n",
    "    batch_file = os.path.join(dataset_path, f'datalist_batch_{batch_num}.pt')\n",
    "    if not os.path.exists(batch_file):\n",
    "        break\n",
    "    batch_data = torch.load(batch_file, map_location='cpu')\n",
    "    if isinstance(batch_data, list):\n",
    "        datalist.extend(batch_data)\n",
    "    batch_num += 1\n",
    "print(f\"Loaded {len(datalist)} items into datalist\")\n",
    "\n",
    "dataset_length = len(datalist)\n",
    "test_indices = range(dataset_length)\n",
    "test_subset = Subset(datalist, test_indices)\n",
    "\n",
    "node_features = [\"VOL_BASE_CASE\",\n",
    "                 \"CAPACITY_BASE_CASE\",\n",
    "                 \"CAPACITY_REDUCTION\",\n",
    "                 \"FREESPEED\",\n",
    "                 \"LENGTH\"]\n",
    "\n",
    "### Use a new scaler! ###\n",
    "# test_set_normalized, scalers_test = normalize_dataset(dataset_input=test_subset, node_features=node_features)\n",
    "# test_set_loader = DataLoader(dataset=test_set_normalized, batch_size=8,\n",
    "#                              shuffle=True, num_workers=4, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "# scaler_x = scalers_test['x_scaler']\n",
    "# scaler_pos = scalers_test['pos_scaler']\n",
    "##########################\n",
    "\n",
    "##### Load scalers from the run for consistency! #####\n",
    "scaler_x = joblib.load(os.path.join(data_created_during_training, 'test_x_scaler.pkl'))\n",
    "data_list = [copy.deepcopy(test_subset.dataset[idx]) for idx in test_subset.indices]\n",
    "test_set_normalized = normalize_x_features_with_scaler(data_list, node_features, scaler_x)\n",
    "test_set_loader = DataLoader(dataset=test_set_normalized, batch_size=8,\n",
    "                             shuffle=True, num_workers=4, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_net_conv_layer_structure_local_mlp = [int(x) for x in point_net_conv_layer_structure_local_mlp.split(',')]\n",
    "point_net_conv_layer_structure_global_mlp = [int(x) for x in point_net_conv_layer_structure_global_mlp.split(',')]\n",
    "gat_conv_layer_structure = [int(x) for x in gat_conv_layer_structure.split(',')]\n",
    "\n",
    "model = PointNetTransfGAT(in_channels=in_channels, out_channels=out_channels,\n",
    "              point_net_conv_layer_structure_local_mlp=point_net_conv_layer_structure_local_mlp, \n",
    "              point_net_conv_layer_structure_global_mlp = point_net_conv_layer_structure_global_mlp,\n",
    "              gat_conv_layer_structure=gat_conv_layer_structure,\n",
    "              dropout=dropout,\n",
    "              use_dropout=use_dropout,\n",
    "              predict_mode_stats=predict_mode_stats)\n",
    "\n",
    "# Load the model state dictionary\n",
    "model_path = os.path.join(run_path, 'trained_model/model.pth')\n",
    "model.load_state_dict(torch.load(model_path), strict=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fct = torch.nn.MSELoss().to(dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, r_squared, actual_vals, predictions, baseline_loss = hf.validate_model_on_test_set(model, test_set_loader.dataset, loss_fct, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"Baseline Loss: {baseline_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we will look at single elements of the test set and visualize the performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2 # index from the test set, pick a particular sample\n",
    "\n",
    "fixed_norm_max = 50\n",
    "    \n",
    "my_test_data = test_set_loader.dataset[i]\n",
    "my_test_x = test_set_loader.dataset[i].x\n",
    "my_test_x = my_test_x.to('cpu')\n",
    "\n",
    "test_loss_my_test_data, r_squared_my_test_data, actual_vals_my_test_data, predictions_my_test_data, baseline_loss_my_test_data = hf.validate_model_on_test_set(model, my_test_data, loss_fct, device)\n",
    "print(f\"Sample {i}\")\n",
    "print(f\"Test Loss: {test_loss_my_test_data}\")\n",
    "print(f\"R-squared: {r_squared_my_test_data}\")\n",
    "print(f\"Baseline Loss: {baseline_loss_my_test_data}\")\n",
    "\n",
    "inversed_x = scaler_x.inverse_transform(my_test_x)\n",
    "\n",
    "gdf_with_og_values = hf.data_to_geodataframe_with_og_values(data=my_test_data, original_gdf=links_base_case, predicted_values=predictions_my_test_data, inversed_x=inversed_x)\n",
    "gdf_with_og_values['capacity_reduction_rounded'] = gdf_with_og_values['capacity_reduction'].round(decimals=3)\n",
    "gdf_with_og_values['highway'] = gdf_with_og_values['highway'].map(highway_mapping)\n",
    "\n",
    "# gdf_with_og_values['district'] = gdf_with_og_values.apply(lambda row: districts[districts.contains(row.geometry)].iloc[0]['c_ar'] if not districts[districts.contains(row.geometry)].empty else 'Unknown', axis=1)\n",
    "# gdf_with_og_values = gpd.sjoin(gdf_with_og_values, districts, how='left', op='intersects')\n",
    "\n",
    "print(f\"\\nPredicted:\")\n",
    "pf.plot_combined_output(gdf_input=gdf_with_og_values, column_to_plot=\"vol_car_change_predicted\", \n",
    "                        save_it=False, number_to_plot=i, zone_to_plot=\"this zone\", is_predicted=True, alpha=0, use_fixed_norm=True, \n",
    "                        fixed_norm_max=fixed_norm_max, known_districts=False, buffer=0.0005, districts_of_interest=None,\n",
    "                        plot_contour_lines=True, plot_policy_roads=False, result_path=result_path, with_legend=False)\n",
    "\n",
    "print(f\"Actual:\")\n",
    "pf.plot_combined_output(gdf_input=gdf_with_og_values, column_to_plot=\"vol_car_change_actual\", save_it=False, \n",
    "                        number_to_plot=i, zone_to_plot=\"this zone\", is_predicted=False,alpha=10,use_fixed_norm=True, \n",
    "                        fixed_norm_max=fixed_norm_max, known_districts=False, buffer=0.0005, districts_of_interest=None,\n",
    "                        plot_contour_lines=True, plot_policy_roads=False, result_path=result_path, with_legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results across the entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gdfs for the entire test set\n",
    "gdfs = []\n",
    "\n",
    "for i in tqdm(range(len(test_set_loader.dataset))):\n",
    "    my_test_data = test_set_loader.dataset[i]\n",
    "    my_test_x = test_set_loader.dataset[i].x\n",
    "    my_test_x = my_test_x.to('cpu')\n",
    "    \n",
    "    test_loss_my_test_data, r_squared_my_test_data, actual_vals_my_test_data, predictions_my_test_data, baseline_loss_my_test_data = hf.validate_model_on_test_set(model, my_test_data, loss_fct, device)\n",
    "    inversed_x = scaler_x.inverse_transform(my_test_x)\n",
    "    \n",
    "    gdf = hf.data_to_geodataframe_with_og_values(data=my_test_data, original_gdf=links_base_case, predicted_values=predictions_my_test_data, inversed_x=inversed_x)\n",
    "    gdf = gpd.sjoin(gdf, districts, how='left', op='intersects')\n",
    "    gdf = gdf.rename(columns={\"c_ar\": \"district\"})\n",
    "    gdf['capacity_reduction_rounded'] = gdf['capacity_reduction'].round(decimals=3)\n",
    "    gdf['highway'] = gdf['highway'].map(hf.highway_mapping)\n",
    "    \n",
    "    gdfs.append(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discrete plot for prediction error\n",
    "# Absolute differences in number of vehicles\n",
    "discrete_thresholds=(5, 10, 20)\n",
    "\n",
    "result_gdf = pf.plot_average_prediction_differences(\n",
    "    gdf_inputs=gdfs,\n",
    "    scale_type=\"discrete\",\n",
    "    discrete_thresholds=discrete_thresholds,\n",
    "    save_it=True,\n",
    "    use_fixed_norm=True,\n",
    "    fixed_norm_max=100,\n",
    "    use_absolute_value_of_difference=True,\n",
    "    use_percentage=False,\n",
    "    disagreement_threshold=None,\n",
    "    result_path=result_path,\n",
    "    loss_fct=\"l1\",\n",
    "    error_threshold=20,\n",
    "    cmap = 'Spectral_r'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous plot for prediction error, relative difference.\n",
    "fixed_norm_max = 100\n",
    "discrete_thresholds = (10, 50, 100)\n",
    "\n",
    "result_gdf = pf.plot_average_prediction_differences(\n",
    "    gdf_inputs=gdfs,\n",
    "    scale_type=\"discrete\",\n",
    "    discrete_thresholds=discrete_thresholds,\n",
    "    save_it=True,\n",
    "    use_fixed_norm=True,\n",
    "    fixed_norm_max=100,\n",
    "    use_absolute_value_of_difference=True,\n",
    "    use_percentage=True,\n",
    "    disagreement_threshold=None,\n",
    "    result_path=result_path,\n",
    "    loss_fct=\"l1\",\n",
    "    error_threshold=100,\n",
    "    cmap = 'Spectral_r'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference on a separate sample!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_of_interest =  [1, 2, 3, 4] \n",
    "test_data = os.path.join(project_root, \"data\", \"test_data\", \"gdf_pop_1pm_policy_in_1_2_3_4.geojson\")\n",
    "test_data = gpd.read_file(test_data)\n",
    "\n",
    "test_data.crs = \"EPSG:2154\"  # Assuming the original CRS is EPSG:2154\n",
    "test_data.to_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "links_base_case = gpd.read_file(os.path.join(project_root, \"data\", \"test_data\", \"pop_1pm_basecase_mean_links_NEW.geojson\"),\n",
    "                                crs=\"EPSG:4326\")\n",
    "\n",
    "def prepare_gdf(test_df, base_case):\n",
    "    gdf = base_case[['link', 'highway','geometry']].merge(test_df, on='link', how='left')\n",
    "    gdf = gdf.drop(columns=['geometry_x'])\n",
    "    gdf = gdf.rename(columns={'geometry_y': 'geometry'})\n",
    "    return gdf\n",
    "\n",
    "test_data_gdf = prepare_gdf(test_df=test_data, base_case=links_base_case)\n",
    "\n",
    "edge_start_point_tensor, stacked_edge_geometries_tensor, edges_base, nodes = get_link_geometries(links_base_case)\n",
    "\n",
    "test_input_linegraph = hf.create_test_object(links_base_case=links_base_case, test_data = test_data_gdf, stacked_edge_geometries_tensor=stacked_edge_geometries_tensor) \n",
    "normalized_x = hf.normalize_tensor(test_input_linegraph.x, scaler_x)\n",
    "normalized_pos = hf.normalize_pos_features(test_input_linegraph.pos, scaler_pos)\n",
    "\n",
    "normalized_data = Data(\n",
    "    x=normalized_x,\n",
    "    pos=normalized_pos,\n",
    "    edge_index=test_input_linegraph.edge_index,\n",
    "    edge_attr=test_input_linegraph.edge_attr,\n",
    "    y=test_input_linegraph.y\n",
    ")\n",
    "\n",
    "test_loss_my_test_data, r_squared_my_test_data, actual_vals_my_test_data, predictions_my_test_data, baseline_loss_my_test_data = hf.validate_model_on_test_set(model, normalized_data, loss_fct, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss_my_test_data}\")\n",
    "print(f\"R-squared: {r_squared_my_test_data}\")\n",
    "print(f\"Baseline Loss: {baseline_loss_my_test_data}\")\n",
    "\n",
    "normalized_data_x = normalized_data.x\n",
    "my_test_x = normalized_data_x.to('cpu')\n",
    "inversed_x = scaler_x.inverse_transform(my_test_x)\n",
    "\n",
    "gdf_with_og_values = hf.data_to_geodataframe_with_og_values(data=normalized_data, original_gdf=links_base_case, predicted_values=predictions_my_test_data, inversed_x=inversed_x)\n",
    "gdf_with_og_values['capacity_reduction_rounded'] = gdf_with_og_values['capacity_reduction'].round(decimals=3)\n",
    "gdf_with_og_values['highway'] = gdf_with_og_values['highway'].map(hf.highway_mapping)\n",
    "\n",
    "fixed_norm_max = 5\n",
    "\n",
    "hf.plot_combined_output(gdf_input=gdf_with_og_values, column_to_plot=\"vol_car_change_predicted\", \n",
    "                        save_it=False, number_to_plot=0, zone_to_plot = \"this zone\", is_predicted=True, alpha=0, use_fixed_norm=True, \n",
    "                        fixed_norm_max = fixed_norm_max,\n",
    "                        known_districts = True, buffer = 0.0005, districts_of_interest=districts_of_interest)\n",
    "hf.plot_combined_output(gdf_input=gdf_with_og_values, column_to_plot=\"vol_car_change_actual\", save_it=False, \n",
    "                        number_to_plot=None, zone_to_plot = \"this zone\",is_predicted=False,alpha=10,use_fixed_norm=True, \n",
    "                        fixed_norm_max = fixed_norm_max,\n",
    "                        known_districts = True, buffer = 0.0005, districts_of_interest=districts_of_interest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenhao-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

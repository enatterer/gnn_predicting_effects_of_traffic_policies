{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "# Add the 'scripts' directory to Python Path\n",
    "scripts_path=os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "\n",
    "import evaluation.help_functions as hf\n",
    "import evaluation.plot_functions as pf\n",
    "\n",
    "from gnn.help_functions import compute_spearman_pearson\n",
    "from gnn.models.point_net_transf_gat import PointNetTransfGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the absolute path to the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "# Paths, adjust as needed\n",
    "run_path = os.path.join(project_root, \"data\", \"runs_01_2025\", \"wannabe_best_6\")\n",
    "districts = gpd.read_file(os.path.join(project_root, \"data\", \"visualisation\", \"districts_paris.geojson\"))\n",
    "base_case_path = os.path.join(project_root, \"data\", \"links_and_stats\", \"pop_1pct_basecase_average_output_links.geojson\")\n",
    "result_path = 'results/'\n",
    "\n",
    "\n",
    "# GNN Parameters\n",
    "point_net_conv_layer_structure_local_mlp=\"256\"\n",
    "point_net_conv_layer_structure_global_mlp = \"512\"\n",
    "gat_conv_layer_structure = \"128,256,512\"\n",
    "dropout = 0.3\n",
    "use_dropout = False\n",
    "predict_mode_stats = False\n",
    "in_channels = 5\n",
    "out_channels = 1\n",
    "\n",
    "links_base_case = gpd.read_file(base_case_path, crs=\"EPSG:4326\")\n",
    "data_created_during_training = os.path.join(run_path, 'data_created_during_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "### Load test data from the run itself! ###\n",
    "###########################################\n",
    "\n",
    "# Load scalers\n",
    "scaler_x = joblib.load(os.path.join(data_created_during_training, 'test_x_scaler.pkl'))\n",
    "scaler_pos = joblib.load(os.path.join(data_created_during_training, 'test_pos_scaler.pkl'))\n",
    "\n",
    "# Load the test dataset created during training\n",
    "test_set_dl = torch.load(os.path.join(data_created_during_training, 'test_dl.pt'))\n",
    "\n",
    "# Load the DataLoader parameters\n",
    "with open(os.path.join(data_created_during_training, 'test_loader_params.json'), 'r') as f:\n",
    "    test_set_dl_loader_params = json.load(f)\n",
    "    \n",
    "# Remove or correct collate_fn if it is incorrectly specified\n",
    "if 'collate_fn' in test_set_dl_loader_params and isinstance(test_set_dl_loader_params['collate_fn'], str):\n",
    "    del test_set_dl_loader_params['collate_fn']  # Remove it to use the default collate function\n",
    "    \n",
    "test_set_loader = torch.utils.data.DataLoader(test_set_dl, **test_set_dl_loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_net_conv_layer_structure_local_mlp = [int(x) for x in point_net_conv_layer_structure_local_mlp.split(',')]\n",
    "point_net_conv_layer_structure_global_mlp = [int(x) for x in point_net_conv_layer_structure_global_mlp.split(',')]\n",
    "gat_conv_layer_structure = [int(x) for x in gat_conv_layer_structure.split(',')]\n",
    "\n",
    "model = PointNetTransfGAT(in_channels=in_channels, out_channels=out_channels,\n",
    "              point_net_conv_layer_structure_local_mlp=point_net_conv_layer_structure_local_mlp, \n",
    "              point_net_conv_layer_structure_global_mlp = point_net_conv_layer_structure_global_mlp,\n",
    "              gat_conv_layer_structure=gat_conv_layer_structure,\n",
    "              dropout=dropout,\n",
    "              use_dropout=use_dropout,\n",
    "              predict_mode_stats=predict_mode_stats)\n",
    "\n",
    "# Load the model state dictionary\n",
    "model_path = os.path.join(run_path, 'trained_model/model.pth')\n",
    "model.load_state_dict(torch.load(model_path), strict=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fct = torch.nn.MSELoss().to(dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gdfs for the entire test set\n",
    "gdfs = []\n",
    "\n",
    "for i in tqdm(range(len(test_set_loader.dataset))):\n",
    "    my_test_data = test_set_loader.dataset[i]\n",
    "    my_test_x = test_set_loader.dataset[i].x\n",
    "    my_test_x = my_test_x.to('cpu')\n",
    "    \n",
    "    test_loss_my_test_data, r_squared_my_test_data, actual_vals_my_test_data, predictions_my_test_data, baseline_loss_my_test_data = hf.validate_model_on_test_set(model, my_test_data, loss_fct, device)\n",
    "    inversed_x = scaler_x.inverse_transform(my_test_x)\n",
    "    \n",
    "    gdf = hf.data_to_geodataframe_with_og_values(data=my_test_data, original_gdf=links_base_case, predicted_values=predictions_my_test_data, inversed_x=inversed_x)\n",
    "    gdf = gpd.sjoin(gdf, districts, how='left', op='intersects')\n",
    "    gdf = gdf.rename(columns={\"c_ar\": \"district\"})\n",
    "    gdf['capacity_reduction_rounded'] = gdf['capacity_reduction'].round(decimals=3)\n",
    "    gdf['highway'] = gdf['highway'].map(hf.highway_mapping)\n",
    "    \n",
    "    gdfs.append(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### District Analysis\n",
    "Do all districts appear with the same frequency in the test set? Investigated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze district-level patterns\n",
    "def analyze_district_selection_bias(gdf_inputs):\n",
    "    \"\"\"\n",
    "    Analyze if districts with higher volumes/variances are selected more often.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf_inputs : list of GeoDataFrames\n",
    "        Each GDF should contain:\n",
    "        - district information\n",
    "        - vol_base_case (base volume)\n",
    "        - capacity_reduction (to identify selected districts)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with district statistics and selection patterns\n",
    "    \"\"\"\n",
    "    # Get first GDF as reference for base case\n",
    "    base_gdf = gdf_inputs[0].copy()\n",
    "    \n",
    "    # Calculate district-level statistics\n",
    "    district_stats = {}\n",
    "    \n",
    "    # For each district\n",
    "    for district in base_gdf['district'].unique():\n",
    "        district_mask = base_gdf['district'] == district\n",
    "        district_data = base_gdf[district_mask]\n",
    "        \n",
    "        # Calculate base case statistics\n",
    "        mean_volume = district_data['vol_base_case'].mean()\n",
    "        total_volume = district_data['vol_base_case'].sum()\n",
    "        volume_variance = district_data['vol_base_case'].var()\n",
    "        n_roads = len(district_data)\n",
    "        \n",
    "        # Count selections across scenarios\n",
    "        selections = 0\n",
    "        for gdf in gdf_inputs:\n",
    "            district_data = gdf[gdf['district'] == district]\n",
    "            # Count how often this district has capacity reductions\n",
    "            has_reduction = (district_data['capacity_reduction'] < 0).any()\n",
    "            if has_reduction:\n",
    "                selections += 1\n",
    "        \n",
    "        district_stats[district] = {\n",
    "            'mean_volume': mean_volume,\n",
    "            'total_volume': total_volume,\n",
    "            'volume_variance': volume_variance,\n",
    "            'n_roads': n_roads,\n",
    "            'selection_frequency': selections,\n",
    "            'selection_probability': selections / len(gdf_inputs)\n",
    "        }\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df_stats = pd.DataFrame.from_dict(district_stats, orient='index')\n",
    "    \n",
    "    # Calculate correlations\n",
    "    volume_selection_corr = df_stats['total_volume'].corr(df_stats['selection_frequency'])\n",
    "    variance_selection_corr = df_stats['volume_variance'].corr(df_stats['selection_frequency'])\n",
    "    \n",
    "    # Sort districts by selection frequency\n",
    "    df_stats_sorted = df_stats.sort_values('selection_frequency', ascending=False)\n",
    "    \n",
    "    return {\n",
    "        'district_stats': df_stats_sorted,\n",
    "        'correlations': {\n",
    "            'volume_selection': volume_selection_corr,\n",
    "            'variance_selection': variance_selection_corr\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "analysis = analyze_district_selection_bias(gdfs)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nDistrict Statistics (sorted by selection frequency):\")\n",
    "print(analysis['district_stats'])\n",
    "\n",
    "print(\"\\nCorrelations:\")\n",
    "print(f\"Volume vs Selection Frequency: {analysis['correlations']['volume_selection']:.3f}\")\n",
    "print(f\"Variance vs Selection Frequency: {analysis['correlations']['variance_selection']:.3f}\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Volume vs Selection Frequency\n",
    "ax1.scatter(analysis['district_stats']['total_volume'], \n",
    "           analysis['district_stats']['selection_frequency'])\n",
    "ax1.set_xlabel('Total Base Case Volume')\n",
    "ax1.set_ylabel('Selection Frequency')\n",
    "ax1.set_title('Volume vs Selection Frequency')\n",
    "\n",
    "# Variance vs Selection Frequency\n",
    "ax2.scatter(analysis['district_stats']['volume_variance'], \n",
    "           analysis['district_stats']['selection_frequency'])\n",
    "ax2.set_xlabel('Base Case Volume Variance')\n",
    "ax2.set_ylabel('Selection Frequency')\n",
    "ax2.set_title('Variance vs Selection Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze district-level patterns focusing on volumes\n",
    "def analyze_district_volume_bias(gdf_inputs):\n",
    "    \"\"\"\n",
    "    Analyze if districts with higher volumes are selected more often.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf_inputs : list of GeoDataFrames\n",
    "        Each GDF should contain:\n",
    "        - district information\n",
    "        - vol_base_case (base volume)\n",
    "        - capacity_reduction (to identify selected districts)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with district statistics and selection patterns\n",
    "    \"\"\"\n",
    "    # Get first GDF as reference for base case\n",
    "    base_gdf = gdf_inputs[0].copy()\n",
    "    \n",
    "    # Calculate district-level statistics\n",
    "    district_stats = {}\n",
    "    \n",
    "    # For each district\n",
    "    for district in base_gdf['district'].unique():\n",
    "        district_mask = base_gdf['district'] == district\n",
    "        district_data = base_gdf[district_mask]\n",
    "        \n",
    "        # Calculate base case volume statistics\n",
    "        mean_volume = district_data['vol_base_case'].mean()\n",
    "        total_volume = district_data['vol_base_case'].sum()\n",
    "        n_roads = len(district_data)\n",
    "        \n",
    "        # Count selections across scenarios\n",
    "        selections = 0\n",
    "        for gdf in gdf_inputs:\n",
    "            district_data = gdf[gdf['district'] == district]\n",
    "            # Count how often this district has capacity reductions\n",
    "            has_reduction = (district_data['capacity_reduction'] < 0).any()\n",
    "            if has_reduction:\n",
    "                selections += 1\n",
    "        \n",
    "        district_stats[district] = {\n",
    "            'mean_volume': mean_volume,\n",
    "            'total_volume': total_volume,\n",
    "            'n_roads': n_roads,\n",
    "            'selection_frequency': selections,\n",
    "            'selection_probability': selections / len(gdf_inputs)\n",
    "        }\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df_stats = pd.DataFrame.from_dict(district_stats, orient='index')\n",
    "    \n",
    "    # Calculate correlation\n",
    "    volume_selection_corr = df_stats['total_volume'].corr(df_stats['selection_frequency'])\n",
    "    \n",
    "    # Sort districts by volume\n",
    "    df_stats_sorted = df_stats.sort_values('total_volume', ascending=False)\n",
    "    \n",
    "    return {\n",
    "        'district_stats': df_stats_sorted,\n",
    "        'correlation': volume_selection_corr\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "volume_analysis = analyze_district_volume_bias(gdfs)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nDistrict Statistics (sorted by total volume):\")\n",
    "print(volume_analysis['district_stats'])\n",
    "\n",
    "print(\"\\nCorrelation between Volume and Selection Frequency:\", \n",
    "      f\"{volume_analysis['correlation']:.3f}\")\n",
    "\n",
    "# Plot relationship\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(volume_analysis['district_stats']['total_volume'], \n",
    "           volume_analysis['district_stats']['selection_frequency'])\n",
    "plt.xlabel('Total Base Case Volume')\n",
    "plt.ylabel('Selection Frequency')\n",
    "plt.title('District Volume vs Selection Frequency')\n",
    "\n",
    "# Add district labels to points\n",
    "for idx, row in volume_analysis['district_stats'].iterrows():\n",
    "    plt.annotate(f\"District {idx}\", \n",
    "                (row['total_volume'], row['selection_frequency']))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze volume distributions within districts\n",
    "def analyze_within_district_volumes(gdf_inputs):\n",
    "    \"\"\"\n",
    "    Analyze how volumes are distributed within each district.\n",
    "    \"\"\"\n",
    "    # Get first GDF as reference for base case\n",
    "    base_gdf = gdf_inputs[0].copy()\n",
    "    \n",
    "    # Calculate district-level statistics\n",
    "    district_stats = {}\n",
    "    \n",
    "    # For each district\n",
    "    for district in base_gdf['district'].unique():\n",
    "        district_mask = base_gdf['district'] == district\n",
    "        district_data = base_gdf[district_mask]\n",
    "        \n",
    "        # Calculate volume statistics\n",
    "        volume_stats = {\n",
    "            'mean': district_data['vol_base_case'].mean(),\n",
    "            'median': district_data['vol_base_case'].median(),\n",
    "            'std': district_data['vol_base_case'].std(),\n",
    "            'q25': district_data['vol_base_case'].quantile(0.25),\n",
    "            'q75': district_data['vol_base_case'].quantile(0.75),\n",
    "            'skew': district_data['vol_base_case'].skew(),\n",
    "            'n_roads': len(district_data),\n",
    "            'n_high_volume': len(district_data[district_data['vol_base_case'] > district_data['vol_base_case'].mean()]),\n",
    "            'total_volume': district_data['vol_base_case'].sum()\n",
    "        }\n",
    "        \n",
    "        # Count selections across scenarios\n",
    "        selections = 0\n",
    "        for gdf in gdf_inputs:\n",
    "            district_data = gdf[gdf['district'] == district]\n",
    "            has_reduction = (district_data['capacity_reduction'] < 0).any()\n",
    "            if has_reduction:\n",
    "                selections += 1\n",
    "        \n",
    "        volume_stats['selection_frequency'] = selections\n",
    "        district_stats[district] = volume_stats\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_stats = pd.DataFrame.from_dict(district_stats, orient='index')\n",
    "    \n",
    "    # Sort by total volume\n",
    "    df_stats_sorted = df_stats.sort_values('total_volume', ascending=False)\n",
    "    \n",
    "    # Calculate proportion of high-volume roads\n",
    "    df_stats_sorted['prop_high_volume'] = df_stats_sorted['n_high_volume'] / df_stats_sorted['n_roads']\n",
    "    \n",
    "    return df_stats_sorted\n",
    "\n",
    "# Run the analysis\n",
    "district_volume_analysis = analyze_within_district_volumes(gdfs)\n",
    "\n",
    "print(\"\\nDistrict Volume Distribution Analysis:\")\n",
    "print(district_volume_analysis)\n",
    "\n",
    "# Create visualization of volume distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Distribution skewness vs selection frequency\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(district_volume_analysis['skew'], \n",
    "           district_volume_analysis['selection_frequency'])\n",
    "plt.xlabel('Volume Distribution Skewness')\n",
    "plt.ylabel('Selection Frequency')\n",
    "plt.title('Skewness vs Selection Frequency')\n",
    "\n",
    "# Plot 2: Proportion of high-volume roads vs selection frequency\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(district_volume_analysis['prop_high_volume'], \n",
    "           district_volume_analysis['selection_frequency'])\n",
    "plt.xlabel('Proportion of High-Volume Roads')\n",
    "plt.ylabel('Selection Frequency')\n",
    "plt.title('High-Volume Road Proportion vs Selection Frequency')\n",
    "\n",
    "# Plot 3: Box plot of volume distributions by selection frequency quartile\n",
    "plt.subplot(2, 2, 3)\n",
    "district_volume_analysis['selection_quartile'] = pd.qcut(district_volume_analysis['selection_frequency'], \n",
    "                                                       q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "selection_groups = district_volume_analysis.groupby('selection_quartile')\n",
    "box_data = [group['mean'] for name, group in selection_groups]\n",
    "plt.boxplot(box_data, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "plt.xlabel('Selection Frequency Quartile')\n",
    "plt.ylabel('Mean Volume')\n",
    "plt.title('Volume Distribution by Selection Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary of findings:\")\n",
    "print(\"\\n1. Volume distribution characteristics:\")\n",
    "print(f\"Mean skewness across districts: {district_volume_analysis['skew'].mean():.2f}\")\n",
    "print(f\"Mean proportion of high-volume roads: {district_volume_analysis['prop_high_volume'].mean():.2f}\")\n",
    "\n",
    "print(\"\\n2. Correlations with selection frequency:\")\n",
    "print(f\"Skewness correlation: {district_volume_analysis['skew'].corr(district_volume_analysis['selection_frequency']):.3f}\")\n",
    "print(f\"High-volume proportion correlation: {district_volume_analysis['prop_high_volume'].corr(district_volume_analysis['selection_frequency']):.3f}\")\n",
    "\n",
    "# Group districts by selection frequency quartile and analyze volume patterns\n",
    "quartile_stats = district_volume_analysis.groupby('selection_quartile').agg({\n",
    "    'mean': 'mean',\n",
    "    'std': 'mean',\n",
    "    'skew': 'mean',\n",
    "    'prop_high_volume': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\n3. Statistics by selection frequency quartile:\")\n",
    "print(quartile_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_district_volumes(gdf_inputs):\n",
    "    \"\"\"\n",
    "    Analyze volume distribution across districts with robust nan handling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf_inputs : list of GeoDataFrames\n",
    "        Each GDF should contain:\n",
    "        - district information\n",
    "        - vol_base_case (base volume)\n",
    "        - capacity_reduction (to identify selected districts)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with district statistics\n",
    "    \"\"\"\n",
    "    if not gdf_inputs or len(gdf_inputs) == 0:\n",
    "        raise ValueError(\"Empty input list provided\")\n",
    "\n",
    "    # Get first GDF as reference\n",
    "    base_gdf = gdf_inputs[0].copy()\n",
    "    \n",
    "    # Validate required columns\n",
    "    required_columns = ['district', 'vol_base_case', 'capacity_reduction']\n",
    "    if not all(col in base_gdf.columns for col in required_columns):\n",
    "        raise ValueError(f\"Missing required columns. Expected: {required_columns}\")\n",
    "    \n",
    "    # Calculate district statistics\n",
    "    district_stats = {}\n",
    "    \n",
    "    # For each district\n",
    "    for district in base_gdf['district'].unique():\n",
    "        if pd.isna(district):  # Skip if district is nan\n",
    "            continue\n",
    "            \n",
    "        district_mask = base_gdf['district'] == district\n",
    "        district_data = base_gdf[district_mask]\n",
    "        \n",
    "        # Calculate mean volume with nan handling\n",
    "        volumes = district_data['vol_base_case'].dropna()\n",
    "        if len(volumes) == 0:\n",
    "            mean_volume = np.nan\n",
    "        else:\n",
    "            mean_volume = volumes.mean()\n",
    "        \n",
    "        # Count edges (excluding nan values)\n",
    "        n_edges = len(district_data.dropna(subset=['vol_base_case']))\n",
    "        \n",
    "        # Count selections across all scenarios\n",
    "        selections = 0\n",
    "        for gdf in gdf_inputs:\n",
    "            if 'district' not in gdf.columns or 'capacity_reduction' not in gdf.columns:\n",
    "                continue\n",
    "                \n",
    "            district_data = gdf[gdf['district'] == district]\n",
    "            # Count how often this district has capacity reductions, handling nans\n",
    "            capacity_reductions = district_data['capacity_reduction'].dropna()\n",
    "            if len(capacity_reductions) > 0 and (capacity_reductions < 0).any():\n",
    "                selections += 1\n",
    "        \n",
    "        district_stats[district] = {\n",
    "            'mean_volume': mean_volume,\n",
    "            'n_edges': n_edges,\n",
    "            'selection_frequency': selections\n",
    "        }\n",
    "    \n",
    "    # Calculate overall statistics, excluding nan values\n",
    "    valid_volumes = [stats['mean_volume'] for stats in district_stats.values() \n",
    "                    if not np.isnan(stats['mean_volume'])]\n",
    "    \n",
    "    if not valid_volumes:\n",
    "        return {\n",
    "            'district_stats': district_stats,\n",
    "            'mean_volume': np.nan,\n",
    "            'std_volume': np.nan,\n",
    "            'threshold': np.nan,\n",
    "            'high_volume_districts': {}\n",
    "        }\n",
    "    \n",
    "    mean_volume = np.mean(valid_volumes)\n",
    "    std_volume = np.std(valid_volumes)\n",
    "    threshold = mean_volume + std_volume\n",
    "    \n",
    "    # Identify high-volume districts, excluding nan values\n",
    "    high_volume_districts = {\n",
    "        d: stats for d, stats in district_stats.items() \n",
    "        if not np.isnan(stats['mean_volume']) and stats['mean_volume'] > threshold\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'district_stats': district_stats,\n",
    "        'mean_volume': mean_volume,\n",
    "        'std_volume': std_volume,\n",
    "        'threshold': threshold,\n",
    "        'high_volume_districts': high_volume_districts\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "stats = analyze_district_volumes(gdfs)\n",
    "print(f\"Mean volume across districts: {stats['mean_volume']:.2f}\")\n",
    "print(f\"Standard deviation: {stats['std_volume']:.2f}\")\n",
    "print(f\"Threshold for high-volume: {stats['threshold']:.2f}\")\n",
    "print(\"\\nHigh-volume districts:\")\n",
    "for district, data in stats['high_volume_districts'].items():\n",
    "    print(f\"District {district}:\")\n",
    "    print(f\"  Mean volume: {data['mean_volume']:.2f}\")\n",
    "    print(f\"  Number of edges: {data['n_edges']}\")\n",
    "    print(f\"  Selection frequency: {data['selection_frequency']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between volume and variance\n",
    "volume_variance_corr = analysis['district_stats']['total_volume'].corr(analysis['district_stats']['volume_variance'])\n",
    "print(\"\\nCorrelation between Volume and Variance:\", f\"{volume_variance_corr:.3f}\")\n",
    "\n",
    "# Create scatter plot of volume vs variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(analysis['district_stats']['total_volume'], \n",
    "           analysis['district_stats']['volume_variance'])\n",
    "plt.xlabel('Total Base Case Volume')\n",
    "plt.ylabel('Volume Variance')\n",
    "plt.title('District Volume vs Variance')\n",
    "\n",
    "# Add district labels to points\n",
    "for idx, row in analysis['district_stats'].iterrows():\n",
    "    plt.annotate(f\"District {idx}\", \n",
    "                (row['total_volume'], row['volume_variance']))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis by Road Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_volumes_by_capacity_reduction(gdf):\n",
    "    \"\"\"\n",
    "    Compare volumes between roads with and without capacity reduction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Should contain:\n",
    "        - vol_base_case (base volume)\n",
    "        - capacity_reduction\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with comparison statistics\n",
    "    \"\"\"\n",
    "    # Create masks for roads with and without capacity reduction\n",
    "    has_reduction = gdf['capacity_reduction'] < 0\n",
    "    no_reduction = gdf['capacity_reduction'] >= 0\n",
    "    \n",
    "    # Calculate statistics for each group\n",
    "    reduced_stats = {\n",
    "        'mean_volume': gdf[has_reduction]['vol_base_case'].mean(),\n",
    "        'std_volume': gdf[has_reduction]['vol_base_case'].std(),\n",
    "        'count': has_reduction.sum()\n",
    "    }\n",
    "    \n",
    "    normal_stats = {\n",
    "        'mean_volume': gdf[no_reduction]['vol_base_case'].mean(),\n",
    "        'std_volume': gdf[no_reduction]['vol_base_case'].std(),\n",
    "        'count': no_reduction.sum()\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'with_reduction': reduced_stats,\n",
    "        'without_reduction': normal_stats\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "comparison = analyze_volumes_by_capacity_reduction(gdfs[0])\n",
    "print(\"\\nComparison of roads with and without capacity reduction:\")\n",
    "print(\"\\nRoads WITH capacity reduction:\")\n",
    "print(f\"  Mean volume: {comparison['with_reduction']['mean_volume']:.2f}\")\n",
    "print(f\"  Std volume: {comparison['with_reduction']['std_volume']:.2f}\")\n",
    "print(f\"  Count: {comparison['with_reduction']['count']}\")\n",
    "print(\"\\nRoads WITHOUT capacity reduction:\")\n",
    "print(f\"  Mean volume: {comparison['without_reduction']['mean_volume']:.2f}\")\n",
    "print(f\"  Std volume: {comparison['without_reduction']['std_volume']:.2f}\")\n",
    "print(f\"  Count: {comparison['without_reduction']['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_statistical_analysis(gdf):\n",
    "    \"\"\"\n",
    "    Perform comprehensive statistical analysis comparing roads with and without capacity reduction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Should contain:\n",
    "        - vol_base_case (base volume)\n",
    "        - capacity_reduction\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with detailed statistical analysis\n",
    "    \"\"\"\n",
    "    import scipy.stats as stats\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create masks for roads with and without capacity reduction\n",
    "    has_reduction = gdf['capacity_reduction'] < 0\n",
    "    no_reduction = gdf['capacity_reduction'] >= 0\n",
    "    \n",
    "    # Get volumes for each group\n",
    "    volumes_reduced = gdf[has_reduction]['vol_base_case'].dropna()\n",
    "    volumes_normal = gdf[no_reduction]['vol_base_case'].dropna()\n",
    "    \n",
    "    # Basic statistics\n",
    "    stats_reduced = {\n",
    "        'count': len(volumes_reduced),\n",
    "        'mean': volumes_reduced.mean(),\n",
    "        'median': volumes_reduced.median(),\n",
    "        'std': volumes_reduced.std(),\n",
    "        'q25': volumes_reduced.quantile(0.25),\n",
    "        'q75': volumes_reduced.quantile(0.75),\n",
    "        'skew': stats.skew(volumes_reduced),\n",
    "        'kurtosis': stats.kurtosis(volumes_reduced)\n",
    "    }\n",
    "    \n",
    "    stats_normal = {\n",
    "        'count': len(volumes_normal),\n",
    "        'mean': volumes_normal.mean(),\n",
    "        'median': volumes_normal.median(),\n",
    "        'std': volumes_normal.std(),\n",
    "        'q25': volumes_normal.quantile(0.25),\n",
    "        'q75': volumes_normal.quantile(0.75),\n",
    "        'skew': stats.skew(volumes_normal),\n",
    "        'kurtosis': stats.kurtosis(volumes_normal)\n",
    "    }\n",
    "    \n",
    "    # Perform Mann-Whitney U test (non-parametric test for different distributions)\n",
    "    mw_stat, mw_pval = stats.mannwhitneyu(volumes_reduced, volumes_normal, alternative='two-sided')\n",
    "    \n",
    "    # Calculate effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt(((stats_reduced['count'] - 1) * stats_reduced['std']**2 + \n",
    "                         (stats_normal['count'] - 1) * stats_normal['std']**2) / \n",
    "                        (stats_reduced['count'] + stats_normal['count'] - 2))\n",
    "    cohens_d = (stats_reduced['mean'] - stats_normal['mean']) / pooled_std\n",
    "    \n",
    "    return {\n",
    "        'with_reduction': stats_reduced,\n",
    "        'without_reduction': stats_normal,\n",
    "        'mann_whitney': {\n",
    "            'statistic': mw_stat,\n",
    "            'p_value': mw_pval\n",
    "        },\n",
    "        'cohens_d': cohens_d\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "analysis = perform_statistical_analysis(gdfs[0])\n",
    "\n",
    "# Print results\n",
    "print(\"\\nDetailed Statistical Analysis:\")\n",
    "print(\"\\nRoads WITH capacity reduction:\")\n",
    "print(f\"  Count: {analysis['with_reduction']['count']}\")\n",
    "print(f\"  Mean: {analysis['with_reduction']['mean']:.2f}\")\n",
    "print(f\"  Median: {analysis['with_reduction']['median']:.2f}\")\n",
    "print(f\"  Std Dev: {analysis['with_reduction']['std']:.2f}\")\n",
    "print(f\"  Q25-Q75: [{analysis['with_reduction']['q25']:.2f} - {analysis['with_reduction']['q75']:.2f}]\")\n",
    "print(f\"  Skewness: {analysis['with_reduction']['skew']:.2f}\")\n",
    "\n",
    "print(\"\\nRoads WITHOUT capacity reduction:\")\n",
    "print(f\"  Count: {analysis['without_reduction']['count']}\")\n",
    "print(f\"  Mean: {analysis['without_reduction']['mean']:.2f}\")\n",
    "print(f\"  Median: {analysis['without_reduction']['median']:.2f}\")\n",
    "print(f\"  Std Dev: {analysis['without_reduction']['std']:.2f}\")\n",
    "print(f\"  Q25-Q75: [{analysis['without_reduction']['q25']:.2f} - {analysis['without_reduction']['q75']:.2f}]\")\n",
    "print(f\"  Skewness: {analysis['without_reduction']['skew']:.2f}\")\n",
    "\n",
    "print(\"\\nStatistical Tests:\")\n",
    "print(f\"  Mann-Whitney U p-value: {analysis['mann_whitney']['p_value']:.10f}\")\n",
    "print(f\"  Effect size (Cohen's d): {analysis['cohens_d']:.2f}\")\n",
    "\n",
    "# Print interpretation\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"1. Statistical Significance:\")\n",
    "if analysis['mann_whitney']['p_value'] < 0.05:\n",
    "    print(\"  - The difference in volumes is statistically significant (p < 0.05)\")\n",
    "else:\n",
    "    print(\"  - The difference in volumes is not statistically significant (p >= 0.05)\")\n",
    "\n",
    "print(\"\\n2. Effect Size Interpretation:\")\n",
    "d = abs(analysis['cohens_d'])\n",
    "if d < 0.2:\n",
    "    effect = \"negligible\"\n",
    "elif d < 0.5:\n",
    "    effect = \"small\"\n",
    "elif d < 0.8:\n",
    "    effect = \"medium\"\n",
    "else:\n",
    "    effect = \"large\"\n",
    "print(f\"  - Cohen's d = {d:.2f} indicates a {effect} effect size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_variances_by_road_type_and_reduction(gdfs):\n",
    "    # Initialize dictionaries for each road type\n",
    "    road_types = {\n",
    "        1: \"Primary\",\n",
    "        2: \"Secondary\",\n",
    "        3: \"Tertiary\"\n",
    "    }\n",
    "    \n",
    "    stats = {road_type: {\n",
    "        'with_reduction': {'actual_changes': [], 'predictions': [], 'errors': []},\n",
    "        'without_reduction': {'actual_changes': [], 'predictions': [], 'errors': []}\n",
    "    } for road_type in road_types.keys()}\n",
    "    \n",
    "    # Collect values from all GDFs\n",
    "    for gdf in gdfs:\n",
    "        for road_type in road_types.keys():\n",
    "            # Filter for specific road type\n",
    "            road_type_mask = gdf['highway'] == road_type\n",
    "            \n",
    "            # Roads with capacity reduction\n",
    "            reduced = (gdf['capacity_reduction_rounded'] < -1e-3) & road_type_mask\n",
    "            stats[road_type]['with_reduction']['actual_changes'].extend(gdf.loc[reduced, 'vol_car_change_actual'])\n",
    "            stats[road_type]['with_reduction']['predictions'].extend(gdf.loc[reduced, 'vol_car_change_predicted'])\n",
    "            stats[road_type]['with_reduction']['errors'].extend(\n",
    "                gdf.loc[reduced, 'vol_car_change_predicted'] - gdf.loc[reduced, 'vol_car_change_actual']\n",
    "            )\n",
    "            \n",
    "            # Roads without capacity reduction\n",
    "            not_reduced = (gdf['capacity_reduction_rounded'] >= -1e-3) & road_type_mask\n",
    "            stats[road_type]['without_reduction']['actual_changes'].extend(gdf.loc[not_reduced, 'vol_car_change_actual'])\n",
    "            stats[road_type]['without_reduction']['predictions'].extend(gdf.loc[not_reduced, 'vol_car_change_predicted'])\n",
    "            stats[road_type]['without_reduction']['errors'].extend(\n",
    "                gdf.loc[not_reduced, 'vol_car_change_predicted'] - gdf.loc[not_reduced, 'vol_car_change_actual']\n",
    "            )\n",
    "    \n",
    "    # Calculate and print statistics for each road type\n",
    "    for road_type in road_types.keys():\n",
    "        print(f\"\\n{road_types[road_type]} Roads:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for reduction_type in ['with_reduction', 'without_reduction']:\n",
    "            # Convert to numpy arrays\n",
    "            actual = np.array(stats[road_type][reduction_type]['actual_changes'])\n",
    "            errors = np.array(stats[road_type][reduction_type]['errors'])\n",
    "            \n",
    "            if len(actual) > 0:  # Only print if we have data\n",
    "                print(f\"\\n{reduction_type.replace('_', ' ').title()}:\")\n",
    "                print(f\"Number of observations: {len(actual)}\")\n",
    "                print(f\"Variance of actual changes: {np.var(actual):.2f}\")\n",
    "                print(f\"Mean absolute error: {np.mean(np.abs(errors)):.2f}\")\n",
    "                print(f\"MSE: {np.mean(errors**2):.2f}\")\n",
    "                print(f\"Mean actual change: {np.mean(actual):.2f}\")\n",
    "                print(f\"Std of actual changes: {np.std(actual):.2f}\")\n",
    "                print(f\"R² score: {1 - np.mean(errors**2)/np.var(actual):.3f}\")\n",
    "\n",
    "analyze_variances_by_road_type_and_reduction(gdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model_with_interpretable_error(indices, gdfs):\n",
    "    \"\"\"\n",
    "    Validate model performance across all test set observations for specific road types.\n",
    "    \n",
    "    Args:\n",
    "        indices: Indices of roads to consider for each GDF\n",
    "        gdfs: List of GeoDataFrames, each representing one test set observation\n",
    "    \"\"\"\n",
    "    loss_fct_l1 = torch.nn.L1Loss()\n",
    "    loss_fct_l2 = torch.nn.MSELoss()\n",
    "    \n",
    "    # Initialize lists to store values across all observations\n",
    "    all_actual_vals = []\n",
    "    all_predicted_vals = []\n",
    "    mean_car_vols = []\n",
    "    variances_base_case = []\n",
    "    variances = []\n",
    "    std_devs = []\n",
    "    std_dev_multiplied = []\n",
    "    cv_percents = []\n",
    "    \n",
    "    # Collect values from all GDFs\n",
    "    i = 0\n",
    "    for gdf in gdfs:\n",
    "        indices = hf.get_road_type_indices(gdf)[road_type]\n",
    "        \n",
    "        if len(indices) > 0:  # Only process if we have roads of this type\n",
    "            i += 1\n",
    "            actual_vals = gdf.loc[indices, 'vol_car_change_actual']\n",
    "            predicted_vals = gdf.loc[indices, 'vol_car_change_predicted']\n",
    "            \n",
    "            all_actual_vals.extend(actual_vals.to_numpy())\n",
    "            all_predicted_vals.extend(predicted_vals.to_numpy())\n",
    "            \n",
    "            # Collect statistics\n",
    "            mean_car_vols.append(gdf.loc[indices, 'vol_base_case'].mean())\n",
    "            \n",
    "            car_volumes = actual_vals.to_numpy()\n",
    "            car_volume_variance = np.var(car_volumes)\n",
    "            \n",
    "            variances.append(car_volume_variance)\n",
    "            \n",
    "            variances_base_case.append(gdf.loc[indices, 'variance_base_case'].mean())\n",
    "            std_devs.append(gdf.loc[indices, 'std_dev'].mean())\n",
    "            std_dev_multiplied.append(gdf.loc[indices, 'std_dev_multiplied'].mean())\n",
    "            cv_percents.append(gdf.loc[indices, 'cv_percent'].mean())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_actual_vals = np.array(all_actual_vals)\n",
    "    all_predicted_vals = np.array(all_predicted_vals)\n",
    "    \n",
    "    actual_mean = torch.mean(torch.tensor(all_actual_vals))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    spearman_corr, pearson_corr = compute_spearman_pearson(all_predicted_vals, all_actual_vals, is_np=True)    \n",
    "    r_squared = hf.compute_r2_torch(preds=torch.tensor(all_predicted_vals), targets=torch.tensor(all_actual_vals))\n",
    "    \n",
    "    l1_loss = loss_fct_l1(torch.tensor(all_actual_vals), torch.tensor(all_predicted_vals))\n",
    "    l2_loss = loss_fct_l2(torch.tensor(all_actual_vals), torch.tensor(all_predicted_vals))\n",
    "    \n",
    "    l1_naive = loss_fct_l1(torch.tensor(all_actual_vals), torch.full_like(torch.tensor(all_actual_vals), actual_mean))    \n",
    "    l2_naive = loss_fct_l2(torch.tensor(all_actual_vals), torch.full_like(torch.tensor(all_actual_vals), actual_mean))\n",
    "    \n",
    "    # Calculate averages of statistics\n",
    "    mean_car_vol = np.mean(mean_car_vols)\n",
    "    variance = np.mean(variances)\n",
    "    variance_base_case = np.mean(variances_base_case)\n",
    "    std_dev = np.mean(std_devs)\n",
    "    std_dev_multiplied = np.mean(std_dev_multiplied)\n",
    "    cv_percent = np.mean(cv_percents)\n",
    "    \n",
    "    print(\" \")\n",
    "    print(f\"Road Type: {road_type}\")\n",
    "    print(f\"Number of observations: {len(all_actual_vals)/416}\")\n",
    "    print(f\"Mean Car Volume: {mean_car_vol}\")\n",
    "    print(f\"R-squared: {round(r_squared.item(), 2)}\")\n",
    "    print(f\"MSE Loss: {l2_loss}\")\n",
    "    print(f\"Naive MSE Loss: {l2_naive}\")\n",
    "    print(f\"Variance Base Case: {variance_base_case}\")\n",
    "    \n",
    "    \n",
    "    # print(f\"Variance over car volumes: {car_volume_variance}\")\n",
    "    print(f\"Variance: {variance}\")\n",
    "    print(f\"L1 Loss: {l1_loss}\")\n",
    "    print(f\"Naive L1 loss: {l1_naive}\")\n",
    "    print(f\"Standard Deviation Multiplied: {std_dev_multiplied}\")\n",
    "    print(f\"Spearman Correlation: {spearman_corr}\")\n",
    "    print(f\"Pearson Correlation: {pearson_corr}\")\n",
    "    print(f\"Standard Deviation: {std_dev}\")\n",
    "    print(f\"Coefficient of Variation: {cv_percent}\")\n",
    "    print(\" \")\n",
    "    \n",
    "    return {\n",
    "        'road_type': road_type,\n",
    "        'number_of_observations': len(all_actual_vals),\n",
    "        'mean_car_vol': mean_car_vol,\n",
    "        'r_squared': r_squared,\n",
    "        'mse': l2_loss,\n",
    "        'naive_mse': l2_naive,\n",
    "        'l1': l1_loss,\n",
    "        'naive_l1': l1_naive,\n",
    "        'variance': variance_base_case,\n",
    "        'std_dev': std_dev,\n",
    "        'std_dev_normalized': std_dev_multiplied,\n",
    "        'spearman': spearman_corr,\n",
    "        'pearson': pearson_corr,\n",
    "        'cv_percent': cv_percent\n",
    "    }\n",
    "\n",
    "road_types = list(hf.get_road_type_indices(gdfs[0]).keys())\n",
    "\n",
    "# Then calculate metrics for each road type\n",
    "metrics_by_type = {}\n",
    "for road_type in road_types:\n",
    "    metrics_by_type[road_type] = validate_model_with_interpretable_error(road_type, gdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_metrics = [       \n",
    "            {\n",
    "                'id': 'spearman',\n",
    "                'label': 'Spearman\\nCorrelation',\n",
    "                'transform': lambda x: max(0, x),\n",
    "                'y_pos': -0.05\n",
    "            },\n",
    "            \n",
    "            {\n",
    "                'id': 'r_squared',\n",
    "                'label': 'R²',\n",
    "                'transform': lambda x: max(0, x),\n",
    "                'y_pos': -0.05\n",
    "            },\n",
    "            \n",
    "            {\n",
    "                'id': 'l1_ratio',\n",
    "                'label': 'Normalized MAE',\n",
    "                'transform': lambda x, max_ratio: (1 - x/max_ratio),\n",
    "                'y_pos': -0.1\n",
    "            },\n",
    "            \n",
    "            {\n",
    "                'id': 'pearson',\n",
    "                'label': 'Pearson\\nCorrelation',\n",
    "                'transform': lambda x: max(0, x),\n",
    "                'y_pos': -0.05\n",
    "            },\n",
    "\n",
    "            {\n",
    "                'id': 'error_distribution',\n",
    "                'label': 'Error\\nDistribution',\n",
    "                'transform': lambda x: max(0, (x - 68) / (100 - 68)),  # Normalize relative to normal distribution baseline\n",
    "                'y_pos': -0.05\n",
    "            }\n",
    "        ]\n",
    "\n",
    "# colors = {\n",
    "#     'Trunk Roads': '#d73027',         # Red\n",
    "#     'Primary Roads': '#fc8d59',       # Orange\n",
    "#     'Secondary Roads': '#fee090',     # Yellow\n",
    "#     'Tertiary Roads': '#e0f3f8',      # Light blue\n",
    "#     'Residential Streets': '#91bfdb',  # Medium blue\n",
    "#     'Living Streets': '#4575b4',      # Dark blue\n",
    "#     'P/S/T Roads with Capacity Reduction': '#999999',    \n",
    "#     'P/S/T Roads with No Capacity Reduction': '#666666',\n",
    "# }\n",
    "\n",
    "# Professional color palette with good contrast and accessibility\n",
    "colors = {\n",
    "    'Trunk Roads': '#1f77b4',         # Muted blue\n",
    "    'Primary Roads': '#2ca02c',       # Muted green\n",
    "    'Secondary Roads': '#ff7f0e',     # Muted orange\n",
    "    'Tertiary Roads': '#9467bd',      # Muted purple\n",
    "    'Residential Streets': '#8c564b',  # Brown\n",
    "    'Living Streets': '#e377c2',      # Pink\n",
    "    'P/S/T Roads with Capacity Reduction': '#7f7f7f',    # Gray\n",
    "    'P/S/T Roads with No Capacity Reduction': '#bcbd22', # Olive\n",
    "}\n",
    "\n",
    "# Define selected road types\n",
    "selected_types = [\n",
    "    # 'All Roads',\n",
    "    'Trunk Roads',\n",
    "    'Primary Roads',\n",
    "    'Secondary Roads',\n",
    "    'Tertiary Roads',\n",
    "    'Residential Streets',\n",
    "    'Living Streets',\n",
    "    'P/S/T Roads with Capacity Reduction',\n",
    "    'P/S/T Roads with No Capacity Reduction'\n",
    "    # 'Primary Roads with Capacity Reduction',\n",
    "    # 'Primary Roads with No Capacity Reduction',\n",
    "    # 'Secondary Roads with Capacity Reduction',\n",
    "    # 'Secondary Roads with No Capacity Reduction',\n",
    "    # 'Tertiary Roads with Capacity Reduction',\n",
    "    # 'Tertiary Roads with No Capacity Reduction'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.create_correlation_radar_plot_sort_by_r2(metrics_by_type, selected_metrics, result_path=result_path, save_it=True, selected_types=selected_types, colors=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.create_error_vs_variability_scatterplots(metrics_by_type, result_path=result_path, save_it=True, selected_types=selected_types, colors=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to your notebook to investigate the distribution of errors\n",
    "def analyze_error_distribution(metrics_by_type):\n",
    "    road_type = \"All roads\"\n",
    "    gdf_data = gdfs[0]  # Get first GDF for \"All roads\"\n",
    "    \n",
    "    actual = gdf_data['vol_car_change_actual']\n",
    "    predicted = gdf_data['vol_car_change_predicted']\n",
    "    \n",
    "    # Calculate errors\n",
    "    errors = predicted - actual\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(errors, bins=50)\n",
    "    plt.title(\"Distribution of Prediction Errors for All Roads\")\n",
    "    plt.xlabel(\"Prediction Error\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"Error Statistics for {road_type}:\")\n",
    "    print(f\"Mean error: {errors.mean():.2f}\")\n",
    "    print(f\"Median error: {errors.median():.2f}\")\n",
    "    print(f\"Standard deviation of errors: {errors.std():.2f}\")\n",
    "    print(f\"Percentage of errors within ±1 std: {(abs(errors) <= errors.std()).mean()*100:.1f}%\")\n",
    "    \n",
    "analyze_error_distribution(metrics_by_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenhao-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

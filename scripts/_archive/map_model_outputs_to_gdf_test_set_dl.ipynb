{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import wandb\n",
    "import pickle\n",
    "import os\n",
    "import shapely.wkt as wkt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "from torch_geometric.transforms import LineGraph\n",
    "\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import processing_io as pio\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from shapely.geometry import Point, LineString, box\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "from shapely.ops import unary_union\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from torch_geometric.data import Data, Batch\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import help_functions as hf\n",
    "\n",
    "districts = gpd.read_file(\"../../data/visualisation/districts_paris.geojson\")\n",
    "\n",
    "# Add the 'scripts' directory to the Python path\n",
    "scripts_path = os.path.abspath(os.path.join('..'))\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "\n",
    "import gnn_io as gio\n",
    "import gnn_architectures as garch\n",
    "\n",
    "highway_mapping = {\n",
    "    'trunk': 0, 'trunk_link': 0, 'motorway_link': 0,\n",
    "    'primary': 1, 'primary_link': 1,\n",
    "    'secondary': 2, 'secondary_link': 2,\n",
    "    'tertiary': 3, 'tertiary_link': 3,\n",
    "    'residential': 4, 'living_street': 5,\n",
    "    'pedestrian': 6, 'service': 7,\n",
    "    'construction': 8, 'unclassified': 9,\n",
    "    'np.nan': -1\n",
    "}\n",
    "\n",
    "def compute_r2_torch_with_mean_targets(mean_targets, preds, targets):\n",
    "    ss_tot = torch.sum((targets - mean_targets) ** 2)\n",
    "    ss_res = torch.sum((targets - preds) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2\n",
    "\n",
    "def validate_one_model(model, data, loss_func, device):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    actual = []\n",
    "    with torch.inference_mode():\n",
    "        input_node_features, targets = data.x.to(device), data.y.to(device)\n",
    "        predicted = model(data.to(device))\n",
    "        # print(predicted.shape)\n",
    "        pred.append(predicted)\n",
    "        actual.append(targets)\n",
    "        val_loss = loss_func(predicted, targets).item()\n",
    "    actual_vals = torch.cat(actual)\n",
    "    predicted_vals = torch.cat(pred)\n",
    "    \n",
    "    mean_targets = torch.mean(targets)\n",
    "    r_squared = compute_r2_torch_with_mean_targets(mean_targets = mean_targets, preds=predicted_vals, targets=actual_vals)\n",
    "    baseline_loss = loss_func(targets, torch.full_like(predicted_vals, mean_targets))\n",
    "    return val_loss, r_squared, targets, predicted, baseline_loss\n",
    "\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import wandb\n",
    "import pickle\n",
    "import os\n",
    "import shapely.wkt as wkt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "from torch_geometric.transforms import LineGraph\n",
    "\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import processing_io as pio\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from shapely.geometry import Point, LineString, box\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "from shapely.ops import unary_union\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from torch_geometric.data import Data, Batch\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import alphashape\n",
    "from matplotlib.lines import Line2D\n",
    "from shapely.geometry import Polygon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PointNetConv(local_nn=Sequential(\n",
      "  (0): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "), global_nn=Sequential(\n",
      "  (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "  (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "  (4): ReLU()\n",
      "))\n",
      "Initializing 0.weight with kaiming_normal\n",
      "Initializing 0.bias with zeros\n",
      "Initializing 0.weight with kaiming_normal\n",
      "Initializing 0.bias with zeros\n",
      "Initializing 1.weight with kaiming_normal\n",
      "Initializing 1.bias with zeros\n",
      "Initializing 3.weight with kaiming_normal\n",
      "Initializing 3.bias with zeros\n",
      "Initializing Linear(in_features=6, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=512, bias=True)\n",
      "Initializing Linear(in_features=512, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=512, bias=True)\n",
      "Initializing GATConv(512, 512, heads=1)\n",
      "Initializing GATConv(512, 256, heads=1)\n",
      "Initializing GATConv(256, 128, heads=1)\n",
      "Initializing GATConv(128, 64, heads=1)\n",
      "Initializing GATConv(64, 1, heads=1)\n",
      "Model initialized\n",
      "MyGnn(\n",
      "  (point_net_layer): PointNetConv(local_nn=Sequential(\n",
      "    (0): Linear(in_features=6, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "  ), global_nn=Sequential(\n",
      "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (graph_layers): Sequential(\n",
      "    (0) - GATConv(512, 512, heads=1): x, edge_index -> x\n",
      "    (1) - ReLU(inplace=True): x -> x\n",
      "    (2) - GATConv(512, 256, heads=1): x, edge_index -> x\n",
      "    (3) - ReLU(inplace=True): x -> x\n",
      "    (4) - GATConv(256, 128, heads=1): x, edge_index -> x\n",
      "    (5) - ReLU(inplace=True): x -> x\n",
      "    (6) - GATConv(128, 64, heads=1): x, edge_index -> x\n",
      "    (7) - ReLU(inplace=True): x -> x\n",
      "    (8) - GATConv(64, 1, heads=1): x, edge_index -> x\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Parameters to define\n",
    "run_path = '/home/enatterer/Development/gnn_predicting_effects_of_traffic_policies/data/runs_optimized/pnc_local_[256]_pnc_global_[512_256]_hidden_layer_str_[512_512_256_128_64]_dropout_0.3_use_dropout_False/'\n",
    "point_net_conv_layer_structure_local_mlp = [256]\n",
    "point_net_conv_layer_structure_global_mlp = [512,256]\n",
    "gat_conv_layer_structure = [512,512,256,128,64]\n",
    "dropout = 0.3\n",
    "use_dropout = False \n",
    "in_channels = 6 \n",
    "out_channels = 1 \n",
    "\n",
    "model_path = run_path +  'trained_model/model.pth'\n",
    "data_created_during_training = run_path + 'data_created_during_training/'\n",
    "indices_of_datasets_to_use = [0, 1, 3, 4]\n",
    "\n",
    "scaler_x = joblib.load(data_created_during_training + 'x_scaler.pkl')\n",
    "scaler_pos = joblib.load(data_created_during_training + 'pos_scaler.pkl')\n",
    "\n",
    "# Initialize the model\n",
    "model = garch.MyGnn(in_channels=in_channels, out_channels=out_channels, \n",
    "                    point_net_conv_layer_structure_local_mlp=point_net_conv_layer_structure_local_mlp, \n",
    "                    point_net_conv_layer_structure_global_mlp = point_net_conv_layer_structure_global_mlp,\n",
    "                    gat_conv_layer_structure=gat_conv_layer_structure,\n",
    "                    dropout=dropout,\n",
    "                    use_dropout=use_dropout)\n",
    "\n",
    "# Load the model state dictionary\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset created during training\n",
    "test_set_dl = torch.load(data_created_during_training + 'test_dl.pt')\n",
    "\n",
    "# Load the DataLoader parameters\n",
    "with open(data_created_during_training + 'test_loader_params.json', 'r') as f:\n",
    "    test_set_dl_loader_params = json.load(f)\n",
    "    \n",
    "# Remove or correct collate_fn if it is incorrectly specified\n",
    "if 'collate_fn' in test_set_dl_loader_params and isinstance(test_set_dl_loader_params['collate_fn'], str):\n",
    "    del test_set_dl_loader_params['collate_fn']  # Remove it to use the default collate function\n",
    "    \n",
    "test_set_loader = torch.utils.data.DataLoader(test_set_dl, **test_set_dl_loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = \"../../data/test_data/gdf_pop_1pm_policy_in_1_2_3_4.geojson\"\n",
    "test_data = gpd.read_file(test_data)\n",
    "base_case = \"../../data/test_data/gdf_basecase_mean_pop_1pm.geojson\"\n",
    "base_case = gpd.read_file(base_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 59\n",
    "loss_fct= torch.nn.MSELoss()\n",
    "test_loss, r_squared, actual_vals, predictions, baseline_loss = validate_one_model(model, test_set_loader.dataset[i], loss_fct, device)\n",
    "\n",
    "gdf = hf.data_to_geodataframe(data=test_set_loader.dataset[0], original_gdf=test_data, predicted_values=predictions)\n",
    "gdf_with_og_values = hf.map_to_original_values(input_gdf=gdf, scaler_x=scaler_x)\n",
    "gdf_with_og_values.crs = \"EPSG:2154\"\n",
    "gdf_with_og_values.to_crs(\"EPSG:4326\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_roads_with_highway_primary_sec_tertiary = gdf_with_og_values[gdf_with_og_values['og_highway'].isin([1,2,3])].index\n",
    "indices_roads_with_highway_primary_ = gdf_with_og_values[gdf_with_og_values['og_highway'].isin([1])].index\n",
    "indices_roads_with_highway_sec = gdf_with_og_values[gdf_with_og_values['og_highway'].isin([2])].index\n",
    "indices_roads_with_highway_tertiary = gdf_with_og_values[gdf_with_og_values['og_highway'].isin([3])].index\n",
    "\n",
    "indices_roads_with_highway_not_primary_sec_tertiary = gdf_with_og_values[~gdf_with_og_values['og_highway'].isin([1, 2, 3])].index\n",
    "\n",
    "gdf_with_og_values['og_capacity_reduction_rounded'] = gdf_with_og_values['og_capacity_reduction'].round(decimals=3)\n",
    "tolerance = 1e-3\n",
    "indices_roads_with_cap_reduction = gdf_with_og_values[gdf_with_og_values['og_capacity_reduction_rounded'] < -1e-3].index\n",
    "indices_roads_with_no_cap_reduction = gdf_with_og_values[gdf_with_og_values['og_capacity_reduction_rounded'] >= -1e-3].index\n",
    "\n",
    "indices_roads_with_highway_primary_sec_tertiary_and_cap_reduction = gdf_with_og_values[\n",
    "    (gdf_with_og_values['og_highway'].isin([1, 2, 3])) & \n",
    "    (gdf_with_og_values['og_capacity_reduction_rounded'] < -1e-3)\n",
    "].index\n",
    "indices_roads_with_highway_primary_sec_tertiary_and_not_cap_reduction = gdf_with_og_values[\n",
    "    (gdf_with_og_values['og_highway'].isin([1, 2, 3])) & \n",
    "    (gdf_with_og_values['og_capacity_reduction_rounded'] >= -1e-3)\n",
    "].index\n",
    "\n",
    "no_filter = gdf_with_og_values.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts_of_interest = [1, 2, 3, 4, 6, 8, 9, 11, 14, 20]\n",
    "districts = gpd.read_file(\"../../data/visualisation/districts_paris.geojson\")\n",
    "target_districts = districts[districts['c_ar'].isin(districts_of_interest)]\n",
    "gdf_with_og_values['intersects_target_districts'] = gdf_with_og_values.apply(lambda row: target_districts.intersects(row.geometry).any(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(12.1289, device='cuda:0')\n",
      "tensor(5.4791, device='cuda:0')\n",
      "tensor(0.5483, device='cuda:0')\n",
      "variance: 785.4876098632812\n"
     ]
    }
   ],
   "source": [
    "indices_this_zone = gdf_with_og_values[gdf_with_og_values['intersects_target_districts']].index\n",
    "overlap = indices_this_zone.intersection(indices_roads_with_highway_primary_sec_tertiary)\n",
    "\n",
    "indices_to_filter_for = overlap\n",
    "\n",
    "filtered_actual = actual_vals[indices_to_filter_for]\n",
    "filtered_actual_mean = torch.mean(filtered_actual)\n",
    "filtered_predicted = predictions[indices_to_filter_for]\n",
    "\n",
    "mse_filtered = loss_fct(filtered_actual, filtered_predicted)\n",
    "baseline_filtered = loss_fct(filtered_actual, torch.full_like(filtered_actual, filtered_actual_mean))\n",
    "loss_fct_aux = torch.nn.MSELoss(reduction='none')\n",
    "variance = torch.var(loss_fct_aux(filtered_actual, torch.full_like(filtered_actual, filtered_actual_mean)))\n",
    "r_squared = hf.compute_r2_torch(preds=filtered_predicted, targets=filtered_actual)\n",
    "print(baseline_filtered)\n",
    "print(mse_filtered)\n",
    "print(r_squared)\n",
    "loss_fct_aux = torch.nn.MSELoss(reduction='none')\n",
    "variance = torch.var(loss_fct_aux(filtered_actual, torch.full_like(filtered_actual, filtered_actual_mean)))\n",
    "print(f'variance: {variance}')\n",
    "\n",
    "\n",
    "# actual_mean = torch.mean(actual_vals)\n",
    "# variance = torch.var(loss_fct_aux(actual_vals, torch.full_like(actual_vals, actual_mean)))\n",
    "# print(f'Test Loss: {test_loss}')\n",
    "# print(f'Baseline Loss: {baseline_loss}')\n",
    "# print(f'r_squared: {r_squared}')\n",
    "# print(f'variance: {variance}')\n",
    "# # print(\"Number of values where 'og_capacity_reduction' is not 0 with tolerance 1e-3:\", num_values_not_zero)\n",
    "\n",
    "# gdf_in_meters = gdf_with_og_values.to_crs(\"EPSG:32633\")\n",
    "# gdf_in_meters.length\n",
    "\n",
    "# tolerance = 1e-3\n",
    "# gdf_with_capacity_reduction = gdf_in_meters[abs(gdf_in_meters['og_capacity_reduction']) > tolerance]\n",
    "# gdf_with_capacity_reduction['length'] = gdf_with_capacity_reduction.length\n",
    "# total_length = gdf_with_capacity_reduction['length'].sum()/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fct = torch.nn.MSELoss()\n",
    "\n",
    "# test_loss, r_squared, actual_vals, predictions, baseline_loss = validate_trained_model(model, test_set_loader.dataset, loss_fct, device)\n",
    "# print(f'Test Loss: {test_loss}')\n",
    "# print(f'r_squared: {r_squared}')\n",
    "# print(f'baseline_loss: {baseline_loss}')\n",
    "\n",
    "# print(f'actual_vals shape: {len(actual_vals)}')\n",
    "# print(f'predictions shape: {len(predictions)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter list of tensors\n",
    "# def filter_tensors_by_indices(tensor_list, indices):\n",
    "#     return [tensor[indices] for tensor in tensor_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure actual_vals and predictions are tensors\n",
    "# if isinstance(actual_vals, list):\n",
    "#     actual_vals = torch.tensor(actual_vals)\n",
    "# if isinstance(predictions, list):\n",
    "#     predictions = torch.tensor(predictions)\n",
    "# THIS IS FOR WHOLE TEST SET ! \n",
    "\n",
    "\n",
    "# # Filter actual and predicted values\n",
    "# filtered_actual = filter_tensors_by_indices(actual_vals, indices_to_filter_for)\n",
    "# filtered_predicted = filter_tensors_by_indices(predictions, indices_to_filter_for)\n",
    "\n",
    "# # Concatenate filtered tensors for computing metrics\n",
    "# filtered_actual_concat = torch.cat(filtered_actual)\n",
    "# filtered_predicted_concat = torch.cat(filtered_predicted)\n",
    "\n",
    "# filtered_actual_mean = torch.mean(filtered_actual_concat)\n",
    "# # filtered_actual = actual_vals[indices_to_filter_for]\n",
    "# # filtered_actual_mean = torch.mean(filtered_actual)\n",
    "# # filtered_predicted = predictions[indices_to_filter_for]\n",
    "\n",
    "# mse_filtered = loss_fct(filtered_actual_concat, filtered_predicted_concat)\n",
    "# baseline_filtered = loss_fct(filtered_actual_concat, torch.full_like(filtered_actual_concat, filtered_actual_mean))\n",
    "# r_squared = compute_r2_torch(preds=filtered_predicted_concat, targets=filtered_actual_concat)\n",
    "# loss_fct_aux = torch.nn.MSELoss(reduction='none')\n",
    "# variance = torch.var(loss_fct_aux(filtered_actual_concat, torch.full_like(filtered_actual_concat, filtered_actual_mean)))\n",
    "\n",
    "# print(f'variance: {variance}')\n",
    "# print(f'Baseline Loss: {baseline_filtered}')\n",
    "# print(f'Test Loss: {mse_filtered}')\n",
    "# print(f'r_squared: {r_squared}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_filter_for = no_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gdf = hf.data_to_geodataframe(data=test_set_loader.dataset[0], original_gdf=test_data, predicted_values=predictions)\n",
    "# # gdf_with_og_values = hf.map_to_original_values(input_gdf=gdf, scaler_x=scaler_x)\n",
    "# # gdf_with_og_values.crs = \"EPSG:2154\"\n",
    "# # gdf_with_og_values.to_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "# gdf_in_meters = gdf_with_og_values.to_crs(\"EPSG:32633\")\n",
    "# gdf_in_meters.length\n",
    "\n",
    "# tolerance = 1e-3\n",
    "# gdf_with_capacity_reduction = gdf_in_meters.loc[indices_to_filter_for]\n",
    "# gdf_with_capacity_reduction['length'] = gdf_with_capacity_reduction.length\n",
    "# total_length = gdf_with_capacity_reduction['length'].sum()/1000\n",
    "# print(total_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/geopandas/geodataframe.py:1525: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "gdf_in_meters = gdf_with_og_values.to_crs(\"EPSG:32633\")\n",
    "gdf_in_meters.length\n",
    "\n",
    "tolerance = 1e-3\n",
    "gdf_with_capacity_reduction = gdf_in_meters[abs(gdf_in_meters['og_capacity_reduction']) > tolerance]\n",
    "gdf_with_capacity_reduction['length'] = gdf_with_capacity_reduction.length\n",
    "total_length = gdf_with_capacity_reduction['length'].sum()/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4492"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gdf_with_capacity_reduction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenhao-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

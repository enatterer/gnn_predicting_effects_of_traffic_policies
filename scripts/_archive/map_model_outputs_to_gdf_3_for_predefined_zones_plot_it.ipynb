{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import wandb\n",
    "import pickle\n",
    "import os\n",
    "import shapely.wkt as wkt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "from torch_geometric.transforms import LineGraph\n",
    "\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import processing_io as pio\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from shapely.geometry import Point, LineString, box\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "from shapely.ops import unary_union\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from torch_geometric.data import Data, Batch\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import alphashape\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "\n",
    "districts = gpd.read_file(\"../../data/visualisation/districts_paris.geojson\")\n",
    "\n",
    "# Add the 'scripts' directory to the Python path\n",
    "scripts_path = os.path.abspath(os.path.join('..'))\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "\n",
    "import gnn_io as gio\n",
    "import gnn_architectures as garch\n",
    "import help_functions as hf\n",
    "\n",
    "import copy\n",
    "\n",
    "def replace_invalid_values(tensor):\n",
    "    tensor[tensor != tensor] = 0  # replace NaNs with 0\n",
    "    tensor[tensor == float('inf')] = 0  # replace inf with 0\n",
    "    tensor[tensor == float('-inf')] = 0  # replace -inf with 0\n",
    "    return tensor\n",
    "\n",
    "def plot_combined_output(gdf_input: gpd.GeoDataFrame, column_to_plot: str, font: str = 'Times New Roman', \n",
    "                         save_it: bool = False, number_to_plot: int = 0,\n",
    "                         zone_to_plot:str= \"this_zone\",\n",
    "                         is_predicted: bool = False, alpha:int=100, \n",
    "                         use_fixed_norm:bool=True, \n",
    "                         fixed_norm_max: int= 10, normalized_y:bool=False, known_districts:bool=False, buffer: float = 0.0005, districts_of_interest: list =[1, 2, 3, 4]):\n",
    "    # call with known_districts if call with 0 or 1\n",
    "\n",
    "    gdf = gdf_input.copy()\n",
    "    gdf, x_min, y_min, x_max, y_max = filter_for_geographic_section(gdf)\n",
    "    # gdf = gdf[gdf[\"og_highway\"].isin([1])]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 15))    \n",
    "    norm = get_norm(column_to_plot=column_to_plot, use_fixed_norm=use_fixed_norm, fixed_norm_max=fixed_norm_max, gdf=gdf)\n",
    "    relevant_area_to_plot = get_relevant_area_to_plot(alpha, known_districts, buffer, districts_of_interest, gdf, ax, column_to_plot, norm, \"og_highway\")\n",
    "    relevant_area_to_plot.plot(ax=ax, edgecolor='black', linewidth=2, facecolor='None', zorder=2)\n",
    "\n",
    "    cbar = plotting(font, x_min, y_min, x_max, y_max, fig, ax, norm)\n",
    "    \n",
    "    cbar.set_label('Car volume: Difference to base case (%)', fontname=font, fontsize=15)\n",
    "    if save_it:\n",
    "        p = \"predicted\" if is_predicted else \"actual\"\n",
    "        identifier = \"n_\" + str(number_to_plot) if number_to_plot is not None else zone_to_plot\n",
    "        plt.savefig(\"results/\" + identifier + \"_\" + p, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def get_norm(column_to_plot, use_fixed_norm, fixed_norm_max, gdf):\n",
    "    if use_fixed_norm:\n",
    "        norm = TwoSlopeNorm(vmin=-fixed_norm_max, vcenter=0, vmax=fixed_norm_max)\n",
    "    else:\n",
    "        norm = TwoSlopeNorm(vmin=gdf[column_to_plot].min(), vcenter=gdf[column_to_plot].median(), vmax=gdf[column_to_plot].max())\n",
    "    return norm\n",
    "    \n",
    "# def plot_difference_output(gdf_input: gpd.GeoDataFrame, column1: str, column2: str, diff_column: str = 'difference', font: str = 'Times New Roman', save_it: bool = False, number_to_plot: int = 0,\n",
    "#                            zone_to_plot:str= \"this_zone\", alpha:int=100, \n",
    "#                          use_fixed_norm:bool=True, \n",
    "#                          fixed_norm_max: int= 10, normalized_y: bool=False, known_districts:bool=False, buffer: float = 0.0005, districts_of_interest: list =[1, 2, 3, 4]):\n",
    "#     gdf = gdf_input.copy()\n",
    "#     gdf[diff_column] = gdf[column1] - gdf[column2]\n",
    "#     column_to_plot = diff_column\n",
    "\n",
    "#     gdf, x_min, y_min, x_max, y_max = filter_for_geographic_section(gdf)\n",
    "\n",
    "#     fig, ax = plt.subplots(1, 1, figsize=(15, 15))    \n",
    "#     norm = get_norm(column_to_plot=column_to_plot, use_fixed_norm=use_fixed_norm, fixed_norm_max=fixed_norm_max, gdf=gdf)\n",
    "#     relevant_area_to_plot = get_relevant_area_to_plot(alpha, known_districts, buffer, districts_of_interest, gdf, ax, column_to_plot, norm, \"og_highway\")\n",
    "#     relevant_area_to_plot.plot(ax=ax, edgecolor='black', linewidth=2, facecolor='None', zorder=2)\n",
    "\n",
    "#     cbar = plotting(font, x_min, y_min, x_max, y_max, fig, ax, norm)\n",
    "#     cbar.set_label('Difference between predicted and actual (%)', fontname=font, fontsize=15)\n",
    "#     if save_it:\n",
    "#         identifier = \"n_\" + str(number_to_plot) if number_to_plot is not None else zone_to_plot\n",
    "#         plt.savefig(\"results/\" + identifier  + \"_difference\", bbox_inches='tight')\n",
    "    # plt.show()\n",
    "\n",
    "def filter_for_geographic_section(gdf):\n",
    "    x_min = gdf.total_bounds[0] + 0.05\n",
    "    y_min = gdf.total_bounds[1] + 0.05\n",
    "    x_max = gdf.total_bounds[2]\n",
    "    y_max = gdf.total_bounds[3]\n",
    "    bbox = box(x_min, y_min, x_max, y_max)\n",
    "\n",
    "    # Filter the network to include only the data within the bounding box\n",
    "    gdf = gdf[gdf.intersects(bbox)]\n",
    "    return gdf,x_min,y_min,x_max,y_max\n",
    "\n",
    "def plotting(font, x_min, y_min, x_max, y_max, fig, ax, norm):\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel(\"Longitude\", fontname=font, fontsize=15)\n",
    "    plt.ylabel(\"Latitude\", fontname=font, fontsize=15)\n",
    "\n",
    "    # Customize tick labels\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontname(font)\n",
    "        label.set_fontsize(15)\n",
    "    \n",
    "    # Create custom legend\n",
    "    custom_lines = [Line2D([0], [0], color='grey', lw=4, label='Street network'),# Add more lines for other labels as needed\n",
    "                    Line2D([0], [0], color='black', lw=2, label='Capacity was decreased in this section')]\n",
    "\n",
    "    ax.legend(handles=custom_lines, prop={'family': font, 'size': 15})\n",
    "    ax.set_position([0.1, 0.1, 0.75, 0.75])\n",
    "    cax = fig.add_axes([0.87, 0.22, 0.03, 0.5])  # Manually position the color bar\n",
    "    \n",
    "    # Create the color bar\n",
    "    sm = plt.cm.ScalarMappable(cmap='coolwarm', norm=norm)\n",
    "    sm._A = []\n",
    "    cbar = plt.colorbar(sm, cax=cax)\n",
    "\n",
    "    # Set color bar font properties\n",
    "    cbar.ax.tick_params(labelsize=15)\n",
    "    for t in cbar.ax.get_yticklabels():\n",
    "        t.set_fontname(font)\n",
    "    cbar.ax.yaxis.label.set_fontname(font)\n",
    "    cbar.ax.yaxis.label.set_size(15)\n",
    "    return cbar\n",
    "\n",
    "def get_relevant_area_to_plot(alpha, known_districts, buffer, districts_of_interest, gdf, ax, column_to_plot, norm, highway_column):\n",
    "    if known_districts:\n",
    "        \n",
    "        # Apply the linewidth mapping\n",
    "        linewidths = gdf[highway_column].apply(get_linewidth)\n",
    "        gdf['linewidth'] = linewidths\n",
    "        # Separate the GeoDataFrame into two groups based on linewidth\n",
    "        large_lines = gdf[gdf['linewidth'] > 1]\n",
    "        small_lines = gdf[gdf['linewidth'] == 1]\n",
    "        \n",
    "        target_districts = districts[districts['c_ar'].isin(districts_of_interest)]\n",
    "        gdf['intersects_target_districts'] = gdf.apply(lambda row: target_districts.intersects(row.geometry).any(), axis=1)\n",
    "        \n",
    "        # Plot small lines first\n",
    "        small_lines.plot(column=column_to_plot, cmap='coolwarm', linewidth=small_lines['linewidth'], ax=ax, legend=False,\n",
    "                        norm=norm, label=\"Street network\", zorder=1)\n",
    "        \n",
    "        # Plot large lines after\n",
    "        large_lines.plot(column=column_to_plot, cmap='coolwarm', linewidth=large_lines['linewidth'], ax=ax, legend=False,\n",
    "                        norm=norm, label=\"Street network\", zorder=2)\n",
    "        \n",
    "        buffered_target_districts = target_districts.copy()\n",
    "        buffered_target_districts['geometry'] = buffered_target_districts.buffer(buffer)\n",
    "        if buffered_target_districts.crs != gdf.crs:\n",
    "            buffered_target_districts.to_crs(gdf.crs, inplace=True)\n",
    "        outer_boundary = unary_union(buffered_target_districts.geometry).boundary\n",
    "        relevant_area_to_plot = gpd.GeoSeries(outer_boundary, crs=gdf.crs)\n",
    "        \n",
    "    else:\n",
    "        gdf['og_capacity_reduction_rounded'] = gdf['og_capacity_reduction'].round(decimals=3)\n",
    "        tolerance = 1e-3\n",
    "        edges_with_capacity_reduction = gdf[np.abs(gdf['og_capacity_reduction_rounded']) > tolerance]\n",
    "        coords = [(x, y) for geom in edges_with_capacity_reduction.geometry for x, y in zip(geom.xy[0], geom.xy[1])]\n",
    "        alpha_shape = alphashape.alphashape(coords, alpha)\n",
    "        relevant_area_to_plot = gpd.GeoSeries([alpha_shape], crs=gdf.crs)\n",
    "    return relevant_area_to_plot\n",
    "\n",
    "def get_linewidth(value):\n",
    "        if value in [0, 1]:\n",
    "            return 5\n",
    "        elif value == 2:\n",
    "            return 3\n",
    "        elif value == 3:\n",
    "            return 2\n",
    "        else:\n",
    "            return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_path = '/home/enatterer/Development/gnn_predicting_effects_of_traffic_policies/data/runs_optimized/pnc_local_[256]_pnc_global_[512_256]_hidden_layer_str_[512_512_256_128_64]_dropout_0.3_use_dropout_False/'\n",
    "point_net_conv_layer_structure_local_mlp = [256]\n",
    "point_net_conv_layer_structure_global_mlp = [512,256]\n",
    "gat_conv_layer_structure = [512,512,256,128,64]\n",
    "dropout = 0.3\n",
    "use_dropout = False \n",
    "in_channels = 6 \n",
    "out_channels = 1 \n",
    "\n",
    "districts_of_interest = [1, 2, 3, 4]\n",
    "zone_to_plot = \"zone_1\"\n",
    "test_data = \"../../data/test_data/gdf_pop_1pm_policy_in_1_2_3_4.geojson\"\n",
    "test_data = gpd.read_file(test_data)\n",
    "base_case = \"../../data/test_data/gdf_basecase_mean_pop_1pm.geojson\"\n",
    "base_case = gpd.read_file(base_case)    \n",
    "    \n",
    "model_path = run_path +  'trained_model/model.pth'\n",
    "data_created_during_training = run_path + 'data_created_during_training/'\n",
    "indices_of_datasets_to_use = [0, 1, 3, 4]\n",
    "\n",
    "scaler_x = joblib.load(data_created_during_training + 'x_scaler.pkl')\n",
    "scaler_pos = joblib.load(data_created_during_training + 'pos_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PointNetConv(local_nn=Sequential(\n",
      "  (0): Linear(in_features=6, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "), global_nn=Sequential(\n",
      "  (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "  (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "  (4): ReLU()\n",
      "))\n",
      "Initializing 0.weight with kaiming_normal\n",
      "Initializing 0.bias with zeros\n",
      "Initializing 0.weight with kaiming_normal\n",
      "Initializing 0.bias with zeros\n",
      "Initializing 1.weight with kaiming_normal\n",
      "Initializing 1.bias with zeros\n",
      "Initializing 3.weight with kaiming_normal\n",
      "Initializing 3.bias with zeros\n",
      "Initializing Linear(in_features=6, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=512, bias=True)\n",
      "Initializing Linear(in_features=512, out_features=256, bias=True)\n",
      "Initializing Linear(in_features=256, out_features=512, bias=True)\n",
      "Initializing GATConv(512, 512, heads=1)\n",
      "Initializing GATConv(512, 256, heads=1)\n",
      "Initializing GATConv(256, 128, heads=1)\n",
      "Initializing GATConv(128, 64, heads=1)\n",
      "Initializing GATConv(64, 1, heads=1)\n",
      "Model initialized\n",
      "MyGnn(\n",
      "  (point_net_layer): PointNetConv(local_nn=Sequential(\n",
      "    (0): Linear(in_features=6, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "  ), global_nn=Sequential(\n",
      "    (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (graph_layers): Sequential(\n",
      "    (0) - GATConv(512, 512, heads=1): x, edge_index -> x\n",
      "    (1) - ReLU(inplace=True): x -> x\n",
      "    (2) - GATConv(512, 256, heads=1): x, edge_index -> x\n",
      "    (3) - ReLU(inplace=True): x -> x\n",
      "    (4) - GATConv(256, 128, heads=1): x, edge_index -> x\n",
      "    (5) - ReLU(inplace=True): x -> x\n",
      "    (6) - GATConv(128, 64, heads=1): x, edge_index -> x\n",
      "    (7) - ReLU(inplace=True): x -> x\n",
      "    (8) - GATConv(64, 1, heads=1): x, edge_index -> x\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = garch.MyGnn(in_channels=in_channels, out_channels=out_channels, \n",
    "                    point_net_conv_layer_structure_local_mlp=point_net_conv_layer_structure_local_mlp, \n",
    "                    point_net_conv_layer_structure_global_mlp = point_net_conv_layer_structure_global_mlp,\n",
    "                    gat_conv_layer_structure=gat_conv_layer_structure,\n",
    "                    dropout=dropout,\n",
    "                    use_dropout=use_dropout)\n",
    "\n",
    "# Load the model state dictionary\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_linegraph = pio.create_test_data_object(base_case=base_case, test_data = test_data) # check this function if there have been changes in the features of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_list = [test_input_linegraph] # we do it for just one test data object, for now.\n",
    "dataset_only_relevant_dimensions = gio.cut_dimensions(dataset=test_data_list, indices_of_dimensions_to_keep=indices_of_datasets_to_use)\n",
    "test_subset = Subset(dataset_only_relevant_dimensions[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_one_dataset_given_scaler(dataset_input, x_scalar_list = None, pos_scalar=None):\n",
    "    dataset = normalize_x_values_given_scaler(dataset_input, x_scalar_list)\n",
    "    dataset.pos = torch.tensor(pos_scalar.transform(dataset.pos.numpy()), dtype=torch.float)\n",
    "    return dataset\n",
    "\n",
    "def normalize_x_values_given_scaler(dataset, x_scaler_list):\n",
    "    for i in range(4):\n",
    "        scaler = x_scaler_list[i]\n",
    "        data_x_dim = replace_invalid_values(dataset.x[:, i].reshape(-1, 1))\n",
    "        normalized_x_dim = torch.tensor(scaler.transform(data_x_dim.numpy()), dtype=torch.float)\n",
    "        dataset.x[:, i]=  normalized_x_dim.squeeze()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset length: 5\n"
     ]
    }
   ],
   "source": [
    "dataset_normalized = normalize_one_dataset_given_scaler(dataset_input=test_subset.dataset, x_scalar_list=scaler_x, pos_scalar=scaler_pos)\n",
    "test_dl = gio.create_dataloader(dataset=dataset_normalized, is_train=False, batch_size=8, train_ratio=0, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_r2_torch_with_mean_targets(mean_targets, preds, targets):\n",
    "    ss_tot = torch.sum((targets - mean_targets) ** 2)\n",
    "    ss_res = torch.sum((targets - preds) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2\n",
    "\n",
    "def validate_one_model(model, data, loss_func, device):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    actual = []\n",
    "    with torch.inference_mode():\n",
    "        input_node_features, targets = data.x.to(device), data.y.to(device)\n",
    "        predicted = model(data.to(device))\n",
    "        # print(predicted.shape)\n",
    "        pred.append(predicted)\n",
    "        actual.append(targets)\n",
    "        val_loss = loss_func(predicted, targets).item()\n",
    "    actual_vals = torch.cat(actual)\n",
    "    predicted_vals = torch.cat(pred)\n",
    "    \n",
    "    mean_targets = torch.mean(targets)\n",
    "    r_squared = compute_r2_torch_with_mean_targets(mean_targets = mean_targets, preds=predicted_vals, targets=actual_vals)\n",
    "    baseline_loss = loss_func(targets, torch.full_like(predicted_vals, mean_targets))\n",
    "    return val_loss, r_squared, targets, predicted, baseline_loss\n",
    "\n",
    "\n",
    "# loss_fct= torch.nn.MSELoss()\n",
    "# test_loss, r_squared, actual_vals, predictions, baseline_loss = validate_one_model(model, dataset_normalized, loss_fct, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31216, 1])\n",
      "Test Loss: 1.0067453384399414\n",
      "r_squared: -0.8707122802734375\n",
      "Baseline Loss: 0.5381614565849304\n"
     ]
    }
   ],
   "source": [
    "def validate_one_model(model, data, loss_func, device):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    actual = []\n",
    "    with torch.inference_mode():\n",
    "        input_node_features, targets = data.x.to(device), data.y.to(device)\n",
    "        predicted = model(data.to(device))\n",
    "        # print(predicted.shape)\n",
    "        pred.append(predicted)\n",
    "        actual.append(targets)\n",
    "        val_loss = loss_func(predicted, targets).item()\n",
    "    actual_vals = torch.cat(actual)\n",
    "    predicted_vals = torch.cat(pred)\n",
    "    r_squared = compute_r2_torch(preds=predicted_vals, targets=actual_vals)\n",
    "    mean_targets = torch.mean(targets)\n",
    "    baseline_loss = loss_func(targets, torch.full_like(predicted_vals, mean_targets))\n",
    "    return val_loss, r_squared, targets, predicted, baseline_loss\n",
    "\n",
    "def compute_r2_torch(preds, targets):\n",
    "    \"\"\"Compute R^2 score using PyTorch.\"\"\"\n",
    "    print(targets.shape)\n",
    "    mean_targets = torch.mean(targets)\n",
    "    ss_tot = torch.sum((targets - mean_targets) ** 2)\n",
    "    ss_res = torch.sum((targets - preds) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2\n",
    "\n",
    "loss_fct = torch.nn.MSELoss()\n",
    "loss_fct_aux = torch.nn.MSELoss(reduce=None)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "test_loss, r_squared, actual_vals, predictions, baseline_loss = validate_one_model(model, dataset_normalized, loss_fct, device)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'r_squared: {r_squared}')\n",
    "print(f'Baseline Loss: {baseline_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_geodataframe(data, original_gdf, predicted_values):\n",
    "    # Extract the edge index and node features\n",
    "    node_features = data.x.cpu().numpy()\n",
    "    target_values = data.y.cpu().numpy()\n",
    "    predicted_values = predicted_values.cpu().numpy() if isinstance(predicted_values, torch.Tensor) else predicted_values\n",
    "\n",
    "    # Create edge data\n",
    "    edge_data = {\n",
    "        'from_node': original_gdf[\"from_node\"].values,\n",
    "        'to_node': original_gdf[\"to_node\"].values,\n",
    "        'vol_base_case': node_features[:, 0],  # Assuming capacity is the first feature, and so on\n",
    "        'capacity_base_case': node_features[:, 1],  \n",
    "        'capacity_reduction': node_features[:, 2],  \n",
    "        'highway': node_features[:, 3],  \n",
    "        'vol_car_change_actual': target_values.squeeze(),  # Assuming target values are car volumes\n",
    "        'vol_car_change_predicted': predicted_values.squeeze()\n",
    "    }\n",
    "    # Convert to DataFrame\n",
    "    edge_df = pd.DataFrame(edge_data)\n",
    "    # Create LineString geometry\n",
    "    edge_df['geometry'] = original_gdf[\"geometry\"].values\n",
    "    # Create GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(edge_df, geometry='geometry')\n",
    "    return gdf\n",
    "\n",
    "\n",
    "gdf = data_to_geodataframe(data=dataset_normalized, original_gdf=test_data, predicted_values=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_with_og_values = hf.map_to_original_values(input_gdf=gdf, scaler_x=scaler_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices_roads_with_highway_primary_sec_tertiary = gdf_with_og_values[gdf_with_og_values['og_highway'].isin([1,2,3])].index\n",
    "# indices_roads_with_highway_primary_ = gdf_with_og_values[gdf_with_og_values['og_highway'].isin([1])].index\n",
    "# indices_roads_with_highway_sec = gdf_with_og_values[gdf_with_og_values['og_highway'].isin([2])].index\n",
    "# indices_roads_with_highway_tertiary = gdf_with_og_values[gdf_with_og_values['og_highway'].isin([3])].index\n",
    "\n",
    "# indices_roads_with_highway_not_primary_sec_tertiary = gdf_with_og_values[~gdf_with_og_values['og_highway'].isin([1, 2, 3])].index\n",
    "\n",
    "# gdf_with_og_values['og_capacity_reduction_rounded'] = gdf_with_og_values['og_capacity_reduction'].round(decimals=3)\n",
    "# tolerance = 1e-3\n",
    "# indices_roads_with_cap_reduction = gdf_with_og_values[gdf_with_og_values['og_capacity_reduction_rounded'] < -1e-3].index\n",
    "# indices_roads_with_no_cap_reduction = gdf_with_og_values[gdf_with_og_values['og_capacity_reduction_rounded'] >= -1e-3].index\n",
    "\n",
    "# indices_roads_with_highway_primary_sec_tertiary_and_cap_reduction = gdf_with_og_values[\n",
    "#     (gdf_with_og_values['og_highway'].isin([1, 2, 3])) & \n",
    "#     (gdf_with_og_values['og_capacity_reduction_rounded'] < -1e-3)\n",
    "# ].index\n",
    "# indices_roads_with_highway_primary_sec_tertiary_and_not_cap_reduction = gdf_with_og_values[\n",
    "#     (gdf_with_og_values['og_highway'].isin([1, 2, 3])) & \n",
    "#     (gdf_with_og_values['og_capacity_reduction_rounded'] >= -1e-3)\n",
    "# ].index\n",
    "\n",
    "\n",
    "# # indices_to_filter_for = indices_roads_with_highway_primary_\n",
    "# # filtered_actual = actual_vals[indices_to_filter_for]\n",
    "# # filtered_actual_mean = torch.mean(filtered_actual)\n",
    "# # filtered_predicted = predictions[indices_to_filter_for]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts = gpd.read_file(\"../../data/visualisation/districts_paris.geojson\")\n",
    "target_districts = districts[districts['c_ar'].isin(districts_of_interest)]\n",
    "gdf_with_og_values['intersects_target_districts'] = gdf_with_og_values.apply(lambda row: target_districts.intersects(row.geometry).any(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1820, 1])\n",
      "tensor(2.4753, device='cuda:0')\n",
      "tensor(1.6242, device='cuda:0')\n",
      "tensor(0.3439, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_904363/2492015623.py:11: UserWarning: var(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /opt/conda/conda-bld/pytorch_1704987280714/work/aten/src/ATen/native/ReduceOps.cpp:1760.)\n",
      "  variance = torch.var(loss_fct_aux(filtered_actual, torch.full_like(filtered_actual, filtered_actual_mean)))\n"
     ]
    }
   ],
   "source": [
    "indices_this_zone = gdf_with_og_values[gdf_with_og_values['intersects_target_districts']].index\n",
    "# # overlap = indices_this_zone.intersection(indices_roads_with_highway_primary_)\n",
    "\n",
    "indices_to_filter_for = indices_this_zone\n",
    "filtered_actual = actual_vals[indices_to_filter_for]\n",
    "filtered_actual_mean = torch.mean(filtered_actual)\n",
    "filtered_predicted = predictions[indices_to_filter_for]\n",
    "\n",
    "mse_filtered = loss_fct(filtered_actual, filtered_predicted)\n",
    "baseline_filtered = loss_fct(filtered_actual, torch.full_like(filtered_actual, filtered_actual_mean))\n",
    "variance = torch.var(loss_fct_aux(filtered_actual, torch.full_like(filtered_actual, filtered_actual_mean)))\n",
    "r_squared = compute_r2_torch(preds=filtered_predicted, targets=filtered_actual)\n",
    "print(baseline_filtered)\n",
    "print(mse_filtered)\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(loss_fct(predictions, actual_vals))\n",
    "# mean_value = torch.mean(actual_vals)\n",
    "# print(mean_value)\n",
    "# print(loss_fct(predictions, torch.full_like(predictions, mean_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "fixed_norm_max = 3\n",
    "gdf = hf.data_to_geodataframe(data=test_dl.dataset, original_gdf=test_data, predicted_values=predictions)\n",
    "gdf_with_og_values = hf.map_to_original_values(input_gdf=gdf, scaler_x =scaler_x)\n",
    "# plot_combined_output(gdf_input=gdf_with_og_values, column_to_plot=\"og_vol_car_change_predicted\", \n",
    "#                         save_it=True, number_to_plot=None, zone_to_plot = zone_to_plot, is_predicted=True, alpha=0, use_fixed_norm=True, \n",
    "#                         fixed_norm_max = fixed_norm_max,\n",
    "#                         known_districts = True, buffer = 0.0005, districts_of_interest=districts_of_interest)\n",
    "# plot_combined_output(gdf_input=gdf_with_og_values, column_to_plot=\"og_vol_car_change_actual\", save_it=True, \n",
    "#                         number_to_plot=None, zone_to_plot = zone_to_plot,is_predicted=False,alpha=10,use_fixed_norm=True, \n",
    "#                         fixed_norm_max = fixed_norm_max,\n",
    "#                         known_districts = True, buffer = 0.0005, districts_of_interest=districts_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0067453384399414\n",
      "Baseline Loss: 0.5381614565849304\n",
      "r_squared: 0.3438594341278076\n",
      "variance: 50.607879638671875\n"
     ]
    }
   ],
   "source": [
    "loss_fct_aux = torch.nn.MSELoss(reduction='none')\n",
    "# actual_mean = torch.mean(actual_vals)\n",
    "variance = torch.var(loss_fct_aux(filtered_actual, torch.full_like(filtered_actual, filtered_actual_mean)))\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'Baseline Loss: {baseline_loss}')\n",
    "print(f'r_squared: {r_squared}')\n",
    "print(f'variance: {variance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/geopandas/geodataframe.py:1525: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "gdf_in_meters = gdf_with_og_values.to_crs(\"EPSG:32633\")\n",
    "gdf_in_meters.length\n",
    "\n",
    "tolerance = 1e-3\n",
    "gdf_with_capacity_reduction = gdf_in_meters[abs(gdf_in_meters['og_capacity_reduction']) > tolerance]\n",
    "gdf_with_capacity_reduction['length'] = gdf_with_capacity_reduction.length\n",
    "total_length = gdf_with_capacity_reduction['length'].sum()/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.03149543114152"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "596"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gdf_with_capacity_reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenhao-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

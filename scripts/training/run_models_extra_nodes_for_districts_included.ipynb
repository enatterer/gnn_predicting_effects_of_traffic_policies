{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import wandb\n",
    "import random\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import signal\n",
    "import joblib\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import help_functions as hf\n",
    "\n",
    "import psutil\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "scripts_path = os.path.abspath(os.path.join('..'))\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "    \n",
    "import gnn_io as gio\n",
    "import gnn_architectures_district_features as garch\n",
    "import copy\n",
    "from torch_geometric.utils import to_undirected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is current working status (11.10.2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def normalize_dataset(dataset_input, directory_path):\n",
    "    try:\n",
    "        print(f\"Starting normalization for {len(dataset_input)} items\")\n",
    "        dataset = copy_subset(dataset_input)\n",
    "        print(\"Dataset copied successfully\")\n",
    "        \n",
    "        dataset = normalize_x_values(dataset, directory_path)\n",
    "        print(\"X values normalized successfully\")\n",
    "        \n",
    "        dataset = normalize_positional_features(dataset, directory_path)\n",
    "        print(\"Positional features normalized successfully\")\n",
    "        \n",
    "        dataset = normalize_mode_stats(dataset, directory_path)\n",
    "        print(\"Mode stats normalized successfully\")\n",
    "        \n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error in normalize_dataset: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    \n",
    "\n",
    "# Call this function during training without the scalars and with the directory path, and during the testing with the saved scalars and without a directory path to save.\n",
    "# def normalize_dataset(dataset_input, directory_path):\n",
    "#     dataset = copy_subset(dataset_input)\n",
    "#     dataset = normalize_x_values(dataset, directory_path)\n",
    "#     dataset = normalize_positional_features(dataset, directory_path)\n",
    "#     dataset = normalize_mode_stats(dataset, directory_path)\n",
    "#     return dataset\n",
    "    \n",
    "    \n",
    "# def normalize_x_values(dataset, directory_path):\n",
    "#     try:\n",
    "#         shape_of_x = dataset[0].x.shape[1]\n",
    "#         print(f\"Shape of x: {shape_of_x}\")\n",
    "        \n",
    "#         list_of_scalers_to_save = []\n",
    "#         print(\"Processing x values...\")\n",
    "\n",
    "#         # Process in batches\n",
    "#         batch_size = 100  # Adjust this value based on your available memory\n",
    "#         for i in range(shape_of_x):\n",
    "#             print(f\"Processing feature {i}/{shape_of_x}\")\n",
    "#             scaler = StandardScaler()\n",
    "            \n",
    "#             # Fit scaler in batches\n",
    "#             for j in range(0, len(dataset), batch_size):\n",
    "#                 batch = dataset[j:j+batch_size]\n",
    "#                 print(f\"Processing batch {j//batch_size + 1}/{len(dataset)//batch_size + 1}\")\n",
    "#                 batch_x_values = torch.cat([data.x[:, i].reshape(-1, 1) for data in batch], dim=0)\n",
    "#                 batch_x_values = replace_invalid_values(batch_x_values)\n",
    "#                 scaler.partial_fit(batch_x_values.numpy())\n",
    "\n",
    "#             list_of_scalers_to_save.append(scaler)\n",
    "\n",
    "#             # Transform data\n",
    "#             for j, data in enumerate(dataset):\n",
    "#                 if j % 100 == 0:\n",
    "#                     print(f\"Transforming data point {j}/{len(dataset)}\")\n",
    "#                 data_x_dim = replace_invalid_values(data.x[:, i].reshape(-1, 1))\n",
    "#                 normalized_x_dim = torch.tensor(scaler.transform(data_x_dim.numpy()), dtype=torch.float)\n",
    "#                 if i == 0:\n",
    "#                     data.normalized_x = normalized_x_dim\n",
    "#                 else:\n",
    "#                     data.normalized_x = torch.cat((data.normalized_x, normalized_x_dim), dim=1)\n",
    "\n",
    "#         print(\"Saving scalers...\")\n",
    "#         joblib.dump(list_of_scalers_to_save, (directory_path + 'x_scaler.pkl'))\n",
    "#         print(\"Scalers saved successfully\")\n",
    "\n",
    "#         print(\"Updating x values in dataset...\")\n",
    "#         for data in dataset:\n",
    "#             data.x = data.normalized_x\n",
    "#             del data.normalized_x\n",
    "#         print(\"Dataset x values updated\")\n",
    "\n",
    "#         return dataset\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in normalize_x_values: {str(e)}\")\n",
    "#         traceback.print_exc()\n",
    "#         raise   \n",
    "    \n",
    "def normalize_x_values(dataset, directory_path):\n",
    "    try:\n",
    "        shape_of_x = dataset[0].x.shape[1]\n",
    "        print(f\"Shape of x: {shape_of_x}\")\n",
    "        \n",
    "        list_of_scalers_to_save = []\n",
    "        print(\"Concatenating x values...\")\n",
    "        x_values = torch.cat([data.x for data in dataset], dim=0)\n",
    "        print(f\"Concatenated x_values shape: {x_values.shape}\")\n",
    "\n",
    "        for i in range(shape_of_x):\n",
    "            print(f\"Processing feature {i}/{shape_of_x}\")\n",
    "            all_node_features = replace_invalid_values(x_values[:, i].reshape(-1, 1)).numpy()\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(all_node_features)\n",
    "            list_of_scalers_to_save.append(scaler)\n",
    "\n",
    "            for j, data in enumerate(dataset):\n",
    "                if j % 100 == 0:\n",
    "                    print(f\"Processing data point {j}/{len(dataset)}\")\n",
    "                data_x_dim = replace_invalid_values(data.x[:, i].reshape(-1, 1))\n",
    "                normalized_x_dim = torch.tensor(scaler.transform(data_x_dim.numpy()), dtype=torch.float)\n",
    "                if i == 0:\n",
    "                    data.normalized_x = normalized_x_dim\n",
    "                else:\n",
    "                    data.normalized_x = torch.cat((data.normalized_x, normalized_x_dim), dim=1)\n",
    "\n",
    "        joblib.dump(list_of_scalers_to_save, (directory_path + 'x_scaler.pkl'))\n",
    "        for data in dataset:\n",
    "            data.x = data.normalized_x\n",
    "            del data.normalized_x\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error in normalize_x_values: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# Function to copy a Subset\n",
    "def copy_subset(subset):\n",
    "    return Subset(copy.deepcopy(subset.dataset), copy.deepcopy(subset.indices))\n",
    "\n",
    "# def normalize_x_values(dataset, directory_path):\n",
    "#     shape_of_x = dataset[0].x.shape[1]\n",
    "#     list_of_scalers_to_save = []\n",
    "#     x_values = torch.cat([data.x for data in dataset], dim=0)\n",
    "\n",
    "#     for i in range(shape_of_x):\n",
    "#         all_node_features = replace_invalid_values(x_values[:, i].reshape(-1, 1)).numpy()\n",
    "        \n",
    "#         scaler = StandardScaler()\n",
    "#         print(f\"Scaler created for x values at index {i}: {scaler}\")\n",
    "#         scaler.fit(all_node_features)\n",
    "#         list_of_scalers_to_save.append(scaler)\n",
    "\n",
    "#         for data in dataset:\n",
    "#             data_x_dim = replace_invalid_values(data.x[:, i].reshape(-1, 1))\n",
    "#             normalized_x_dim = torch.tensor(scaler.transform(data_x_dim.numpy()), dtype=torch.float)\n",
    "#             if i == 0:\n",
    "#                 data.normalized_x = normalized_x_dim\n",
    "#             else:\n",
    "#                 data.normalized_x = torch.cat((data.normalized_x, normalized_x_dim), dim=1)\n",
    "\n",
    "#     joblib.dump(list_of_scalers_to_save, (directory_path + 'x_scaler.pkl'))\n",
    "#     for data in dataset:\n",
    "#         data.x = data.normalized_x\n",
    "#         del data.normalized_x\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "def normalize_positional_features(dataset, directory_path):\n",
    "    try:\n",
    "        shape_of_pos = dataset[0].pos.shape\n",
    "        print(f\"Shape of pos: {shape_of_pos}\")\n",
    "        \n",
    "        list_of_scalers_to_save = []\n",
    "        print(\"Concatenating positional values...\")\n",
    "        pos_values = torch.cat([data.pos.reshape(data.pos.shape[0], -1) for data in dataset], dim=0)\n",
    "        print(f\"Concatenated pos_values shape: {pos_values.shape}\")\n",
    "\n",
    "        for i in range(pos_values.shape[1]):\n",
    "            print(f\"Processing positional feature {i}/{pos_values.shape[1]}\")\n",
    "            all_pos_features = replace_invalid_values(pos_values[:, i].reshape(-1, 1)).numpy()\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(all_pos_features)\n",
    "            list_of_scalers_to_save.append(scaler)\n",
    "\n",
    "            for j, data in enumerate(dataset):\n",
    "                if j % 100 == 0:\n",
    "                    print(f\"Processing data point {j}/{len(dataset)}\")\n",
    "                data_pos_dim = replace_invalid_values(data.pos.reshape(data.pos.shape[0], -1)[:, i].reshape(-1, 1))\n",
    "                normalized_pos_dim = torch.tensor(scaler.transform(data_pos_dim.numpy()), dtype=torch.float)\n",
    "                if i == 0:\n",
    "                    data.normalized_pos = normalized_pos_dim\n",
    "                else:\n",
    "                    data.normalized_pos = torch.cat((data.normalized_pos, normalized_pos_dim), dim=1)\n",
    "\n",
    "        print(\"Saving positional scalers...\")\n",
    "        joblib.dump(list_of_scalers_to_save, (directory_path + 'pos_scaler.pkl'))\n",
    "        print(\"Positional scalers saved successfully\")\n",
    "\n",
    "        print(\"Updating pos values in dataset...\")\n",
    "        for data in dataset:\n",
    "            data.pos = data.normalized_pos.reshape(shape_of_pos)\n",
    "            del data.normalized_pos\n",
    "        print(\"Dataset pos values updated\")\n",
    "\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error in normalize_positional_features: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "\n",
    "# def normalize_positional_features(dataset, directory_path):\n",
    "#     # Initialize 6 StandardScalers for 3 sets of 2 dimensions\n",
    "#     scalers = [[StandardScaler() for _ in range(2)] for _ in range(3)]\n",
    "\n",
    "#     # Standardize the data\n",
    "#     for i in range(3):  # Iterate over the second dimension (3 sets)\n",
    "#         for j in range(2):  # Iterate over the third dimension (2D vectors)\n",
    "#             values = np.vstack([data.pos[:, i, j].numpy() for data in dataset]).reshape(-1, 1)\n",
    "#             # Fit the corresponding scaler on the extracted values\n",
    "#             scalers[i][j].fit(values)\n",
    "#             for data in dataset:\n",
    "#                 transformed = scalers[i][j].transform(data.pos[:, i, j].numpy().reshape(-1, 1)).flatten()\n",
    "#                 # Convert the transformed NumPy array back into a torch tensor\n",
    "#                 data.pos[:, i, j] = torch.tensor(transformed, dtype=torch.float32)\n",
    "#     # Save the scalers using joblib\n",
    "#     for i in range(3):\n",
    "#         for j in range(2):\n",
    "#             # Dump the scalers with meaningful names to differentiate them\n",
    "#             scaler_path = directory_path + f'scaler_pos_{i}_{j}.pkl'\n",
    "#             joblib.dump(scalers[i][j], scaler_path)\n",
    "\n",
    "#     print(\"Postional scalers saved and dataset standardized.\")\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "def normalize_mode_stats(dataset, directory_path):\n",
    "    try:\n",
    "        shape_of_mode_stats = dataset[0].mode_stats.shape\n",
    "        print(f\"Shape of mode_stats: {shape_of_mode_stats}\")\n",
    "        \n",
    "        list_of_scalers_to_save = []\n",
    "        print(\"Concatenating mode stats values...\")\n",
    "        mode_stats_values = torch.cat([data.mode_stats.reshape(-1) for data in dataset], dim=0)\n",
    "        print(f\"Concatenated mode_stats_values shape: {mode_stats_values.shape}\")\n",
    "\n",
    "        for i in range(mode_stats_values.shape[0]):\n",
    "            print(f\"Processing mode stats feature {i}/{mode_stats_values.shape[0]}\")\n",
    "            all_mode_stats_features = replace_invalid_values(mode_stats_values[i].reshape(-1, 1)).numpy()\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(all_mode_stats_features)\n",
    "            list_of_scalers_to_save.append(scaler)\n",
    "\n",
    "            for j, data in enumerate(dataset):\n",
    "                if j % 100 == 0:\n",
    "                    print(f\"Processing data point {j}/{len(dataset)}\")\n",
    "                data_mode_stats_dim = replace_invalid_values(data.mode_stats.reshape(-1)[i].reshape(-1, 1))\n",
    "                normalized_mode_stats_dim = torch.tensor(scaler.transform(data_mode_stats_dim.numpy()), dtype=torch.float)\n",
    "                if i == 0:\n",
    "                    data.normalized_mode_stats = normalized_mode_stats_dim\n",
    "                else:\n",
    "                    data.normalized_mode_stats = torch.cat((data.normalized_mode_stats, normalized_mode_stats_dim), dim=0)\n",
    "\n",
    "        print(\"Saving mode stats scalers...\")\n",
    "        joblib.dump(list_of_scalers_to_save, (directory_path + 'mode_stats_scaler.pkl'))\n",
    "        print(\"Mode stats scalers saved successfully\")\n",
    "\n",
    "        print(\"Updating mode_stats values in dataset...\")\n",
    "        for data in dataset:\n",
    "            data.mode_stats = data.normalized_mode_stats.reshape(shape_of_mode_stats)\n",
    "            del data.normalized_mode_stats\n",
    "        print(\"Dataset mode_stats values updated\")\n",
    "\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error in normalize_mode_stats: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "# def normalize_mode_stats(dataset, directory_path):\n",
    "#     # Initialize 12 StandardScalers for 6 sets of 2 dimensions\n",
    "#     scalers = [[StandardScaler() for _ in range(2)] for _ in range(6)]\n",
    "\n",
    "#     # Standardize the data\n",
    "#     for i in range(6):  # Iterate over the first dimension (6 sets)\n",
    "#         for j in range(2):  # Iterate over the second dimension (2D vectors)\n",
    "#             values = np.vstack([data.mode_stats[i, j].numpy().reshape(-1, 1) for data in dataset])\n",
    "#             # Fit the corresponding scaler on the extracted values\n",
    "#             scalers[i][j].fit(values)\n",
    "#             for data in dataset:\n",
    "#                 transformed = scalers[i][j].transform(data.mode_stats[i, j].numpy().reshape(-1, 1)).flatten()\n",
    "#                 # Convert the transformed NumPy array back into a torch tensor\n",
    "#                 data.mode_stats[i, j] = torch.tensor(transformed, dtype=torch.float32)\n",
    "    \n",
    "#     # Save the scalers using joblib\n",
    "#     for i in range(6):\n",
    "#         for j in range(2):\n",
    "#             # Dump the scalers with meaningful names to differentiate them\n",
    "#             scaler_path = directory_path + f'scaler_mode_stats_{i}_{j}.pkl'\n",
    "#             joblib.dump(scalers[i][j], scaler_path)\n",
    "\n",
    "#     print(\"Mode stats scalers saved and dataset standardized.\")\n",
    "#     return dataset\n",
    "\n",
    "def replace_invalid_values(tensor):\n",
    "    tensor[tensor != tensor] = 0  # replace NaNs with 0\n",
    "    tensor[tensor == float('inf')] = 0  # replace inf with 0\n",
    "    tensor[tensor == float('-inf')] = 0  # replace -inf with 0\n",
    "    return tensor\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# def prepare_data_with_graph_features(datalist, batch_size, path_to_save_dataloader):\n",
    "#     # datalist = [Data(x=d['x'], edge_index=d['edge_index'], edge_attr=d['edge_attr'], pos=d['pos'], y=d['y'], mode_stats=d['mode_stats']) for d in data_dict_list]\n",
    "#     train_set, valid_set, test_set = gio.split_into_subsets(dataset=datalist, train_ratio=0.8, val_ratio=0.15, test_ratio=0.05)\n",
    "    \n",
    "#     train_set_normalized = normalize_dataset(dataset_input = train_set, directory_path=path_to_save_dataloader + \"train_\")\n",
    "#     valid_set_normalized = normalize_dataset(dataset_input = valid_set, directory_path=path_to_save_dataloader + \"valid_\")\n",
    "#     # # test_set_normalized = normalize_dataset(dataset_input = test_set, directory_path=path_to_save_dataloader + \"test_\")\n",
    "        \n",
    "#     train_loader = DataLoader(dataset=train_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, prefetch_factor=2, pin_memory=True, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "#     val_loader = DataLoader(dataset=valid_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "#     # test_loader = DataLoader(dataset=test_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "#     # gio.save_dataloader(test_loader, path_to_save_dataloader + 'test_dl.pt')\n",
    "#     # gio.save_dataloader_params(test_loader, path_to_save_dataloader + 'test_loader_params.json')\n",
    "    \n",
    "#     return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 type: <class 'list'>, length: 100\n",
      "Loaded batch 1, current total: 100 items\n",
      "Batch 2 type: <class 'list'>, length: 100\n",
      "Loaded batch 2, current total: 200 items\n",
      "Batch 3 type: <class 'list'>, length: 100\n",
      "Loaded batch 3, current total: 300 items\n",
      "Batch 4 type: <class 'list'>, length: 100\n",
      "Loaded batch 4, current total: 400 items\n",
      "Batch 5 type: <class 'list'>, length: 100\n",
      "Loaded batch 5, current total: 500 items\n",
      "Batch 6 type: <class 'list'>, length: 100\n",
      "Loaded batch 6, current total: 600 items\n",
      "Batch 7 type: <class 'list'>, length: 100\n",
      "Loaded batch 7, current total: 700 items\n",
      "Batch 8 type: <class 'list'>, length: 100\n",
      "Loaded batch 8, current total: 800 items\n",
      "Batch 9 type: <class 'list'>, length: 100\n",
      "Loaded batch 9, current total: 900 items\n",
      "Batch 10 type: <class 'list'>, length: 100\n",
      "Loaded batch 10, current total: 1000 items\n",
      "Successfully loaded 1000 data points\n",
      "Final count: Successfully loaded 1000 data points\n"
     ]
    }
   ],
   "source": [
    "# Set parameters here\n",
    "params = {\"project_name\": \"test\",\n",
    "            \"num_epochs\": 1000,\n",
    "            \"batch_size\": 8,\n",
    "            \"point_net_conv_layer_structure_local_mlp\": [64, 128],\n",
    "            \"point_net_conv_layer_structure_global_mlp\": [256, 64],\n",
    "            \"gat_conv_layer_structure\": [128, 256, 256, 128],\n",
    "            \"graph_mlp_layer_structure\": [128, 256, 128],\n",
    "            \"lr\": 0.001,\n",
    "            \"gradient_accumulation_steps\": 3,\n",
    "            \"in_channels\": 15,\n",
    "            \"out_channels\": 1,\n",
    "            \"early_stopping_patience\": 100,\n",
    "            \"unique_model_description\": \"my_test\",\n",
    "            \"dropout\": 0.3,\n",
    "            \"use_dropout\": False\n",
    "        } \n",
    "\n",
    "# Define the paths here\n",
    "def get_paths(base_dir: str, unique_model_description: str, model_save_path: str = 'trained_model/model.pth'):\n",
    "    data_path = os.path.join(base_dir, unique_model_description)\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    model_save_to = os.path.join(data_path, model_save_path)\n",
    "    path_to_save_dataloader = os.path.join(data_path, 'data_created_during_training/')\n",
    "    os.makedirs(os.path.dirname(model_save_to), exist_ok=True)\n",
    "    os.makedirs(path_to_save_dataloader, exist_ok=True)\n",
    "    return model_save_to, path_to_save_dataloader\n",
    "\n",
    "\n",
    "def get_combined_data(dataset_path, max_batches=None):\n",
    "    data_list = []\n",
    "    batch_num = 1\n",
    "    while max_batches is None or batch_num <= max_batches:\n",
    "        try:\n",
    "            batch_file = os.path.join(dataset_path, f'datalist_batch_{batch_num}.pt')\n",
    "            batch_data = torch.load(batch_file, map_location='cpu')\n",
    "            print(f\"Batch {batch_num} type: {type(batch_data)}, length: {len(batch_data)}\")\n",
    "            \n",
    "            if isinstance(batch_data, list):\n",
    "                for idx, item in enumerate(batch_data):\n",
    "                    try:\n",
    "                        # print(f\"Item {idx} type: {type(item)}\")\n",
    "                        if isinstance(item, Data):\n",
    "                            required_attrs = ['x', 'edge_index', 'edge_attr', 'pos', 'y', 'mode_stats']\n",
    "                            missing_attrs = [attr for attr in required_attrs if not hasattr(item, attr)]\n",
    "                            if not missing_attrs:\n",
    "                                data_list.append(item)\n",
    "                                # print(f\"Added item {idx} to data_list\")\n",
    "                            else:\n",
    "                                print(f\"Skipping invalid item {idx} in batch {batch_num}. Missing attributes: {missing_attrs}\")\n",
    "                        else:\n",
    "                            print(f\"Skipping non-Data item {idx} in batch {batch_num}.\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing item {idx} in batch {batch_num}: {str(e)}\")\n",
    "            else:\n",
    "                print(f\"Unexpected batch data type in batch {batch_num}: {type(batch_data)}\")\n",
    "            \n",
    "            batch_num += 1\n",
    "            print(f\"Loaded batch {batch_num-1}, current total: {len(data_list)} items\")\n",
    "            \n",
    "            if len(data_list) % 1000 == 0:\n",
    "                if psutil.virtual_memory().percent > 90:\n",
    "                    print(\"Memory usage high, stopping data loading\")\n",
    "                    break\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Finished loading {batch_num-1} batches\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading batch {batch_num}: {str(e)}\")\n",
    "            batch_num += 1\n",
    "\n",
    "    print(f\"Successfully loaded {len(data_list)} data points\")\n",
    "    return data_list\n",
    "\n",
    "# Usage\n",
    "try:\n",
    "    dataset_path = '../../data/train_data/sim_output_1pm_capacity_reduction_10k_09_10_2024/'\n",
    "    data_list = get_combined_data(dataset_path, max_batches=10)  # Let's look at the first two batches\n",
    "    print(f\"Final count: Successfully loaded {len(data_list)} data points\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base directory for the run\n",
    "base_dir = '../../data/' + params['project_name'] + '/'\n",
    "unique_run_dir = os.path.join(base_dir, params['unique_model_description'])\n",
    "os.makedirs(unique_run_dir, exist_ok=True)\n",
    "\n",
    "model_save_path, path_to_save_dataloader = get_paths(base_dir=base_dir, unique_model_description= params['unique_model_description'], model_save_path= 'trained_model/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prepare_data_with_graph_features with 1000 items\n",
      "Splitting into subsets...\n",
      "Total dataset length: 1000\n",
      "Training subset length: 800\n",
      "Validation subset length: 150\n",
      "Test subset length: 50\n",
      "Split complete. Train: 800, Valid: 150, Test: 50\n",
      "Normalizing train set...\n",
      "Starting normalization for 800 items\n",
      "Dataset copied successfully\n",
      "Shape of x: 15\n",
      "Concatenating x values...\n",
      "Concatenated x_values shape: torch.Size([24928000, 15])\n",
      "Processing feature 0/15\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing feature 1/15\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing feature 2/15\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing feature 3/15\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing feature 4/15\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing feature 5/15\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing feature 6/15\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing feature 7/15\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing feature 8/15\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing feature 9/15\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing feature 10/15\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing feature 11/15\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing feature 12/15\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing feature 13/15\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing feature 14/15\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "X values normalized successfully\n",
      "Shape of pos: torch.Size([31160, 3, 2])\n",
      "Concatenating positional values...\n",
      "Concatenated pos_values shape: torch.Size([24928000, 6])\n",
      "Processing positional feature 0/6\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing positional feature 1/6\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing positional feature 2/6\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing positional feature 3/6\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing positional feature 4/6\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing positional feature 5/6\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Saving positional scalers...\n",
      "Positional scalers saved successfully\n",
      "Updating pos values in dataset...\n",
      "Dataset pos values updated\n",
      "Positional features normalized successfully\n",
      "Shape of mode_stats: torch.Size([6, 2])\n",
      "Concatenating mode stats values...\n",
      "Concatenated mode_stats_values shape: torch.Size([9600])\n",
      "Processing mode stats feature 0/9600\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing mode stats feature 1/9600\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing mode stats feature 2/9600\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing mode stats feature 3/9600\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing mode stats feature 4/9600\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing mode stats feature 5/9600\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing mode stats feature 6/9600\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing mode stats feature 7/9600\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing mode stats feature 8/9600\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing mode stats feature 9/9600\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing mode stats feature 10/9600\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing mode stats feature 11/9600\n",
      "Processing data point 0/800\n",
      "Processing data point 100/800\n",
      "Processing data point 200/800\n",
      "Processing data point 300/800\n",
      "Processing data point 400/800\n",
      "Processing data point 500/800\n",
      "Processing data point 600/800\n",
      "Processing data point 700/800\n",
      "Processing mode stats feature 12/9600\n",
      "Processing data point 0/800\n",
      "Error in normalize_mode_stats: index 12 is out of bounds for dimension 0 with size 12\n",
      "Error in normalize_dataset: index 12 is out of bounds for dimension 0 with size 12\n",
      "Error in prepare_data_with_graph_features: index 12 is out of bounds for dimension 0 with size 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3401976/1863502656.py\", line 246, in normalize_mode_stats\n",
      "    data_mode_stats_dim = replace_invalid_values(data.mode_stats.reshape(-1)[i].reshape(-1, 1))\n",
      "IndexError: index 12 is out of bounds for dimension 0 with size 12\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3401976/1863502656.py\", line 15, in normalize_dataset\n",
      "    dataset = normalize_mode_stats(dataset, directory_path)\n",
      "  File \"/tmp/ipykernel_3401976/1863502656.py\", line 246, in normalize_mode_stats\n",
      "    data_mode_stats_dim = replace_invalid_values(data.mode_stats.reshape(-1)[i].reshape(-1, 1))\n",
      "IndexError: index 12 is out of bounds for dimension 0 with size 12\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_3401976/3112007668.py\", line 10, in prepare_data_with_graph_features\n",
      "    train_set_normalized = normalize_dataset(dataset_input=train_set, directory_path=path_to_save_dataloader + \"train_\")\n",
      "  File \"/tmp/ipykernel_3401976/1863502656.py\", line 15, in normalize_dataset\n",
      "    dataset = normalize_mode_stats(dataset, directory_path)\n",
      "  File \"/tmp/ipykernel_3401976/1863502656.py\", line 246, in normalize_mode_stats\n",
      "    data_mode_stats_dim = replace_invalid_values(data.mode_stats.reshape(-1)[i].reshape(-1, 1))\n",
      "IndexError: index 12 is out of bounds for dimension 0 with size 12\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 12 is out of bounds for dimension 0 with size 12",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m train_dl, valid_dl \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_with_graph_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatalist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_to_save_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpath_to_save_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 10\u001b[0m, in \u001b[0;36mprepare_data_with_graph_features\u001b[0;34m(datalist, batch_size, path_to_save_dataloader)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSplit complete. Train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_set)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Valid: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(valid_set)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_set)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNormalizing train set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m train_set_normalized \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_to_save_dataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain set normalized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNormalizing validation set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m, in \u001b[0;36mnormalize_dataset\u001b[0;34m(dataset_input, directory_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m normalize_positional_features(dataset, directory_path)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPositional features normalized successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize_mode_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirectory_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMode stats normalized successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "Cell \u001b[0;32mIn[2], line 246\u001b[0m, in \u001b[0;36mnormalize_mode_stats\u001b[0;34m(dataset, directory_path)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m j \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing data point \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 246\u001b[0m data_mode_stats_dim \u001b[38;5;241m=\u001b[39m replace_invalid_values(\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode_stats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    247\u001b[0m normalized_mode_stats_dim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(scaler\u001b[38;5;241m.\u001b[39mtransform(data_mode_stats_dim\u001b[38;5;241m.\u001b[39mnumpy()), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mIndexError\u001b[0m: index 12 is out of bounds for dimension 0 with size 12"
     ]
    }
   ],
   "source": [
    "def prepare_data_with_graph_features(datalist, batch_size, path_to_save_dataloader):\n",
    "    print(f\"Starting prepare_data_with_graph_features with {len(datalist)} items\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Splitting into subsets...\")\n",
    "        train_set, valid_set, test_set = gio.split_into_subsets(dataset=datalist, train_ratio=0.8, val_ratio=0.15, test_ratio=0.05)\n",
    "        print(f\"Split complete. Train: {len(train_set)}, Valid: {len(valid_set)}, Test: {len(test_set)}\")\n",
    "        \n",
    "        print(\"Normalizing train set...\")\n",
    "        train_set_normalized = normalize_dataset(dataset_input=train_set, directory_path=path_to_save_dataloader + \"train_\")\n",
    "        print(\"Train set normalized\")\n",
    "        \n",
    "        print(\"Normalizing validation set...\")\n",
    "        valid_set_normalized = normalize_dataset(dataset_input=valid_set, directory_path=path_to_save_dataloader + \"valid_\")\n",
    "        print(\"Validation set normalized\")\n",
    "        \n",
    "        print(\"Creating train loader...\")\n",
    "        train_loader = DataLoader(dataset=train_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, prefetch_factor=2, pin_memory=True, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "        print(\"Train loader created\")\n",
    "        \n",
    "        print(\"Creating validation loader...\")\n",
    "        val_loader = DataLoader(dataset=valid_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "        print(\"Validation loader created\")\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    except Exception as e:\n",
    "        print(f\"Error in prepare_data_with_graph_features: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "train_dl, valid_dl = prepare_data_with_graph_features(datalist=data_list, batch_size= params['batch_size'], path_to_save_dataloader= path_to_save_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outgoing edges from node 0: [9477, 9478, 9479, 31157]\n",
      "Incoming edges to node 0: [9479, 23978, 31157]\n",
      "The graph is unidirectional\n"
     ]
    }
   ],
   "source": [
    "def check_directionality(data):\n",
    "    # Get all edges starting from node 0\n",
    "    outgoing = data.edge_index[1, data.edge_index[0] == 25318].tolist()\n",
    "    \n",
    "    # Get all edges ending at node 0\n",
    "    incoming = data.edge_index[0, data.edge_index[1] == 25318].tolist()\n",
    "    \n",
    "    # Check if all outgoing edges have a corresponding incoming edge\n",
    "    bidirectional = all(node in incoming for node in outgoing) and len(outgoing) == len(incoming)\n",
    "    \n",
    "    print(f\"Outgoing edges from node 0: {outgoing}\")\n",
    "    print(f\"Incoming edges to node 0: {incoming}\")\n",
    "    print(f\"The graph is {'bidirectional' if bidirectional else 'unidirectional'}\")\n",
    "    \n",
    "    return bidirectional\n",
    "\n",
    "# Use the function on your data object\n",
    "data = train_dl.dataset[0]\n",
    "is_bidirectional = check_directionality(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[31160, 15], edge_index=[2, 117679], edge_attr=[117679, 1], y=[31160, 1], pos=[31160, 3, 2], mode_stats=[6, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU 1 with CUDA_VISIBLE_DEVICES=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33menatterer\u001b[0m (\u001b[33mtum-traffic-engineering\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/enatterer/Development/gnn_predicting_effects_of_traffic_policies/scripts/training/wandb/run-20241011_091402-txxtjca7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tum-traffic-engineering/test/runs/txxtjca7' target=\"_blank\">efficient-wood-52</a></strong> to <a href='https://wandb.ai/tum-traffic-engineering/test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tum-traffic-engineering/test' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tum-traffic-engineering/test/runs/txxtjca7' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/test/runs/txxtjca7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized\n",
      "baseline loss mean 48.64235305786133\n",
      "baseline loss no  48.66389083862305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:   0%|          | 0/30 [00:00<?, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "Epoch 1/1000: 100%|██████████| 30/30 [00:07<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, validation loss: 49.5520388285319, lr: 1.4499999999999999e-06, r^2: -0.0015276670455932617\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 49.5520388285319\n",
      "Checkpoint saved to ../../data/test/my_test/trained_model/checkpoints/checkpoint_epoch_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000: 100%|██████████| 30/30 [00:07<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, validation loss: 49.29929224650065, lr: 2.95e-06, r^2: -0.0013796091079711914\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 49.29929224650065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000: 100%|██████████| 30/30 [00:07<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, validation loss: 49.30713971455892, lr: 4.45e-06, r^2: -0.0011576414108276367\n",
      "EarlyStopping counter: 1 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000: 100%|██████████| 30/30 [00:07<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, validation loss: 49.73073132832845, lr: 5.950000000000001e-06, r^2: -0.0008859634399414062\n",
      "EarlyStopping counter: 2 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000: 100%|██████████| 30/30 [00:07<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, validation loss: 49.074965159098305, lr: 7.45e-06, r^2: -0.0006104707717895508\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 49.074965159098305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000: 100%|██████████| 30/30 [00:07<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, validation loss: 49.18925666809082, lr: 8.949999999999999e-06, r^2: -0.00037097930908203125\n",
      "EarlyStopping counter: 1 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000: 100%|██████████| 30/30 [00:07<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, validation loss: 50.495652516682945, lr: 1.045e-05, r^2: -0.00017380714416503906\n",
      "EarlyStopping counter: 2 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000: 100%|██████████| 30/30 [00:07<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, validation loss: 49.40178426106771, lr: 1.195e-05, r^2: -9.894371032714844e-06\n",
      "EarlyStopping counter: 3 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000: 100%|██████████| 30/30 [00:07<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, validation loss: 49.95569038391113, lr: 1.345e-05, r^2: 8.463859558105469e-05\n",
      "EarlyStopping counter: 4 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000: 100%|██████████| 30/30 [00:07<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, validation loss: 49.48337491353353, lr: 1.4950000000000001e-05, r^2: 0.00013941526412963867\n",
      "EarlyStopping counter: 5 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000: 100%|██████████| 30/30 [00:07<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, validation loss: 49.7298469543457, lr: 1.645e-05, r^2: 0.00019550323486328125\n",
      "EarlyStopping counter: 6 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000: 100%|██████████| 30/30 [00:07<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11, validation loss: 49.568040211995445, lr: 1.7950000000000003e-05, r^2: 0.0002677440643310547\n",
      "EarlyStopping counter: 7 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000: 100%|██████████| 30/30 [00:07<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, validation loss: 49.93026351928711, lr: 1.945e-05, r^2: 0.0003350973129272461\n",
      "EarlyStopping counter: 8 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000: 100%|██████████| 30/30 [00:07<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13, validation loss: 50.09173075358073, lr: 2.095e-05, r^2: 0.00040203332901000977\n",
      "EarlyStopping counter: 9 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000: 100%|██████████| 30/30 [00:07<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, validation loss: 49.53401184082031, lr: 2.245e-05, r^2: 0.00046759843826293945\n",
      "EarlyStopping counter: 10 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000: 100%|██████████| 30/30 [00:07<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, validation loss: 50.276679356892906, lr: 2.395e-05, r^2: 0.0005347728729248047\n",
      "EarlyStopping counter: 11 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000: 100%|██████████| 30/30 [00:07<00:00,  4.06it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline loss no  \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(baseline_loss) )\n\u001b[1;32m     37\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m gio\u001b[38;5;241m.\u001b[39mEarlyStopping(patience\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly_stopping_patience\u001b[39m\u001b[38;5;124m'\u001b[39m], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 38\u001b[0m best_val_loss, best_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mgarch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloss_fct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m            \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_gradient_clipping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler_warmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler_cosine_decay_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest model saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \n",
      "File \u001b[0;32m~/Development/gnn_predicting_effects_of_traffic_policies/scripts/gnn_architectures_district_features.py:320\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, config, loss_fct, optimizer, train_dl, valid_dl, device, early_stopping, accumulation_steps, model_save_path, use_gradient_clipping, lr_scheduler_warmup_steps, lr_scheduler_cosine_decay_rate)\u001b[0m\n\u001b[1;32m    317\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    318\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 320\u001b[0m val_loss, r_squared \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model_during_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m: lr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr^2\u001b[39m\u001b[38;5;124m\"\u001b[39m: r_squared})\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, lr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, r^2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr_squared\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Development/gnn_predicting_effects_of_traffic_policies/scripts/gnn_architectures_district_features.py:375\u001b[0m, in \u001b[0;36mvalidate_model_during_training\u001b[0;34m(model, dataset, loss_func, device)\u001b[0m\n\u001b[1;32m    373\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n\u001b[1;32m    376\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    377\u001b[0m         input_node_features, targets \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39my\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1285\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1287\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gpus = hf.get_available_gpus()\n",
    "best_gpu = hf.select_best_gpu(gpus)\n",
    "hf.set_cuda_visible_device(best_gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = hf.setup_wandb(params['project_name'], {\n",
    "    \"epochs\": params['num_epochs'],\n",
    "    \"batch_size\": params['batch_size'],\n",
    "    \"lr\": params['lr'],\n",
    "    \"gradient_accumulation_steps\": params['gradient_accumulation_steps'],\n",
    "    \"early_stopping_patience\": params['early_stopping_patience'],\n",
    "    \"point_net_conv_local_mlp\": params['point_net_conv_layer_structure_local_mlp'],\n",
    "    \"point_net_conv_global_mlp\": params['point_net_conv_layer_structure_global_mlp'],\n",
    "    \"gat_conv_layer_structure\": params['gat_conv_layer_structure'],\n",
    "    \"graph_mlp_layer_structure\": params['graph_mlp_layer_structure'],\n",
    "    \"in_channels\": params['in_channels'],\n",
    "    \"out_channels\": params['out_channels'],\n",
    "    \"dropout\": params['dropout'],\n",
    "    \"use_dropout\": params['use_dropout']\n",
    "})\n",
    "\n",
    "model = garch.MyGnn(in_channels=config.in_channels, out_channels=config.out_channels, point_net_conv_layer_structure_local_mlp=config.point_net_conv_local_mlp,\n",
    "                            point_net_conv_layer_structure_global_mlp=config.point_net_conv_global_mlp,\n",
    "                            gat_conv_layer_structure=config.gat_conv_layer_structure,\n",
    "                            graph_mlp_layer_structure=config.graph_mlp_layer_structure,\n",
    "                            dropout=config.dropout, use_dropout=config.use_dropout)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_fct = torch.nn.MSELoss()\n",
    "\n",
    "baseline_loss_mean_target = gio.compute_baseline_of_mean_target(dataset=train_dl, loss_fct=loss_fct)\n",
    "baseline_loss = gio.compute_baseline_of_no_policies(dataset=train_dl, loss_fct=loss_fct)\n",
    "print(\"baseline loss mean \" + str(baseline_loss_mean_target))\n",
    "print(\"baseline loss no  \" +str(baseline_loss) )\n",
    "\n",
    "early_stopping = gio.EarlyStopping(patience=params['early_stopping_patience'], verbose=True)\n",
    "best_val_loss, best_epoch = garch.train(model=model, \n",
    "            config=config, \n",
    "            loss_fct=loss_fct,\n",
    "            optimizer=torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=1e-4),\n",
    "            train_dl=train_dl,  \n",
    "            valid_dl=valid_dl,\n",
    "            device=device, \n",
    "            early_stopping=early_stopping,\n",
    "            accumulation_steps=config.gradient_accumulation_steps,\n",
    "            model_save_path=model_save_path,\n",
    "            use_gradient_clipping=True,\n",
    "            lr_scheduler_warmup_steps=20000,\n",
    "            lr_scheduler_cosine_decay_rate=0.2)\n",
    "print(f'Best model saved to {model_save_path} with validation loss: {best_val_loss} at epoch {best_epoch}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from collections import Counter\n",
    "\n",
    "# def check_scale_free_distribution(edge_index, num_nodes):\n",
    "#     # Create a NetworkX graph from the edge_index\n",
    "#     G = nx.Graph()\n",
    "#     G.add_nodes_from(range(num_nodes))\n",
    "#     edge_list = edge_index.t().tolist()\n",
    "#     G.add_edges_from(edge_list)\n",
    "\n",
    "#     # Calculate degree for each node\n",
    "#     degrees = [d for n, d in G.degree()]\n",
    "#     degree_counts = Counter(degrees)\n",
    "\n",
    "#     # Sort the degree counts\n",
    "#     sorted_degree_counts = sorted(degree_counts.items())\n",
    "#     x = [k for k, v in sorted_degree_counts]\n",
    "#     y = [v for k, v in sorted_degree_counts]\n",
    "\n",
    "#     # Plot degree distribution on log-log scale\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.loglog(x, y, 'bo-')\n",
    "#     plt.xlabel('Degree (log scale)')\n",
    "#     plt.ylabel('Count (log scale)')\n",
    "#     plt.title('Degree Distribution (Log-Log Scale)')\n",
    "#     plt.grid(True)\n",
    "\n",
    "#     # Fit a power law distribution\n",
    "#     x_log = np.log(x)\n",
    "#     y_log = np.log(y)\n",
    "#     coeffs = np.polyfit(x_log[1:], y_log[1:], 1)\n",
    "#     power_law_exponent = -coeffs[0]\n",
    "\n",
    "#     # Plot the fitted line\n",
    "#     x_fit = np.logspace(np.log10(min(x)), np.log10(max(x)), 100)\n",
    "#     y_fit = np.exp(coeffs[1]) * x_fit**(-power_law_exponent)\n",
    "#     plt.loglog(x_fit, y_fit, 'r--', label=f'Power Law Fit (γ ≈ {power_law_exponent:.2f})')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     print(f\"Estimated power law exponent: γ ≈ {power_law_exponent:.2f}\")\n",
    "    \n",
    "#     if 2 < power_law_exponent < 3:\n",
    "#         print(\"The network shows characteristics of a scale-free network.\")\n",
    "#     else:\n",
    "#         print(\"The network may not be scale-free.\")\n",
    "\n",
    "#     return power_law_exponent\n",
    "\n",
    "# # Usage example:\n",
    "# # Assuming you have a PyTorch Geometric Data object called 'data'\n",
    "# exponent = check_scale_free_distribution(data.edge_index, data.num_nodes)\n",
    "\n",
    "# from torch_geometric.utils import to_undirected, is_undirected\n",
    "\n",
    "# # Assuming you're working with the first graph in your dataset\n",
    "# data = train_dl.dataset[0]\n",
    "\n",
    "# # Check if the graph is already undirected\n",
    "# # if not is_undirected(data.edge_index):\n",
    "# #     # If it's directed, convert it to undirected\n",
    "# #     data.edge_index = to_undirected(data.edge_index)\n",
    "# #     print(\"Graph has been converted to undirected.\")\n",
    "# # else:\n",
    "# #     print(\"Graph is already undirected.\")\n",
    "\n",
    "# # Verify that the graph is now undirected\n",
    "# print(f\"Is the graph undirected? {is_undirected(data.edge_index)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenhao-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

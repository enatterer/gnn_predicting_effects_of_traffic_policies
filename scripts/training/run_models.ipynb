{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used for debugging. Should have the same functionality as run_models.py, but with more verbose output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import wandb\n",
    "import random\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import signal\n",
    "import joblib\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import help_functions as hf\n",
    "\n",
    "# Add the 'scripts' directory to the Python path\n",
    "scripts_path = os.path.abspath(os.path.join('..'))\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "    \n",
    "import gnn_io as gio\n",
    "import gnn_architecture as garch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output from data_preprocessing.process_simulations_for_gnn.py\n",
    "dataset_path = '../../data/train_data/dist_not_connected_10k_1pct/'\n",
    "\n",
    "# Base directory for the run\n",
    "base_dir = '../../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETERS = [\n",
    "    \"project_name\",\n",
    "    \"predict_mode_stats\",\n",
    "    \"in_channels\",\n",
    "    \"use_all_features\",\n",
    "    \"out_channels\",\n",
    "    \"loss_fct\",\n",
    "    \"use_weighted_loss\",\n",
    "    \"point_net_conv_layer_structure_local_mlp\",\n",
    "    \"point_net_conv_layer_structure_global_mlp\",\n",
    "    \"gat_conv_layer_structure\",\n",
    "    \"use_bootrappping\",\n",
    "    \"num_epochs\",\n",
    "    \"batch_size\",\n",
    "    \"lr\",\n",
    "    \"early_stopping_patience\",\n",
    "    \"use_dropout\",\n",
    "    \"dropout\",\n",
    "    \"use_monte_carlo_dropout\",\n",
    "    \"gradient_accumulation_steps\",\n",
    "    \"use_gradient_clipping\",\n",
    "    \"lr_scheduler_warmup_steps\",\n",
    "    \"device_nr\",\n",
    "    \"unique_model_description\"\n",
    "]\n",
    "\n",
    "def get_parameters(args):\n",
    "    params = {\n",
    "        # KEEP IN MIND: IF WE CHANGE PARAMETERS, WE NEED TO CHANGE THE NAME OF THE RUN IN WANDB (for the config)\n",
    "        \"project_name\": \"runs_01_2025\",\n",
    "        \"predict_mode_stats\": args.predict_mode_stats,\n",
    "        \"in_channels\": args.in_channels,\n",
    "        \"use_all_features\": args.use_all_features,\n",
    "        \"out_channels\": args.out_channels,\n",
    "        \"loss_fct\": args.loss_fct,\n",
    "        \"use_weighted_loss\": args.use_weighted_loss,\n",
    "        \"point_net_conv_layer_structure_local_mlp\": [int(x) for x in args.point_net_conv_layer_structure_local_mlp.split(',')],\n",
    "        \"point_net_conv_layer_structure_global_mlp\": [int(x) for x in args.point_net_conv_layer_structure_global_mlp.split(',')],\n",
    "        \"gat_conv_layer_structure\": [int(x) for x in args.gat_conv_layer_structure.split(',')],\n",
    "        \"use_bootrappping\": args.use_bootrappping,\n",
    "        \"num_epochs\": args.num_epochs,\n",
    "        \"batch_size\": int(args.batch_size),\n",
    "        \"lr\": float(args.lr),\n",
    "        \"early_stopping_patience\": args.early_stopping_patience,\n",
    "        \"use_dropout\": args.use_dropout,\n",
    "        \"dropout\": args.dropout,\n",
    "        \"use_monte_carlo_dropout\": args.use_monte_carlo_dropout,\n",
    "        \"gradient_accumulation_steps\": args.gradient_accumulation_steps,\n",
    "        \"use_gradient_clipping\": args.use_gradient_clipping,\n",
    "        \"lr_scheduler_warmup_steps\": args.lr_scheduler_warmup_steps,\n",
    "        \"device_nr\": args.device_nr\n",
    "    }\n",
    "    \n",
    "    params[\"unique_model_description\"] = (\n",
    "        f\"pnc_local_{gio.int_list_to_string(lst=params['point_net_conv_layer_structure_local_mlp'], delimiter='_')}_\"\n",
    "        f\"pnc_global_{gio.int_list_to_string(lst=params['point_net_conv_layer_structure_global_mlp'], delimiter='_')}_\"\n",
    "        f\"gat_conv_{gio.int_list_to_string(lst=params['gat_conv_layer_structure'], delimiter='_')}_\"\n",
    "        f\"use_dropout_{params['use_dropout']}_\"\n",
    "        f\"do_{params['dropout']}_\"\n",
    "        f\"use_mc_do_{params['use_monte_carlo_dropout']}_\"\n",
    "        f\"predict_mode_stats_{params['predict_mode_stats']}\"\n",
    "    )\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = []\n",
    "batch_num = 1\n",
    "while True:\n",
    "    print(f\"Processing batch number: {batch_num}\")\n",
    "    # total_memory, available_memory, used_memory = get_memory_info()\n",
    "    # print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "    # print(f\"Available Memory: {available_memory:.2f} GB\")\n",
    "    # print(f\"Used Memory: {used_memory:.2f} GB\")\n",
    "    batch_file = os.path.join(dataset_path, f'datalist_batch_{batch_num}.pt')\n",
    "    if not os.path.exists(batch_file):\n",
    "        break\n",
    "    batch_data = torch.load(batch_file, map_location='cpu')\n",
    "    if isinstance(batch_data, list):\n",
    "        datalist.extend(batch_data)\n",
    "    batch_num += 1\n",
    "print(f\"Loaded {len(datalist)} items into datalist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the argparse section with this:\n",
    "args = {\n",
    "    \"in_channels\": 5,\n",
    "    \"use_all_features\": False,\n",
    "    \"out_channels\": 1,\n",
    "    \"loss_fct\": \"mse\",\n",
    "    \"use_weighted_loss\": True,\n",
    "    \"predict_mode_stats\": False,\n",
    "    \"point_net_conv_layer_structure_local_mlp\": \"256\",\n",
    "    \"point_net_conv_layer_structure_global_mlp\": \"512\",\n",
    "    \"gat_conv_layer_structure\": \"128,256,512,256\",\n",
    "    \"use_bootrappping\": False,\n",
    "    \"num_epochs\": 3000,\n",
    "    \"batch_size\": 8,\n",
    "    \"lr\": 0.001,\n",
    "    \"early_stopping_patience\": 100,\n",
    "    \"use_dropout\": True,\n",
    "    \"dropout\": 0.3,\n",
    "    \"use_monte_carlo_dropout\": True,\n",
    "    \"gradient_accumulation_steps\": 3,\n",
    "    \"use_gradient_clipping\": True,\n",
    "    \"lr_scheduler_warmup_steps\": 10000,\n",
    "    \"device_nr\": 0\n",
    "}\n",
    "\n",
    "# Convert the dictionary to an object with attributes\n",
    "class Args:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "args = Args(**args)\n",
    "hf.set_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = hf.get_available_gpus()\n",
    "best_gpu = hf.select_best_gpu(gpus)\n",
    "hf.set_cuda_visible_device(best_gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "params = get_parameters(args)\n",
    "\n",
    "# Create directory for the run\n",
    "unique_run_dir = os.path.join(base_dir, params['project_name'], params['unique_model_description'])\n",
    "os.makedirs(unique_run_dir, exist_ok=True)\n",
    "\n",
    "model_save_path, path_to_save_dataloader = hf.get_paths(base_dir=os.path.join(base_dir, params['project_name']), unique_model_description= params['unique_model_description'], model_save_path= 'trained_model/model.pth')\n",
    "train_dl, valid_dl, scalers_train, scalers_validation = hf.prepare_data_with_graph_features(datalist=datalist,\n",
    "                                                                                            batch_size=params['batch_size'],\n",
    "                                                                                            path_to_save_dataloader=path_to_save_dataloader,\n",
    "                                                                                            use_all_features=params['use_all_features'],\n",
    "                                                                                            use_bootstrapping=params['use_bootrappping'])\n",
    "\n",
    "config = hf.setup_wandb({param: params[param] for param in PARAMETERS})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_instance = garch.MyGnn(in_channels=config.in_channels, \n",
    "                        out_channels=config.out_channels, \n",
    "                        point_net_conv_layer_structure_local_mlp=config.point_net_conv_layer_structure_local_mlp,\n",
    "                        point_net_conv_layer_structure_global_mlp=config.point_net_conv_layer_structure_global_mlp,\n",
    "                        gat_conv_layer_structure=config.gat_conv_layer_structure,\n",
    "                        use_dropout=config.use_dropout, \n",
    "                        dropout=config.dropout, \n",
    "                        use_monte_carlo_dropout=config.use_monte_carlo_dropout,\n",
    "                        predict_mode_stats=config.predict_mode_stats, \n",
    "                        dtype=torch.float32)\n",
    "        \n",
    "model = gnn_instance.to(device)\n",
    "loss_fct = gio.GNN_Loss(config.loss_fct, datalist[0].x.shape[0], device, config.use_weighted_loss)\n",
    "\n",
    "baseline_loss_mean_target = gio.compute_baseline_of_mean_target(dataset=train_dl, loss_fct=loss_fct, device=device, scalers=scalers_train)\n",
    "baseline_loss = gio.compute_baseline_of_no_policies(dataset=train_dl, loss_fct=loss_fct, device=device, scalers=scalers_train)\n",
    "print(\"baseline loss mean \" + str(baseline_loss_mean_target))\n",
    "print(\"baseline loss no  \" + str(baseline_loss) )\n",
    "\n",
    "early_stopping = gio.EarlyStopping(patience=params['early_stopping_patience'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss, best_epoch = garch.train(model=model, \n",
    "                    config=config, \n",
    "                    loss_fct=loss_fct,\n",
    "                    optimizer=torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=1e-4),\n",
    "                    train_dl=train_dl, \n",
    "                    valid_dl=valid_dl,\n",
    "                    device=device, \n",
    "                    early_stopping=early_stopping,\n",
    "                    model_save_path=model_save_path,\n",
    "                    scalers_train=scalers_train,\n",
    "                    scalers_validation=scalers_validation)\n",
    "\n",
    "print(f'Best model saved to {model_save_path} with validation loss: {best_val_loss} at epoch {best_epoch}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenhao-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory: 125.49 GB\n",
      "Available Memory: 54.87 GB\n",
      "Used Memory: 69.39 GB\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import wandb\n",
    "import random\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import signal\n",
    "import joblib\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import help_functions as hf\n",
    "\n",
    "# Add the 'scripts' directory to the Python path\n",
    "scripts_path = os.path.abspath(os.path.join('..'))\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "    \n",
    "import gnn_io as gio\n",
    "import gnn_architecture as garch\n",
    "import psutil\n",
    "\n",
    "def get_memory_info():\n",
    "        memory_info = psutil.virtual_memory()\n",
    "        total_memory = memory_info.total / (1024 ** 3)  # Convert bytes to GB\n",
    "        available_memory = memory_info.available / (1024 ** 3)  # Convert bytes to GB\n",
    "        used_memory = memory_info.used / (1024 ** 3)  # Convert bytes to GB\n",
    "        return total_memory, available_memory, used_memory\n",
    "\n",
    "total_memory, available_memory, used_memory = get_memory_info()\n",
    "print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "print(f\"Available Memory: {available_memory:.2f} GB\")\n",
    "print(f\"Used Memory: {used_memory:.2f} GB\")\n",
    "\n",
    "\n",
    "dataset_path = '../../data/train_data/sim_output_1pm_22_10_2024/'\n",
    "\n",
    "PARAMETERS = [\n",
    "    \"project_name\",\n",
    "    \"predict_mode_stats\",\n",
    "    \"in_channels\",\n",
    "    \"out_channels\",\n",
    "    \"point_net_conv_layer_structure_local_mlp\",\n",
    "    \"point_net_conv_layer_structure_global_mlp\",\n",
    "    \"gat_conv_layer_structure\",\n",
    "    \"num_epochs\",\n",
    "    \"batch_size\",\n",
    "    \"lr\",\n",
    "    \"early_stopping_patience\",\n",
    "    \"use_dropout\",\n",
    "    \"dropout\",\n",
    "    \"gradient_accumulation_steps\",\n",
    "    \"use_gradient_clipping\",\n",
    "    \"lr_scheduler_warmup_steps\",\n",
    "    \"device_nr\",\n",
    "    \"unique_model_description\"\n",
    "]\n",
    "\n",
    "def get_parameters(args):\n",
    "    params = {\n",
    "        # KEEP IN MIND: IF WE CHANGE PARAMETERS, WE NEED TO CHANGE THE NAME OF THE RUN IN WANDB (for the config)\n",
    "        \"project_name\": \"runs_21_10_2024\",\n",
    "        \"predict_mode_stats\": args.predict_mode_stats,\n",
    "        \"in_channels\": args.in_channels,\n",
    "        \"out_channels\": args.out_channels,\n",
    "        \"point_net_conv_layer_structure_local_mlp\": [int(x) for x in args.point_net_conv_layer_structure_local_mlp.split(',')],\n",
    "        \"point_net_conv_layer_structure_global_mlp\": [int(x) for x in args.point_net_conv_layer_structure_global_mlp.split(',')],\n",
    "        \"gat_conv_layer_structure\": [int(x) for x in args.gat_conv_layer_structure.split(',')],\n",
    "        \"num_epochs\": args.num_epochs,\n",
    "        \"batch_size\": int(args.batch_size),\n",
    "        \"lr\": float(args.lr),\n",
    "        \"early_stopping_patience\": args.early_stopping_patience,\n",
    "        \"use_dropout\": args.use_dropout,\n",
    "        \"dropout\": args.dropout,\n",
    "        \"gradient_accumulation_steps\": args.gradient_accumulation_steps,\n",
    "        \"use_gradient_clipping\": args.use_gradient_clipping,\n",
    "        \"lr_scheduler_warmup_steps\": args.lr_scheduler_warmup_steps,\n",
    "        \"device_nr\": args.device_nr\n",
    "    }\n",
    "    params[\"unique_model_description\"] = (\n",
    "        f\"pnc_local_{gio.int_list_to_string(lst=params['point_net_conv_layer_structure_local_mlp'], delimiter='_')}_\"\n",
    "        f\"pnc_global_{gio.int_list_to_string(lst=params['point_net_conv_layer_structure_global_mlp'], delimiter='_')}_\"\n",
    "        f\"gat_conv_{gio.int_list_to_string(lst=params['gat_conv_layer_structure'], delimiter='_')}_\"\n",
    "        f\"use_dropout_{params['use_dropout']}_\"\n",
    "        f\"dropout_{params['dropout']}_\"\n",
    "        f\"predict_mode_stats_{params['predict_mode_stats']}\"\n",
    "    )\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch number: 1\n",
      "Loaded 50 items into datalist\n"
     ]
    }
   ],
   "source": [
    "datalist = []\n",
    "batch_num = 1\n",
    "while True and batch_num < 2:\n",
    "    print(f\"Processing batch number: {batch_num}\")\n",
    "    # total_memory, available_memory, used_memory = get_memory_info()\n",
    "    # print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "    # print(f\"Available Memory: {available_memory:.2f} GB\")\n",
    "    # print(f\"Used Memory: {used_memory:.2f} GB\")\n",
    "    batch_file = os.path.join(dataset_path, f'datalist_batch_{batch_num}.pt')\n",
    "    if not os.path.exists(batch_file):\n",
    "        break\n",
    "    batch_data = torch.load(batch_file, map_location='cpu')\n",
    "    if isinstance(batch_data, list):\n",
    "        datalist.extend(batch_data)\n",
    "    batch_num += 1\n",
    "print(f\"Loaded {len(datalist)} items into datalist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the argparse section with this:\n",
    "args = {\n",
    "    \"in_channels\": 13,\n",
    "    \"out_channels\": 1,\n",
    "    \"predict_mode_stats\": False,\n",
    "    \"point_net_conv_layer_structure_local_mlp\": \"64\",\n",
    "    \"point_net_conv_layer_structure_global_mlp\": \"256\",\n",
    "    \"gat_conv_layer_structure\": \"128\",\n",
    "    \"num_epochs\": 3000,\n",
    "    \"batch_size\": 8,\n",
    "    \"lr\": 0.001,\n",
    "    \"early_stopping_patience\": 100,\n",
    "    \"use_dropout\": False,\n",
    "    \"dropout\": 0.3,\n",
    "    \"gradient_accumulation_steps\": 3,\n",
    "    \"use_gradient_clipping\": True,\n",
    "    \"lr_scheduler_warmup_steps\": 10000,\n",
    "    \"device_nr\": 0\n",
    "}\n",
    "\n",
    "# Convert the dictionary to an object with attributes\n",
    "class Args:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "args = Args(**args)\n",
    "\n",
    "hf.set_random_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU 1 with CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "gpus = hf.get_available_gpus()\n",
    "best_gpu = hf.select_best_gpu(gpus)\n",
    "hf.set_cuda_visible_device(best_gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "params = get_parameters(args)\n",
    "\n",
    "# Create base directory for the run\n",
    "base_dir = '../../data/' + params['project_name'] + '/'\n",
    "unique_run_dir = os.path.join(base_dir, params['unique_model_description'])\n",
    "os.makedirs(unique_run_dir, exist_ok=True)\n",
    "\n",
    "model_save_path, path_to_save_dataloader = hf.get_paths(base_dir=base_dir, unique_model_description= params['unique_model_description'], model_save_path= 'trained_model/model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into subsets...\n",
      "Total dataset length: 50\n",
      "Training subset length: 40\n",
      "Validation subset length: 7\n",
      "Test subset length: 3\n",
      "Split complete. Train: 40, Valid: 7, Test: 3\n",
      "3\n",
      "Normalizing test set...\n",
      "<torch.utils.data.dataset.Subset object at 0x7efe4454c790>\n",
      "Fitting and normalizing x features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting scaler: 100%|██████████| 1/1 [00:00<00:00, 40.90it/s]\n",
      "Normalizing x features: 100%|██████████| 1/1 [00:00<00:00, 71.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x features normalized\n",
      "Fitting and normalizing pos features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting scaler: 100%|██████████| 1/1 [00:00<00:00, 98.96it/s]\n",
      "Normalizing pos features: 100%|██████████| 1/1 [00:00<00:00, 81.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos features normalized\n",
      "Data(edge_index=[2, 59135], num_nodes=31140, x=[31140, 13], pos=[31140, 3, 2], y=[31140, 1], mode_stats=[6, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../../data/runs_21_10_2024/pnc_local_[64]_pnc_global_[256]_gat_conv_[128]_use_dropout_False_dropout_0.3_predict_mode_stats_False/data_created_during_training/test_x_scaler.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size= params['batch_size']\n",
    "path_to_save_dataloader= path_to_save_dataloader\n",
    "\n",
    "print(\"Splitting into subsets...\")\n",
    "train_set, valid_set, test_set = gio.split_into_subsets(dataset=datalist, train_ratio=0.8, val_ratio=0.15, test_ratio=0.05)\n",
    "print(f\"Split complete. Train: {len(train_set)}, Valid: {len(valid_set)}, Test: {len(test_set)}\")\n",
    "\n",
    "\n",
    "\n",
    "print(len(test_set))\n",
    "print(\"Normalizing test set...\")\n",
    "first_element = test_set\n",
    "print(first_element)\n",
    "test_set_normalized, scalers_test = hf.normalize_dataset(dataset_input=test_set, directory_path=path_to_save_dataloader + \"test_\")\n",
    "first_element_normalized = test_set_normalized[0]\n",
    "print(first_element_normalized)\n",
    "# print(\"Test set normalized\")\n",
    "\n",
    "# print(\"Creating test loader...\")\n",
    "test_loader = DataLoader(dataset=test_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=gio.collate_fn, worker_init_fn=hf.seed_worker)\n",
    "# print(\"Test loader created\")\n",
    "\n",
    "# joblib.dump(scalers_train['x_scaler'], os.path.join(path_to_save_dataloader, 'train_x_scaler.pkl'))\n",
    "# joblib.dump(scalers_train['pos_scaler'], os.path.join(path_to_save_dataloader, 'train_pos_scaler.pkl'))\n",
    "# joblib.dump(scalers_train['modestats_scaler'], os.path.join(path_to_save_dataloader, 'train_mode_stats_scaler.pkl'))\n",
    "\n",
    "# joblib.dump(scalers_validation['x_scaler'], os.path.join(path_to_save_dataloader, 'validation_x_scaler.pkl'))\n",
    "# joblib.dump(scalers_validation['pos_scaler'], os.path.join(path_to_save_dataloader, 'validation_pos_scaler.pkl'))\n",
    "# joblib.dump(scalers_validation['modestats_scaler'], os.path.join(path_to_save_dataloader, 'validation_mode_stats_scaler.pkl'))\n",
    "\n",
    "joblib.dump(scalers_test['x_scaler'], os.path.join(path_to_save_dataloader, 'test_x_scaler.pkl'))\n",
    "# joblib.dump(scalers_test['pos_scaler'], os.path.join(path_to_save_dataloader, 'test_pos_scaler.pkl'))\n",
    "# joblib.dump(scalers_test['modestats_scaler'], os.path.join(path_to_save_dataloader, 'test_mode_stats_scaler.pkl'))  \n",
    "\n",
    "# gio.save_dataloader(test_loader, path_to_save_dataloader + 'test_dl.pt')\n",
    "# gio.save_dataloader_params(test_loader, path_to_save_dataloader + 'test_loader_params.json')\n",
    "# print(\"Dataloaders and scalers saved\")\n",
    "        \n",
    "#         return train_loader, val_loader\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in prepare_data_with_graph_features: {str(e)}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         raise\n",
    "\n",
    "# train_dl, valid_dl = prepare_data_with_graph_features(datalist=datalist, batch_size= params['batch_size'], path_to_save_dataloader= path_to_save_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[0].pos.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3321, -0.4309, -0.3926,  0.2395,  0.0553,  1.0689,  0.7439,  0.3491,\n",
       "         1.0819,  1.1068, -0.1794, -0.2104, -0.2053], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_normalized[0].x[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x = joblib.load(os.path.join(path_to_save_dataloader, 'test_x_scaler.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        4.        , 20.03709686,  0.        ,  0.        ,  1.        ,\n",
       "        1.        ,  1.        ,  0.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_x.inverse_transform(test_set_normalized[0].x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# config = hf.setup_wandb(params['project_name'], {param: params[param] for param in PARAMETERS})\n",
    "\n",
    "# gnn_instance = garch.MyGnn(in_channels=config.in_channels, \n",
    "#                 out_channels=config.out_channels, \n",
    "#                 point_net_conv_layer_structure_local_mlp=config.point_net_conv_layer_structure_local_mlp,\n",
    "#                 point_net_conv_layer_structure_global_mlp=config.point_net_conv_layer_structure_global_mlp,\n",
    "#                 gat_conv_layer_structure=config.gat_conv_layer_structure,\n",
    "#                 use_dropout=config.use_dropout, \n",
    "#                 dropout=config.dropout, \n",
    "#                 predict_mode_stats=config.predict_mode_stats)\n",
    "\n",
    "# model = gnn_instance.to(device)\n",
    "# loss_fct = torch.nn.MSELoss()\n",
    "\n",
    "# baseline_loss_mean_target = gio.compute_baseline_of_mean_target(dataset=train_dl, loss_fct=loss_fct)\n",
    "# baseline_loss = gio.compute_baseline_of_no_policies(dataset=train_dl, loss_fct=loss_fct)\n",
    "# print(\"baseline loss mean \" + str(baseline_loss_mean_target))\n",
    "# print(\"baseline loss no  \" + str(baseline_loss) )\n",
    "\n",
    "# early_stopping = gio.EarlyStopping(patience=params['early_stopping_patience'], verbose=True)\n",
    "# best_val_loss, best_epoch = garch.train(model=model, \n",
    "#             config=config, \n",
    "#             loss_fct=loss_fct,\n",
    "#             optimizer=torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=1e-4),\n",
    "#             train_dl=train_dl, \n",
    "#             valid_dl=valid_dl,\n",
    "#             device=device, \n",
    "#             early_stopping=early_stopping,\n",
    "#             model_save_path=model_save_path)\n",
    "# print(f'Best model saved to {model_save_path} with validation loss: {best_val_loss} at epoch {best_epoch}')  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenhao-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the 'scripts' directory to the Python path\n",
    "scripts_path = os.path.abspath(os.path.join('..'))\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "    \n",
    "import joblib\n",
    "\n",
    "# Now you can import the gnn_io module\n",
    "import gnn_io as gio\n",
    "\n",
    "import gnn_architectures as garch\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler created for x values: StandardScaler()\n",
      "Scaler created for x values: StandardScaler()\n",
      "Scaler created for x values: StandardScaler()\n",
      "Scaler created for x values: StandardScaler()\n",
      "Scaler created for pos features: StandardScaler()\n",
      "Scaler created for y values: StandardScaler()\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "# \n",
    "# Create base directory for the run\n",
    "base_dir = '../../data/runs_1234_bliblablu/'\n",
    "unique_run_dir = os.path.join(base_dir, \"this_is_it\")\n",
    "os.makedirs(unique_run_dir, exist_ok=True)\n",
    "\n",
    "# Define the paths here\n",
    "def get_paths(base_dir, unique_model_description):\n",
    "    data_path = os.path.join(base_dir, unique_model_description)\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    model_save_path = os.path.join(data_path, 'trained_model/model.pth')\n",
    "    path_to_save_dataloader = os.path.join(data_path, 'data_created_during_training/')\n",
    "    config_save_path = os.path.join(data_path, 'trained_models/config.json')\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    os.makedirs(path_to_save_dataloader, exist_ok=True)\n",
    "    data_dict_list = torch.load('../../data/train_data/dataset_1pm_0-4400.pt')\n",
    "    return data_dict_list, model_save_path, config_save_path, path_to_save_dataloader\n",
    "\n",
    "def create_dataloaders_and_save_test_set(dataset_normalized, batch_size, path_to_save_dataloader):\n",
    "    train_dl, valid_dl, test_dl = gio.create_dataloaders(batch_size=batch_size, dataset=dataset_normalized, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "    gio.save_dataloader(test_dl, path_to_save_dataloader + 'test_dl.pt')\n",
    "    gio.save_dataloader_params(test_dl, path_to_save_dataloader + 'test_loader_params.json')\n",
    "    return train_dl, valid_dl\n",
    "\n",
    "def prepare_data(data_dict_list, indices_of_datasets_to_use, path_to_save_dataloader):\n",
    "    datalist = [Data(x=d['x'], edge_index=d['edge_index'], pos=d['pos'], y=d['y']) for d in data_dict_list]\n",
    "    dataset_only_relevant_dimensions = gio.cut_dimensions(dataset=datalist, indices_of_dimensions_to_keep=indices_of_datasets_to_use)\n",
    "    dataset_normalized = gio.normalize_dataset(dataset_only_relevant_dimensions, y_scalar=None, x_scalar_list=None, pos_scalar=None, directory_path=path_to_save_dataloader)\n",
    "    return dataset_normalized\n",
    "\n",
    "data_dict_list, model_save_path, config_save_path, path_to_save_dataloader = get_paths(base_dir, \"this_is_it\")\n",
    "dataset_normalized = prepare_data(data_dict_list, [0,1,3,4], path_to_save_dataloader)\n",
    "\n",
    "# train_dl, valid_dl = create_dataloaders_and_save_test_set(dataset_normalized, 16, path_to_save_dataloader)\n",
    "\n",
    "# config = setup_wandb(params['project_name'], {\n",
    "#     \"epochs\": params['num_epochs'],\n",
    "#     \"batch_size\": params['batch_size'],\n",
    "#     \"lr\": params['lr'],\n",
    "#     \"gradient_accumulation_steps\": params['gradient_accumulation_steps'],\n",
    "#     \"early_stopping_patience\": params['early_stopping_patience'],\n",
    "#     \"hidden_layers_base_for_point_net_conv\": params['hidden_layers_base_for_point_net_conv'],\n",
    "#     \"hidden_layer_structure\": params['hidden_layer_structure'],\n",
    "#     \"indices_to_use\": params['indices_of_datasets_to_use'],\n",
    "#     \"dataset_length\": len(dataset_normalized), \n",
    "#     \"in_channels\": params['in_channels'],\n",
    "#     \"out_channels\": params['out_channels'],\n",
    "# })\n",
    "\n",
    "# gnn_instance = garch.MyGnn(in_channels=6, out_channels=1, hidden_layers_base_for_point_net_conv=64, hidden_layer_structure=[64,128])\n",
    "# model = gnn_instance.to(device)\n",
    "loss_fct = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_y_normalized: \n",
      "5.1006186e-09\n",
      "median_y_normalized: \n",
      "-0.06859979\n",
      "Mean y normalized tensor: \n",
      "torch.Size([133854208, 1])\n",
      "tensor([[ 0.3793],\n",
      "        [-0.6240],\n",
      "        [ 1.3825],\n",
      "        [ 0.8092],\n",
      "        [ 0.8450],\n",
      "        [-0.7673],\n",
      "        [-0.7673],\n",
      "        [-0.0686],\n",
      "        [-0.1223],\n",
      "        [-1.8601]])\n",
      "Target tensor: \n",
      "torch.Size([133854208, 1])\n",
      "tensor([[5.1006e-09],\n",
      "        [5.1006e-09],\n",
      "        [5.1006e-09],\n",
      "        [5.1006e-09],\n",
      "        [5.1006e-09],\n",
      "        [5.1006e-09],\n",
      "        [5.1006e-09],\n",
      "        [5.1006e-09],\n",
      "        [5.1006e-09],\n",
      "        [5.1006e-09]])\n",
      "no policies \n",
      "torch.Size([133854208, 1])\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "torch.Size([133854208, 1])\n",
      "tensor([[ 0.3793],\n",
      "        [-0.6240],\n",
      "        [ 1.3825],\n",
      "        [ 0.8092],\n",
      "        [ 0.8450],\n",
      "        [-0.7673],\n",
      "        [-0.7673],\n",
      "        [-0.0686],\n",
      "        [-0.1223],\n",
      "        [-1.8601]])\n",
      "baseline loss 1.0\n",
      "baeline loss no policies 1.0\n"
     ]
    }
   ],
   "source": [
    "baseline_loss_mean_target = gio.compute_baseline_of_mean_target(dataset=dataset_normalized, loss_fct=loss_fct)\n",
    "baseline_loss = gio.compute_baseline_of_no_policies(dataset=dataset_normalized, loss_fct=loss_fct)\n",
    "print(\"baseline loss \" + str(baseline_loss_mean_target) )\n",
    "print(\"baeline loss no policies \" + str(baseline_loss) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no policies \n",
      "torch.Size([133854208, 1])\n",
      "tensor([[1.5000],\n",
      "        [1.5000],\n",
      "        [1.5000],\n",
      "        [1.5000],\n",
      "        [1.5000],\n",
      "        [1.5000],\n",
      "        [1.5000],\n",
      "        [1.5000],\n",
      "        [1.5000],\n",
      "        [1.5000]])\n",
      "torch.Size([133854208, 1])\n",
      "tensor([[ 0.3793],\n",
      "        [-0.6240],\n",
      "        [ 1.3825],\n",
      "        [ 0.8092],\n",
      "        [ 0.8450],\n",
      "        [-0.7673],\n",
      "        [-0.7673],\n",
      "        [-0.0686],\n",
      "        [-0.1223],\n",
      "        [-1.8601]])\n",
      "tensor(3.2500)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "actual_difference_vol_car = np.concatenate([data.y for data in dataset_normalized])\n",
    "\n",
    "target_tensor = 1.5 * np.ones(actual_difference_vol_car.shape) # presume no difference in vol car due to policy\n",
    "\n",
    "target_tensor = torch.tensor(target_tensor, dtype=torch.float32)\n",
    "actual_difference_vol_car = torch.tensor(actual_difference_vol_car, dtype=torch.float32)\n",
    "\n",
    "print('no policies ')\n",
    "print(target_tensor.shape)\n",
    "print(target_tensor[:10])\n",
    "print(actual_difference_vol_car.shape)\n",
    "print(actual_difference_vol_car[:10])\n",
    "\n",
    "# Compute the loss\n",
    "loss = loss_fct(actual_difference_vol_car, target_tensor)\n",
    "print(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_baseline_of_mean_target() missing 1 required positional argument: 'loss_fct'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gio\u001b[39m.\u001b[39;49mcompute_baseline_of_mean_target(dataset\u001b[39m=\u001b[39;49mdataset_normalized, )\n",
      "\u001b[0;31mTypeError\u001b[0m: compute_baseline_of_mean_target() missing 1 required positional argument: 'loss_fct'"
     ]
    }
   ],
   "source": [
    "gio.compute_baseline_of_mean_target(dataset=dataset_normalized, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_hidden_layer_structure(list_of_halfs_and_duplicates: list, hidden_layer_size: int):\n",
    "    \"\"\"\n",
    "    Generates a list of hidden layer sizes based on an initial size and a list of instructions.\n",
    "\n",
    "    Parameters:\n",
    "    list_of_halfs_and_duplicates (list): List of instructions where 1 means double the size,\n",
    "                                         0 means the same size, and -1 means half the size.\n",
    "    hidden_layer_size (int): The initial size of the hidden layer.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of integers representing the sizes of the hidden layers.\n",
    "    \"\"\"\n",
    "    if not all(isinstance(i, int) and i in [-1, 0, 1] for i in list_of_halfs_and_duplicates):\n",
    "        raise ValueError(\"list_of_halfs_and_duplicates must contain only -1, 0, or 1.\")\n",
    "    if hidden_layer_size <= 0:\n",
    "        raise ValueError(\"hidden_layer_size must be a positive integer.\")\n",
    "    \n",
    "    result_list = [hidden_layer_size]\n",
    "    for i in list_of_halfs_and_duplicates:\n",
    "        if i == 1:\n",
    "            result_list.append(int(result_list[-1] * 2))\n",
    "        elif i == 0:\n",
    "            result_list.append(result_list[-1])\n",
    "        elif i == -1:\n",
    "            result_list.append(int(result_list[-1] / 2))\n",
    "    result_list.append(hidden_layer_size)\n",
    "    return result_list\n",
    "        \n",
    "hidden_layer_structure = define_hidden_layer_structure([1, -1, 0, 1, 0], 16)\n",
    "\n",
    "def define_layers(hidden_layer_structure: list[int], gat_and_conv_structure: list[int]) -> list:\n",
    "    \"\"\"\n",
    "    Generates a list of GNN layers and ReLU activations based on the provided hidden layer structure.\n",
    "\n",
    "    Parameters:\n",
    "    hidden_layer_structure (list[int]): A list of integers representing the sizes of the hidden layers.\n",
    "    gat_and_conv_structure (list[int]): A list specifying the type of GNN layer to use. \n",
    "        Use 1 for 'GATConv' and -1 for 'GCNConv'.\n",
    "        Note that the size of hidden_layer_structure must be the size of gat_and_conv_structure + 1.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of tuples and ReLU activations, where each tuple contains a GNN layer and a string describing the data flow.\n",
    "    \n",
    "    Raises:\n",
    "    ValueError: If an invalid layer type is specified or if the input lengths are incompatible.\n",
    "    \"\"\"\n",
    "    if len(hidden_layer_structure) != len(gat_and_conv_structure) + 1:\n",
    "        raise ValueError(\"The size of hidden_layer_structure must be the size of gat_and_conv_structure + 1.\")\n",
    "\n",
    "    # Mapping layer types to their corresponding classes\n",
    "    layer_types = {\n",
    "        1: torch_geometric.nn.GATConv,\n",
    "        -1: torch_geometric.nn.GCNConv\n",
    "    }\n",
    "\n",
    "    layers = []\n",
    "    for idx in range(len(hidden_layer_structure) - 1):\n",
    "        layer_type = gat_and_conv_structure[idx]\n",
    "        if layer_type in layer_types:\n",
    "            layer_class = layer_types[layer_type]\n",
    "            layers.append((layer_class(hidden_layer_structure[idx], hidden_layer_structure[idx + 1]), 'x, edge_index -> x'))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid layer_type. Choose 1 for 'GATConv' or -1 for 'GCNConv'.\")\n",
    "        layers.append(torch.nn.ReLU(inplace=True))\n",
    "    \n",
    "    return layers\n",
    "\n",
    "layers = define_layers(hidden_layer_structure=hidden_layer_structure, gat_and_conv_structure=[1, -1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 32, 16, 16, 32, 32, 16]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(GATConv(16, 32, heads=1), 'x, edge_index -> x'),\n",
       " ReLU(inplace=True),\n",
       " (GCNConv(32, 16), 'x, edge_index -> x'),\n",
       " ReLU(inplace=True),\n",
       " (GATConv(16, 16, heads=1), 'x, edge_index -> x'),\n",
       " ReLU(inplace=True),\n",
       " (GATConv(16, 32, heads=1), 'x, edge_index -> x'),\n",
       " ReLU(inplace=True),\n",
       " (GATConv(32, 32, heads=1), 'x, edge_index -> x'),\n",
       " ReLU(inplace=True),\n",
       " (GATConv(32, 16, heads=1), 'x, edge_index -> x'),\n",
       " ReLU(inplace=True)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters \n",
    "num_epochs = 1000\n",
    "project_name = \"try_overfitting_3\"\n",
    "path_to_save_dataloader = \"../../data/data_created_during_training_needed_for_testing/\"\n",
    "indices_of_datasets_to_use = [0, 1, 3, 4]\n",
    "\n",
    "loss_fct = torch.nn.MSELoss()\n",
    "batch_size = 4\n",
    "output_layer_parameter = 'gat'\n",
    "hidden_size_parameter = 64\n",
    "gat_layer_parameter = 5\n",
    "gcn_layer_parameter = 0\n",
    "lr = 0.001\n",
    "in_channels = len(indices_of_datasets_to_use) + 2 # dimensions of the x vector + 2 (pos)\n",
    "out_channels = 1 # we are predicting one value\n",
    "early_stopping_patience = 10\n",
    "\n",
    "data_dict_list = torch.load('../../data/train_data/dataset_1pm_0-3500_new.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the Data objects\n",
    "datalist = [Data(x=d['x'], edge_index=d['edge_index'], pos=d['pos'], y=d['y']) for d in data_dict_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist_new=datalist[0:3000]\n",
    "\n",
    "# for data in datalist[3200: len(datalist)]:\n",
    "#     datalist_new.append(data)\n",
    "    \n",
    "dataset_length = len(datalist_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input normalisation: standardScalar\n",
      "Scaler created for x values: StandardScaler()\n",
      "Scaler created for x values: StandardScaler()\n",
      "Scaler created for x values: StandardScaler()\n",
      "Scaler created for x values: StandardScaler()\n",
      "Scaler created for pos features: StandardScaler()\n"
     ]
    }
   ],
   "source": [
    "dataset_only_relevant_dimensions = gio.cut_dimensions(dataset=datalist_new, indices_of_dimensions_to_keep=indices_of_datasets_to_use)\n",
    "dataset_normalized = gio.normalize_dataset(dataset=dataset_only_relevant_dimensions, y_scalar=None, x_scalar_list=None, pos_scalar=None, directory_path=path_to_save_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline error no policies: 0.3216273784637451\n",
      "Baseline error mean: 0.0032576550729572773\n"
     ]
    }
   ],
   "source": [
    "baseline_error = gio.compute_baseline_of_no_policies(dataset=dataset_normalized, loss_fct=loss_fct)\n",
    "print(f'Baseline error no policies: {baseline_error}')\n",
    "\n",
    "baseline_error = gio.compute_baseline_of_mean_target(dataset=dataset_normalized, loss_fct=loss_fct)\n",
    "print(f'Baseline error mean: {baseline_error}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the model\n",
    "\n",
    "We first find a good model for one batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset length: 3000\n",
      "Training subset length: 2100\n",
      "Validation subset length: 450\n",
      "Test subset length: 450\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl, test_dl = gio.create_dataloaders(batch_size = batch_size, dataset=dataset_normalized, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "gio.save_dataloader(test_dl, path_to_save_dataloader + 'test_dl_' + unique_model_description + '.pt')\n",
    "gio.save_dataloader_params(test_dl, path_to_save_dataloader + 'test_loader_params_' + unique_model_description+ '.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with 2 GPUS\n",
      "Name is  NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Running with {torch.cuda.device_count()} GPUS\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Name is \", torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33menatterer\u001b[0m (\u001b[33mtum-traffic-engineering\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/enatterer/Development/gnn_predicting_effects_of_traffic_policies/scripts/training/wandb/run-20240720_064542-a9b2wid2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tum-traffic-engineering/try_overfitting_3/runs/a9b2wid2' target=\"_blank\">glorious-plasma-3</a></strong> to <a href='https://wandb.ai/tum-traffic-engineering/try_overfitting_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tum-traffic-engineering/try_overfitting_3' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/try_overfitting_3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tum-traffic-engineering/try_overfitting_3/runs/a9b2wid2' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/try_overfitting_3/runs/a9b2wid2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_layer:  gat\n",
      "hidden_size:  64\n",
      "gat_layers:  5\n",
      "gcn_layers:  0\n",
      "Model initialized\n",
      "MyGnnHardCoded(\n",
      "  (pointLayer): PointNetConv(local_nn=Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  ), global_nn=Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "  ))\n",
      "  (graph_layers): Sequential(\n",
      "    (0) - GATConv(64, 128, heads=1): x, edge_index -> x\n",
      "    (1) - ReLU(inplace=True): x -> x\n",
      "    (2) - GATConv(128, 32, heads=1): x, edge_index -> x\n",
      "    (3) - ReLU(inplace=True): x -> x\n",
      "    (4) - GATConv(32, 128, heads=1): x, edge_index -> x\n",
      "    (5) - ReLU(inplace=True): x -> x\n",
      "    (6) - GATConv(128, 64, heads=1): x, edge_index -> x\n",
      "    (7) - ReLU(inplace=True): x -> x\n",
      "    (8) - GATConv(64, 64, heads=1): x, edge_index -> x\n",
      "    (9) - ReLU(inplace=True): x -> x\n",
      "  )\n",
      "  (output_layer): GATConv(64, 1, heads=1)\n",
      ")\n",
      "Loaded checkpoint from epoch 9 with val_loss 0.0031441194044773356 and train_loss 0.0031441194005310535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:51, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, validation loss: 0.0031139697319110936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:51, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, validation loss: 0.0030884561128914356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:52,  9.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, validation loss: 0.0030703635420650244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:50, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, validation loss: 0.003056793685887108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:51, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, validation loss: 0.0030368021689355373\n",
      "Model checkpoint saved at epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:51, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, validation loss: 0.003034990280866623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:51, 10.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, validation loss: 0.002995197904353912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:52, 10.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, validation loss: 0.003033470366545747\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:50, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, validation loss: 0.00297945411875844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:53,  9.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, validation loss: 0.002970233248480785\n",
      "Model checkpoint saved at epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:52,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, validation loss: 0.0029909785371273756\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:52, 10.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11, validation loss: 0.0029343392316713533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:52,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, validation loss: 0.0029296712080362887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:50, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13, validation loss: 0.0029019871087893182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:53,  9.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, validation loss: 0.0029067131752494426\n",
      "Model checkpoint saved at epoch 14\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:51, 10.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, validation loss: 0.0029109576634601154\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:51, 10.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16, validation loss: 0.0029723022660586686\n",
      "EarlyStopping counter: 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:51, 10.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17, validation loss: 0.0028666958307633095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:51, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18, validation loss: 0.002884331624954939\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:52,  9.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19, validation loss: 0.002844018628820777\n",
      "Model checkpoint saved at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:51, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, validation loss: 0.0029125306034147474\n",
      "EarlyStopping counter: 1 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:53,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21, validation loss: 0.002852027990599781\n",
      "EarlyStopping counter: 2 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:51, 10.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22, validation loss: 0.00285186804831028\n",
      "EarlyStopping counter: 3 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:51, 10.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23, validation loss: 0.0028821455780416727\n",
      "EarlyStopping counter: 4 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:52, 10.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24, validation loss: 0.002897255790246799\n",
      "Model checkpoint saved at epoch 24\n",
      "EarlyStopping counter: 5 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "525it [00:52, 10.05it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb.init(\n",
    "    project=project_name,\n",
    "    config={\n",
    "        \"epochs\": num_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": lr,\n",
    "        \"early_stopping_patience\": 10,\n",
    "        \"hidden_layer_size\": hidden_size_parameter,\n",
    "        \"gat_layers\": gat_layer_parameter,\n",
    "        \"gcn_layers\": gcn_layer_parameter,\n",
    "        \"output_layer\": output_layer_parameter,\n",
    "        \"indices_to_use\": indices_of_datasets_to_use,\n",
    "        \"dataset_length\": dataset_length\n",
    "    }\n",
    ")\n",
    "config = wandb.config\n",
    "\n",
    "print(\"output_layer: \", output_layer_parameter)\n",
    "print(\"hidden_size: \", hidden_size_parameter)\n",
    "print(\"gat_layers: \", gat_layer_parameter)\n",
    "print(\"gcn_layers: \", gcn_layer_parameter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "early_stopping = gio.EarlyStopping(patience=early_stopping_patience, verbose=True)\n",
    "# torch.set_printoptions(precision=4, sci_mode=False)\n",
    "\n",
    "# gnn_instance = garch.MyGnn(in_channels=in_channels, out_channels=out_channels, hidden_size=hidden_size_parameter, gat_layers=gat_layer_parameter, gcn_layers=gcn_layer_parameter, output_layer=output_layer_parameter)\n",
    "\n",
    "gnn_instance = garch.MyGnnHardCoded(in_channels=in_channels, out_channels=out_channels, hidden_size=hidden_size_parameter, output_layer=output_layer_parameter)\n",
    "\n",
    "model = gnn_instance.to(device)\n",
    "\n",
    "best_val_loss, best_epoch = garch.train(model, config=config, \n",
    "                                loss_fct=loss_fct, \n",
    "                                optimizer=torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.0),\n",
    "                                train_dl=train_dl, valid_dl=valid_dl,\n",
    "                                device=device, early_stopping=early_stopping,\n",
    "                                use_existing_checkpoint=True, path_existing_checkpoints = \"../../data/checkpoints_batchsize_8/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = \"../../data/trained_models/model_1.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_path = '../../data/trained_models/model_' + unique_model_description + '.pth'\n",
    "\n",
    "# # Save the model state dictionary and configuration\n",
    "# torch.save({\n",
    "#     'state_dict': model.state_dict(),\n",
    "#     'config': {\n",
    "#         'in_channels': model.in_channels,\n",
    "#         'out_channels': model.out_channels,\n",
    "#         'hidden_size': model.hidden_size,\n",
    "#         'gat_layers': model.gat_layers,\n",
    "#         'gcn_layers': model.gcn_layers,ls\n",
    "#         'output_layer': model.output_layer\n",
    "#     }\n",
    "# }, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenhao-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

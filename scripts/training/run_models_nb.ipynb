{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import wandb\n",
    "import random\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import signal\n",
    "import joblib\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "\n",
    "# Add the 'scripts' directory to the Python path\n",
    "scripts_path = os.path.abspath(os.path.join('..'))\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "    \n",
    "import gnn_io as gio\n",
    "import gnn_architectures as garch\n",
    "    \n",
    "def get_available_gpus():\n",
    "    command = \"nvidia-smi --query-gpu=index,utilization.gpu,memory.free --format=csv,noheader,nounits\"\n",
    "    result = subprocess.run(command.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"Error executing nvidia-smi: {result.stderr.decode('utf-8')}\")\n",
    "    gpu_info = result.stdout.decode('utf-8').strip().split('\\n')\n",
    "    gpus = []\n",
    "    for info in gpu_info:\n",
    "        index, utilization, memory_free = info.split(', ')\n",
    "        gpus.append({\n",
    "            'index': int(index),\n",
    "            'utilization': int(utilization),\n",
    "            'memory_free': int(memory_free)\n",
    "        })\n",
    "    return gpus\n",
    "    \n",
    "def select_best_gpu(gpus):\n",
    "    # Sort by free memory (descending) and then by utilization (ascending)\n",
    "    gpus = sorted(gpus, key=lambda x: (-x['memory_free'], x['utilization']))\n",
    "    return gpus[0]['index']\n",
    "\n",
    "def set_cuda_visible_device(gpu_index):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_index)\n",
    "    print(f\"Using GPU {gpu_index} with CUDA_VISIBLE_DEVICES={os.environ['CUDA_VISIBLE_DEVICES']}\")\n",
    "\n",
    "# Define the paths here\n",
    "def get_paths(base_dir, unique_model_description):\n",
    "    data_path = os.path.join(base_dir, unique_model_description)\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    model_save_path = os.path.join(data_path, 'trained_model/model.pth')\n",
    "    path_to_save_dataloader = os.path.join(data_path, 'data_created_during_training/')\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    os.makedirs(path_to_save_dataloader, exist_ok=True)\n",
    "    data_dict_list = torch.load('../../data/train_data/dataset_1pm_0-5000.pt')\n",
    "    return data_dict_list, model_save_path, path_to_save_dataloader\n",
    "\n",
    "# Define parameters\n",
    "def get_parameters(args):\n",
    "        project_name = \"runs_NEW\"\n",
    "        indices_of_datasets_to_use = [0, 1, 3, 4]\n",
    "        num_epochs = 1000\n",
    "        in_channels = len(indices_of_datasets_to_use) + 2\n",
    "        out_channels = 1\n",
    "        lr = float(args.lr)\n",
    "        batch_size = int(args.batch_size)\n",
    "        hidden_layers_base_for_point_net_conv = int(args.hidden_layers_base_for_point_net_conv)\n",
    "        hidden_layer_structure = [int(x) for x in args.hidden_layer_structure.split(',')]\n",
    "        gradient_accumulation_steps = int(args.gradient_accumulation_steps)\n",
    "        early_stopping_patience = int(args.early_stopping_patience)\n",
    "\n",
    "        unique_model_description = (\n",
    "            # f\"features_{gio.int_list_to_string(lst = indices_of_datasets_to_use, delimiter= '_')}_\"\n",
    "            # f\"batch_{batch_size}_\"\n",
    "            f\"hidden_{hidden_layers_base_for_point_net_conv}_\"\n",
    "            f\"hidden_layer_str_{gio.int_list_to_string(lst = hidden_layer_structure, delimiter='_')}_\"\n",
    "            # f\"gat_and_conv_structure_{gio.int_list_to_string(lst = gat_and_conv_structure, delimiter='_')}\"\n",
    "            # f\"lr_{lr}_\"\n",
    "            # f\"g_accumulation_steps_{gradient_accumulation_steps}\"\n",
    "            # f\"early_stopping_{early_stopping_patience}\"\n",
    "            # f\"in_channels_{in_channels}_\"\n",
    "            # f\"out_channels_{out_channels}_\"\n",
    "        )\n",
    "        return {\n",
    "            \"project_name\": project_name,\n",
    "            \"indices_of_datasets_to_use\": indices_of_datasets_to_use,\n",
    "            \"num_epochs\": num_epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"hidden_layers_base_for_point_net_conv\": hidden_layers_base_for_point_net_conv,\n",
    "            \"hidden_layer_structure\": hidden_layer_structure,\n",
    "            \"lr\": lr,\n",
    "            \"gradient_accumulation_steps\": gradient_accumulation_steps,\n",
    "            \"in_channels\": in_channels,\n",
    "            \"out_channels\": out_channels,\n",
    "            \"early_stopping_patience\": early_stopping_patience,\n",
    "            \"unique_model_description\": unique_model_description\n",
    "        }\n",
    "        \n",
    "def set_random_seeds():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "    np.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\n",
    "    torch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\n",
    "    torch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\n",
    "\n",
    "def setup_wandb(project_name, config):\n",
    "    wandb.login()\n",
    "    wandb.init(project=project_name, config=config)\n",
    "    return wandb.config\n",
    "        \n",
    "def train_model(config, train_dl, valid_dl, device, early_stopping, checkpoint_dir, model_save_path):\n",
    "    gnn_instance = garch.MyGnn(in_channels=config.in_channels, out_channels=config.out_channels, hidden_layers_base_for_point_net_conv=config.hidden_layers_base_for_point_net_conv, hidden_layer_structure=config.hidden_layer_structure)\n",
    "    model = gnn_instance.to(device)\n",
    "    loss_fct = torch.nn.MSELoss()\n",
    "    best_val_loss, best_epoch = garch.train(model=model, \n",
    "                config=config, \n",
    "                loss_fct=loss_fct,\n",
    "                optimizer=torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=0.0),\n",
    "                train_dl=train_dl, \n",
    "                valid_dl=valid_dl,\n",
    "                device=device, \n",
    "                early_stopping=early_stopping,\n",
    "                accumulation_steps=config.gradient_accumulation_steps,\n",
    "                use_existing_checkpoint=False, \n",
    "                path_existing_checkpoints=checkpoint_dir,\n",
    "                compute_r_squared=False,\n",
    "                model_save_path=model_save_path)\n",
    "    print(f'Best model saved to {model_save_path} with validation loss: {best_val_loss} at epoch {best_epoch}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# Call this function during training without the scalars and with the directory path, and during the testing with the saved scalars and without a directory path to save.\n",
    "def normalize_dataset_create_scaler(dataset_input, directory_path=None):\n",
    "    dataset = dataset_input.copy()\n",
    "    dataset, x_scaler = normalize_x_values_create_scalers(dataset, directory_path)\n",
    "    dataset, pos_scaler = normalize_positional_features_create_scaler(dataset, directory_path)\n",
    "    dataset, y_scaler = normalize_y_values_create_scaler(dataset, directory_path)\n",
    "    return dataset, x_scaler, pos_scaler, y_scaler\n",
    "\n",
    "def normalize_dataset_with_given_scaler(dataset_input, x_scalar_list = None, pos_scalar=None,  y_scalar=None):\n",
    "    dataset = dataset_input.copy()\n",
    "    dataset = normalize_x_values_given_scaler(dataset, x_scalar_list)\n",
    "    dataset = normalize_positional_features_given_scaler(dataset, pos_scalar)\n",
    "    dataset = normalize_y_values_given_scaler(dataset, y_scalar)\n",
    "    return dataset\n",
    "\n",
    "def normalize_x_values_given_scaler(dataset, x_scaler_list):\n",
    "    shape_of_x = dataset[0].x.shape[1]\n",
    "    for i in range(shape_of_x):\n",
    "        scaler = x_scaler_list[i]\n",
    "        for data in dataset:\n",
    "            data_x_dim = replace_invalid_values(data.x[:, i].reshape(-1, 1))\n",
    "            normalized_x_dim = torch.tensor(scaler.transform(data_x_dim.numpy()), dtype=torch.float)\n",
    "            if i == 0:\n",
    "                data.normalized_x = normalized_x_dim\n",
    "            else:\n",
    "                data.normalized_x = torch.cat((data.normalized_x, normalized_x_dim), dim=1)\n",
    "    for data in dataset:\n",
    "        data.x = data.normalized_x\n",
    "        del data.normalized_x\n",
    "    return dataset\n",
    "\n",
    "def normalize_positional_features_given_scaler(dataset, pos_scalar=None):\n",
    "    for data in dataset:\n",
    "        data.pos = torch.tensor(pos_scalar.transform(data.pos.numpy()), dtype=torch.float)\n",
    "    return dataset\n",
    "\n",
    "def normalize_y_values_given_scaler(dataset, y_scalar=None):\n",
    "    for data in dataset:\n",
    "        data.y = torch.tensor(y_scalar.transform(data.y.numpy()), dtype=torch.float)\n",
    "    return dataset\n",
    "\n",
    "def normalize_x_values_create_scalers(dataset, directory_path):\n",
    "    shape_of_x = dataset[0].x.shape[1]\n",
    "    list_of_scalers_to_save = []\n",
    "    x_values = torch.cat([data.x for data in dataset], dim=0)\n",
    "\n",
    "    for i in range(shape_of_x):\n",
    "        all_node_features = replace_invalid_values(x_values[:, i].reshape(-1, 1)).numpy()\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        print(f\"Scaler created for x values at index {i}: {scaler}\")\n",
    "        scaler.fit(all_node_features)\n",
    "        list_of_scalers_to_save.append(scaler)\n",
    "\n",
    "        for data in dataset:\n",
    "            data_x_dim = replace_invalid_values(data.x[:, i].reshape(-1, 1))\n",
    "            normalized_x_dim = torch.tensor(scaler.transform(data_x_dim.numpy()), dtype=torch.float)\n",
    "            if i == 0:\n",
    "                data.normalized_x = normalized_x_dim\n",
    "            else:\n",
    "                data.normalized_x = torch.cat((data.normalized_x, normalized_x_dim), dim=1)\n",
    "\n",
    "    joblib.dump(list_of_scalers_to_save, os.path.join(directory_path, 'x_scaler.pkl'))\n",
    "\n",
    "    for data in dataset:\n",
    "        data.x = data.normalized_x\n",
    "        del data.normalized_x\n",
    "    return dataset, list_of_scalers_to_save\n",
    "\n",
    "def normalize_positional_features_create_scaler(dataset, directory_path):\n",
    "    all_pos_features = torch.cat([data.pos for data in dataset], dim=0)\n",
    "    all_pos_features = replace_invalid_values(all_pos_features).numpy()\n",
    "    scaler = StandardScaler()\n",
    "    print(f\"Scaler created for pos features: {scaler}\")\n",
    "    scaler.fit(all_pos_features)\n",
    "    joblib.dump(scaler, os.path.join(directory_path, 'pos_scaler.pkl'))\n",
    "    for data in dataset:\n",
    "        data.pos = torch.tensor(scaler.transform(data.pos.numpy()), dtype=torch.float)\n",
    "    return dataset, scaler\n",
    "\n",
    "\n",
    "def normalize_y_values_create_scaler(dataset, directory_path):\n",
    "    all_y_values = torch.cat([data.y for data in dataset], dim=0).reshape(-1, 1)\n",
    "    all_y_values = replace_invalid_values(all_y_values).numpy()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    print(f\"Scaler created for y values: {scaler}\")\n",
    "    scaler.fit(all_y_values)\n",
    "    joblib.dump(scaler, os.path.join(directory_path, 'y_scaler.pkl'))\n",
    "\n",
    "    for data in dataset:\n",
    "        data.y = torch.tensor(scaler.transform(data.y.reshape(-1, 1).numpy()), dtype=torch.float)\n",
    "    return dataset, scaler\n",
    "\n",
    "def replace_invalid_values(tensor):\n",
    "    tensor[tensor != tensor] = 0  # replace NaNs with 0\n",
    "    tensor[tensor == float('inf')] = 0  # replace inf with 0\n",
    "    tensor[tensor == float('-inf')] = 0  # replace -inf with 0\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_list = torch.load('../../data/train_data/dataset_1pm_0-5000.pt')\n",
    "\n",
    "set_random_seeds()\n",
    "\n",
    "# Create base directory for the run\n",
    "base_dir = '../../data/runs_NEW/'\n",
    "unique_run_dir = os.path.join(base_dir, \"this_IS\")\n",
    "os.makedirs(unique_run_dir, exist_ok=True)\n",
    "\n",
    "data_dict_list, model_save_path, path_to_save_dataloader = get_paths(base_dir, \"this_IS\")\n",
    "\n",
    "indices_of_datasets_to_use=[0,1,3,4]\n",
    "batch_size= 8\n",
    "path_to_save_dataloader= path_to_save_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "compute_r_2 = True\n",
    "\n",
    "if compute_r_2:\n",
    "    print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset length: 4887\n",
      "Training subset length: 3420\n",
      "Validation subset length: 733\n",
      "Test subset length: 734\n",
      "Scaler created for x values at index 0: StandardScaler()\n",
      "Scaler created for x values at index 1: StandardScaler()\n",
      "Scaler created for x values at index 2: StandardScaler()\n",
      "Scaler created for x values at index 3: StandardScaler()\n",
      "Scaler created for pos features: StandardScaler()\n"
     ]
    }
   ],
   "source": [
    "# def prepare_data(data_dict_list, indices_of_datasets_to_use, batch_size, path_to_save_dataloader):\n",
    "indices_of_datasets_to_use=[0,1,3,4]\n",
    "batch_size= 8\n",
    "path_to_save_dataloader= path_to_save_dataloader\n",
    "\n",
    "def prepare_data(data_dict_list, indices_of_datasets_to_use, batch_size, path_to_save_dataloader, normalize_y, normalize_pos):\n",
    "    datalist = [Data(x=d['x'], edge_index=d['edge_index'], pos=d['pos'], y=d['y']) for d in data_dict_list]\n",
    "    dataset_only_relevant_dimensions = gio.cut_dimensions(dataset=datalist, indices_of_dimensions_to_keep=indices_of_datasets_to_use)\n",
    "    train_set, valid_set, test_set = gio.split_into_subsets(dataset=dataset_only_relevant_dimensions, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15)\n",
    "    # print(train_set[0].x[:10])\n",
    "    # print(train_set[0].y[:10])\n",
    "    if normalize_y and normalize_pos:\n",
    "        train_set_normalized, x_scaler, pos_scaler, y_scaler = gio.normalize_dataset_create_scaler(dataset_input = train_set, directory_path=path_to_save_dataloader, normalize_y=True, normalize_pos=True)\n",
    "        valid_set_normalized = gio.normalize_dataset_with_given_scaler(dataset_input=valid_set, x_scalar_list=x_scaler, pos_scalar=pos_scaler, y_scalar=y_scaler, normalize_y=True, normalize_pos=True)\n",
    "        test_set_normalized =  gio.normalize_dataset_with_given_scaler(dataset_input=test_set, x_scalar_list=x_scaler, pos_scalar=pos_scaler, y_scalar=y_scaler, normalize_y=True, normalize_pos=True) \n",
    "    if normalize_y and not normalize_pos:\n",
    "        train_set_normalized, x_scaler, y_scaler = gio.normalize_dataset_create_scaler(dataset_input = train_set, directory_path=path_to_save_dataloader, normalize_y=True, normalize_pos=False)\n",
    "        valid_set_normalized = gio.normalize_dataset_with_given_scaler(dataset_input=valid_set, x_scalar_list=x_scaler, pos_scalar=None, y_scalar=y_scaler, normalize_y=True, normalize_pos=False)\n",
    "        test_set_normalized =  gio.normalize_dataset_with_given_scaler(dataset_input=test_set, x_scalar_list=x_scaler, pos_scalar=None, y_scalar=y_scaler, normalize_y=True, normalize_pos=False) \n",
    "    if not normalize_y and normalize_pos:\n",
    "        train_set_normalized, x_scaler, pos_scaler = gio.normalize_dataset_create_scaler(dataset_input = train_set, directory_path=path_to_save_dataloader, normalize_y=False, normalize_pos=True)\n",
    "        valid_set_normalized = gio.normalize_dataset_with_given_scaler(dataset_input=valid_set, x_scalar_list=x_scaler, pos_scalar=pos_scaler, y_scalar= None,normalize_y=False, normalize_pos=True)\n",
    "        test_set_normalized =  gio.normalize_dataset_with_given_scaler(dataset_input=test_set, x_scalar_list=x_scaler, pos_scalar=pos_scaler,y_scalar=None, normalize_y=False, normalize_pos=True)\n",
    "    if not normalize_y and not normalize_pos:\n",
    "        train_set_normalized, x_scaler = gio.normalize_dataset_create_scaler(dataset_input = train_set, directory_path=path_to_save_dataloader, normalize_y=False, normalize_pos=False)\n",
    "        valid_set_normalized = gio.normalize_dataset_with_given_scaler(dataset_input=valid_set, x_scalar_list=x_scaler, pos_scalar=None, y_scalar= None, normalize_y=False, normalize_pos=False)\n",
    "        test_set_normalized =  gio.normalize_dataset_with_given_scaler(dataset_input=test_set, x_scalar_list=x_scaler, pos_scalar=None, y_scalar=None, normalize_y=False, normalize_pos=False)\n",
    "      \n",
    "    train_loader = DataLoader(dataset=train_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, prefetch_factor=2, pin_memory=True, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "    val_loader = DataLoader(dataset=valid_set_normalized, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "    test_loader = DataLoader(dataset=test_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "    gio.save_dataloader(test_loader, path_to_save_dataloader + 'test_dl.pt')\n",
    "    gio.save_dataloader_params(test_loader, path_to_save_dataloader + 'test_loader_params.json')\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "train_dl, valid_dl = prepare_data(data_dict_list=data_dict_list, indices_of_datasets_to_use=indices_of_datasets_to_use, batch_size= batch_size, path_to_save_dataloader= path_to_save_dataloader, normalize_y=False, normalize_pos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33menatterer\u001b[0m (\u001b[33mtum-traffic-engineering\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/enatterer/Development/gnn_predicting_effects_of_traffic_policies/scripts/training/wandb/run-20240726_173251-5zkphxgi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tum-traffic-engineering/test/runs/5zkphxgi' target=\"_blank\">firm-vortex-8</a></strong> to <a href='https://wandb.ai/tum-traffic-engineering/test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tum-traffic-engineering/test' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tum-traffic-engineering/test/runs/5zkphxgi' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/test/runs/5zkphxgi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PointNetConv(local_nn=Sequential(\n",
      "  (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "), global_nn=Sequential(\n",
      "  (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=32, out_features=128, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (6): ReLU()\n",
      "))\n",
      "Initializing 0.weight with kaiming_normal\n",
      "Initializing 0.bias with zeros\n",
      "Initializing 0.weight with kaiming_normal\n",
      "Initializing 0.bias with zeros\n",
      "Initializing 1.weight with kaiming_normal\n",
      "Initializing 1.bias with zeros\n",
      "Initializing 3.weight with kaiming_normal\n",
      "Initializing 3.bias with zeros\n",
      "Initializing 5.weight with kaiming_normal\n",
      "Initializing 5.bias with zeros\n",
      "Initializing Linear(in_features=6, out_features=64, bias=True)\n",
      "Initializing Linear(in_features=64, out_features=64, bias=True)\n",
      "Initializing Linear(in_features=64, out_features=32, bias=True)\n",
      "Initializing Linear(in_features=32, out_features=128, bias=True)\n",
      "Initializing Linear(in_features=128, out_features=64, bias=True)\n",
      "Initializing GATConv(64, 1, heads=1)\n",
      "Model initialized\n",
      "MyGnn(\n",
      "  (point_net_layer): PointNetConv(local_nn=Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "  ), global_nn=Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=32, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (6): ReLU()\n",
      "  ))\n",
      "  (graph_layers): Sequential(\n",
      "    (0) - GATConv(64, 1, heads=1): x, edge_index -> x\n",
      "  )\n",
      ")\n",
      "baseline loss mean 3.9451329708099365\n",
      "baseline loss no  3.9662485122680664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:   0%|          | 0/428 [00:00<?, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "Epoch 1/1000: 100%|██████████| 428/428 [00:07<00:00, 54.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mbaseline loss no  \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(baseline_loss) )\n\u001b[1;32m     29\u001b[0m early_stopping \u001b[39m=\u001b[39m gio\u001b[39m.\u001b[39mEarlyStopping(patience\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 30\u001b[0m best_val_loss, best_epoch \u001b[39m=\u001b[39m garch\u001b[39m.\u001b[39;49mtrain(model\u001b[39m=\u001b[39;49mmodel, \n\u001b[1;32m     31\u001b[0m             config\u001b[39m=\u001b[39;49mconfig, \n\u001b[1;32m     32\u001b[0m             loss_fct\u001b[39m=\u001b[39;49mloss_fct,\n\u001b[1;32m     33\u001b[0m             optimizer\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mAdamW(model\u001b[39m.\u001b[39;49mparameters(), lr\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mlr, weight_decay\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m),\n\u001b[1;32m     34\u001b[0m             train_dl\u001b[39m=\u001b[39;49mtrain_dl, \n\u001b[1;32m     35\u001b[0m             valid_dl\u001b[39m=\u001b[39;49mvalid_dl,\n\u001b[1;32m     36\u001b[0m             device\u001b[39m=\u001b[39;49mdevice, \n\u001b[1;32m     37\u001b[0m             early_stopping\u001b[39m=\u001b[39;49mearly_stopping,\n\u001b[1;32m     38\u001b[0m             accumulation_steps\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mgradient_accumulation_steps,\n\u001b[1;32m     39\u001b[0m             compute_r_squared\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     40\u001b[0m             model_save_path\u001b[39m=\u001b[39;49mmodel_save_path)\n",
      "File \u001b[0;32m~/Development/gnn_predicting_effects_of_traffic_policies/scripts/gnn_architectures.py:171\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, config, loss_fct, optimizer, train_dl, valid_dl, device, early_stopping, accumulation_steps, compute_r_squared, model_save_path, use_gradient_clipping, lr_scheduler_warmup_steps, lr_scheduler_cosine_decay_rate)\u001b[0m\n\u001b[1;32m    169\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m    170\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 171\u001b[0m \u001b[39mfor\u001b[39;00m idx, data \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(train_dl), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_dl), desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39mepochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m     step \u001b[39m=\u001b[39m epoch \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(train_dl) \u001b[39m+\u001b[39m idx\n\u001b[1;32m    173\u001b[0m     lr \u001b[39m=\u001b[39m scheduler\u001b[39m.\u001b[39mget_lr(step)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1318\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1316\u001b[0m     \u001b[39m# no valid `self._rcvd_idx` is found (i.e., didn't break)\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent_workers:\n\u001b[0;32m-> 1318\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_shutdown_workers()\n\u001b[1;32m   1319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \u001b[39m# Now `self._rcvd_idx` is the batch index we want to fetch\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \n\u001b[1;32m   1323\u001b[0m \u001b[39m# Check if the next sample has already been generated\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1443\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._shutdown_workers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mark_worker_as_unavailable(worker_id, shutdown\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   1439\u001b[0m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers:\n\u001b[1;32m   1440\u001b[0m     \u001b[39m# We should be able to join here, but in case anything went\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m     \u001b[39m# wrong, we set a timeout and if the workers fail to join,\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m     \u001b[39m# they are killed in the `finally` block.\u001b[39;00m\n\u001b[0;32m-> 1443\u001b[0m     w\u001b[39m.\u001b[39;49mjoin(timeout\u001b[39m=\u001b[39;49m_utils\u001b[39m.\u001b[39;49mMP_STATUS_CHECK_INTERVAL)\n\u001b[1;32m   1444\u001b[0m \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues:\n\u001b[1;32m   1445\u001b[0m     q\u001b[39m.\u001b[39mcancel_join_thread()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_pid \u001b[39m==\u001b[39m os\u001b[39m.\u001b[39mgetpid(), \u001b[39m'\u001b[39m\u001b[39mcan only join a child process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39mcan only join a started process\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_popen\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    150\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[39m.\u001b[39mdiscard(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/multiprocessing/popen_fork.py:40\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmultiprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconnection\u001b[39;00m \u001b[39mimport\u001b[39;00m wait\n\u001b[0;32m---> 40\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m wait([\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentinel], timeout):\n\u001b[1;32m     41\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/selectors.py:416\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    417\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    418\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "config = setup_wandb(\"test\", {\n",
    "    \"epochs\": 1000,\n",
    "    \"batch_size\": 8,\n",
    "    \"lr\": 0.001,\n",
    "    \"gradient_accumulation_steps\": 5,\n",
    "    \"early_stopping_patience\": 20,\n",
    "    \"point_net_conv_local_mlp\": [64],\n",
    "    \"point_net_conv_global_mlp\": [64,32,128],\n",
    "    \"gat_conv_layer_structure\": [64],\n",
    "    \"indices_to_use\": [0,1,3,4],\n",
    "    \"in_channels\": 6,\n",
    "    \"out_channels\": 1,\n",
    "    \"dropout\": 0.3\n",
    "})\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gnn_instance = garch.MyGnn(in_channels=config.in_channels, out_channels=config.out_channels, point_net_conv_layer_structure_local_mlp=config.point_net_conv_local_mlp,\n",
    "                                   point_net_conv_layer_structure_global_mlp=config.point_net_conv_global_mlp,\n",
    "                                   gat_conv_layer_structure=config.gat_conv_layer_structure,\n",
    "                                   dropout=config.dropout, use_dropout=False)\n",
    "model = gnn_instance.to(device)\n",
    "loss_fct = torch.nn.MSELoss()\n",
    "\n",
    "baseline_loss_mean_target = gio.compute_baseline_of_mean_target(dataset=train_dl, loss_fct=loss_fct)\n",
    "baseline_loss = gio.compute_baseline_of_no_policies(dataset=train_dl, loss_fct=loss_fct)\n",
    "print(\"baseline loss mean \" + str(baseline_loss_mean_target))\n",
    "print(\"baseline loss no  \" +str(baseline_loss) )\n",
    "\n",
    "early_stopping = gio.EarlyStopping(patience=20, verbose=True)\n",
    "best_val_loss, best_epoch = garch.train(model=model, \n",
    "            config=config, \n",
    "            loss_fct=loss_fct,\n",
    "            optimizer=torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=1e-4),\n",
    "            train_dl=train_dl, \n",
    "            valid_dl=valid_dl,\n",
    "            device=device, \n",
    "            early_stopping=early_stopping,\n",
    "            accumulation_steps=config.gradient_accumulation_steps,\n",
    "            compute_r_squared=False,\n",
    "            model_save_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   7.0741,  480.0000,    0.0000,    4.0000],\n",
       "         [   9.1481,  480.0000, -240.0000,    3.0000],\n",
       "         [   2.0000,  960.0000, -480.0000,    3.0000],\n",
       "         [   7.1852,  960.0000,    0.0000,    4.0000],\n",
       "         [   8.1111,  480.0000,    0.0000,    4.0000],\n",
       "         [   5.4444,  480.0000,    0.0000,    4.0000],\n",
       "         [   5.4444,  480.0000,    0.0000,    4.0000],\n",
       "         [   0.0000,  240.0000,    0.0000,    5.0000],\n",
       "         [  17.1111,  480.0000, -240.0000,    3.0000],\n",
       "         [   6.7037,  480.0000,    0.0000,    4.0000]]),\n",
       " tensor([[ 0.9259],\n",
       "         [-0.1481],\n",
       "         [ 2.0000],\n",
       "         [ 3.8148],\n",
       "         [ 2.8889],\n",
       "         [-1.4444],\n",
       "         [-1.4444],\n",
       "         [ 0.0000],\n",
       "         [-0.1111],\n",
       "         [-2.7037]]),\n",
       " tensor([[ 2.3386, 48.8518],\n",
       "         [ 2.3387, 48.8524],\n",
       "         [ 2.3387, 48.8524],\n",
       "         [ 2.3399, 48.8519],\n",
       "         [ 2.3395, 48.8517],\n",
       "         [ 2.3426, 48.8503],\n",
       "         [ 2.3413, 48.8501],\n",
       "         [ 2.3421, 48.8509],\n",
       "         [ 2.3384, 48.8518],\n",
       "         [ 2.3382, 48.8503]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0].x[:10], train_set[0].y[:10], train_set[0].pos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0795, -0.5715,  0.3782,  0.5976],\n",
       "         [ 0.2217, -0.5715, -0.1847,  0.1290],\n",
       "         [-0.2682, -0.3785, -0.7476,  0.1290],\n",
       "         [ 0.0871, -0.3785,  0.3782,  0.5976],\n",
       "         [ 0.1506, -0.5715,  0.3782,  0.5976],\n",
       "         [-0.0321, -0.5715,  0.3782,  0.5976],\n",
       "         [-0.0321, -0.5715,  0.3782,  0.5976],\n",
       "         [-0.4052, -0.6681,  0.3782,  1.0662],\n",
       "         [ 0.7673, -0.5715, -0.1847,  0.1290],\n",
       "         [ 0.0541, -0.5715,  0.3782,  0.5976]]),\n",
       " tensor([[ 0.9259],\n",
       "         [-0.1481],\n",
       "         [ 2.0000],\n",
       "         [ 3.8148],\n",
       "         [ 2.8889],\n",
       "         [-1.4444],\n",
       "         [-1.4444],\n",
       "         [ 0.0000],\n",
       "         [-0.1111],\n",
       "         [-2.7037]]),\n",
       " tensor([[-0.0564, -0.2530],\n",
       "         [-0.0531, -0.2299],\n",
       "         [-0.0531, -0.2299],\n",
       "         [-0.0282, -0.2489],\n",
       "         [-0.0372, -0.2592],\n",
       "         [ 0.0297, -0.3170],\n",
       "         [ 0.0014, -0.3249],\n",
       "         [ 0.0193, -0.2924],\n",
       "         [-0.0594, -0.2548],\n",
       "         [-0.0636, -0.3185]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dl.dataset[0].x[:10], train_dl.dataset[0].y[:10], train_dl.dataset[0].pos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline loss mean 3.9451329708099365\n",
      "baseline loss no  3.9662485122680664\n"
     ]
    }
   ],
   "source": [
    "loss_fct = torch.nn.MSELoss()\n",
    "\n",
    "baseline_loss_mean_target = gio.compute_baseline_of_mean_target(dataset=train_dl, loss_fct=loss_fct)\n",
    "baseline_loss = gio.compute_baseline_of_no_policies(dataset=train_dl, loss_fct=loss_fct)\n",
    "print(\"baseline loss mean \" + str(baseline_loss_mean_target))\n",
    "print(\"baseline loss no  \" +str(baseline_loss) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.Subset object at 0x7ff650ea6e30> tensor(-31.2963) tensor(50.9630) 3222 851\n",
      "<torch.utils.data.dataset.Subset object at 0x7ff650ea6e90> tensor(-27.) tensor(31.5185) 692 505\n",
      "<torch.utils.data.dataset.Subset object at 0x7ff650ea6ef0> tensor(-27.7037) tensor(29.5185) 536 77\n"
     ]
    }
   ],
   "source": [
    "for split in [train_set, valid_set, test_set]:\n",
    "    max = 0\n",
    "    min = 99\n",
    "    for i in range(len(split)):\n",
    "        if split[i].y.max() > max:\n",
    "            max = split[i].y.max()\n",
    "            i_max = i\n",
    "        if split[i].y.min() < min:\n",
    "            min = split[i].y.min()\n",
    "            i_min = i\n",
    "            \n",
    "    print(split, min, max, i_max, i_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "(train_set[1].y == train_set[2].y).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenhao-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import wandb\n",
    "import random\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import signal\n",
    "import joblib\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "import help_functions as hf\n",
    "\n",
    "import psutil\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "scripts_path = os.path.abspath(os.path.join('..'))\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "    \n",
    "import gnn_io as gio\n",
    "import gnn_architectures_improved as garch\n",
    "import copy\n",
    "from torch_geometric.utils import to_undirected\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is current working status (11.10.2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanned 199 batches, found 9912 valid items\n",
      "Created DataLoader with 9912 items\n",
      "Batch size: 32\n",
      "Number of nodes: 996480\n",
      "Number of edges: 1892320\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "import psutil\n",
    "import gc\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "import os\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "class BatchedGraphDataset(Dataset):\n",
    "    def __init__(self, dataset_path, max_data_points=10000):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.max_data_points = max_data_points\n",
    "        self.data_files = []\n",
    "        self.data_indices = []\n",
    "        self.total_items = 0\n",
    "        self._scan_files()\n",
    "\n",
    "    def _scan_files(self):\n",
    "        batch_num = 1\n",
    "        while self.total_items < self.max_data_points:\n",
    "            batch_file = os.path.join(self.dataset_path, f'datalist_batch_{batch_num}.pt')\n",
    "            if not os.path.exists(batch_file):\n",
    "                break\n",
    "            batch_data = torch.load(batch_file, map_location='cpu')\n",
    "            if isinstance(batch_data, list):\n",
    "                valid_items = sum(1 for item in batch_data if isinstance(item, Data) and self._has_required_attrs(item))\n",
    "                self.data_files.append(batch_file)\n",
    "                self.data_indices.extend([(batch_num - 1, i) for i in range(valid_items)])\n",
    "                self.total_items += valid_items\n",
    "                if self.total_items >= self.max_data_points:\n",
    "                    self.data_indices = self.data_indices[:self.max_data_points]\n",
    "                    self.total_items = self.max_data_points\n",
    "                    break\n",
    "            batch_num += 1\n",
    "        print(f\"Scanned {batch_num-1} batches, found {self.total_items} valid items\")\n",
    "\n",
    "    def _has_required_attrs(self, item):\n",
    "        required_attrs = ['x', 'edge_index', 'pos', 'y', 'mode_stats']\n",
    "        return all(hasattr(item, attr) for attr in required_attrs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_items\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_idx, item_idx = self.data_indices[idx]\n",
    "        batch_data = torch.load(self.data_files[batch_idx], map_location='cpu')\n",
    "        return batch_data[item_idx]\n",
    "\n",
    "def get_data_loader(dataset_path, batch_size=32, max_data_points=10000):\n",
    "    dataset = BatchedGraphDataset(dataset_path, max_data_points)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "def custom_collate(batch):\n",
    "    return Batch.from_data_list(batch)\n",
    "\n",
    "def get_data_loader(dataset_path, batch_size=32, max_data_points=10000):\n",
    "    dataset = BatchedGraphDataset(dataset_path, max_data_points)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, \n",
    "                      pin_memory=True, collate_fn=custom_collate)\n",
    "\n",
    "# Usage\n",
    "try:\n",
    "    dataset_path = '../../data/train_data/sim_output_1pm_capacity_reduction_10k_11_10_2024/'\n",
    "    data_loader = get_data_loader(dataset_path, batch_size=32, max_data_points=10000)\n",
    "    print(f\"Created DataLoader with {len(data_loader.dataset)} items\")\n",
    "\n",
    "    # Example of iterating through the data\n",
    "    for batch in data_loader:\n",
    "        print(f\"Batch size: {batch.num_graphs}\")\n",
    "        print(f\"Number of nodes: {batch.num_nodes}\")\n",
    "        print(f\"Number of edges: {batch.num_edges}\")\n",
    "        # Process your batch here\n",
    "        break  # Remove this line when you're ready to process all batches\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    \n",
    "# Set parameters here\n",
    "params = {\"project_name\": \"test\",\n",
    "            \"num_epochs\": 1000,\n",
    "            \"batch_size\": 8,\n",
    "            \"point_net_conv_layer_structure_local_mlp\": [64, 128],\n",
    "            \"point_net_conv_layer_structure_global_mlp\": [256, 64],\n",
    "            \"gat_conv_layer_structure\": [128, 256, 256, 128],\n",
    "            \"graph_mlp_layer_structure\": [128, 256, 128],\n",
    "            \"lr\": 0.001,\n",
    "            \"gradient_accumulation_steps\": 3,\n",
    "            \"in_channels\": 15,\n",
    "            \"out_channels\": 1,\n",
    "            \"early_stopping_patience\": 100,\n",
    "            \"unique_model_description\": \"my_test\",\n",
    "            \"dropout\": 0.3,\n",
    "            \"use_dropout\": False\n",
    "        } \n",
    "    \n",
    "base_dir = '../../data/' + params['project_name'] + '/'\n",
    "unique_run_dir = os.path.join(base_dir, params['unique_model_description'])\n",
    "os.makedirs(unique_run_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Define the paths here\n",
    "def get_paths(base_dir: str, unique_model_description: str, model_save_path: str = 'trained_model/model.pth'):\n",
    "    data_path = os.path.join(base_dir, unique_model_description)\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    model_save_to = os.path.join(data_path, model_save_path)\n",
    "    path_to_save_dataloader = os.path.join(data_path, 'data_created_during_training/')\n",
    "    os.makedirs(os.path.dirname(model_save_to), exist_ok=True)\n",
    "    os.makedirs(path_to_save_dataloader, exist_ok=True)\n",
    "    return model_save_to, path_to_save_dataloader\n",
    "\n",
    "model_save_path, path_to_save_dataloader = get_paths(base_dir=base_dir, unique_model_description= params['unique_model_description'], model_save_path= 'trained_model/model.pth')\n",
    "\n",
    "# # Usage\n",
    "# try:\n",
    "#     dataset_path = '../../data/train_data/sim_output_1pm_capacity_reduction_10k_11_10_2024/'\n",
    "#     data_loader = get_data_loader(dataset_path, batch_size=32, max_data_points=10000)\n",
    "#     print(f\"Created DataLoader with {len(data_loader.dataset)} items\")\n",
    "\n",
    "#     # Example of iterating through the data\n",
    "#     for batch in data_loader:\n",
    "#         # Process your batch here\n",
    "#         pass\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prepare_data_with_graph_features with 9912 items\n",
      "Splitting into subsets...\n",
      "Total dataset length: 9912\n",
      "Training subset length: 7929\n",
      "Validation subset length: 1486\n",
      "Test subset length: 497\n",
      "Split complete. Train: 7929, Valid: 1486, Test: 497\n",
      "Normalizing train set...\n",
      "Starting normalization for 7929 items\n",
      "Dataset copied successfully\n",
      "Shape of x: 14\n",
      "Processing feature 0/14\n",
      "Fitted on 1000/7929 items\n",
      "Fitted on 2000/7929 items\n",
      "Fitted on 3000/7929 items\n",
      "Fitted on 4000/7929 items\n",
      "Fitted on 5000/7929 items\n",
      "Fitted on 6000/7929 items\n",
      "Fitted on 7000/7929 items\n",
      "Transformed 1000/7929 items\n",
      "Transformed 2000/7929 items\n",
      "Transformed 3000/7929 items\n",
      "Transformed 4000/7929 items\n",
      "Transformed 5000/7929 items\n",
      "Transformed 6000/7929 items\n",
      "Transformed 7000/7929 items\n",
      "Processing feature 1/14\n",
      "Fitted on 1000/7929 items\n",
      "Fitted on 2000/7929 items\n",
      "Fitted on 3000/7929 items\n",
      "Fitted on 4000/7929 items\n",
      "Fitted on 5000/7929 items\n",
      "Fitted on 6000/7929 items\n",
      "Fitted on 7000/7929 items\n",
      "Transformed 1000/7929 items\n",
      "Transformed 2000/7929 items\n",
      "Transformed 3000/7929 items\n",
      "Transformed 4000/7929 items\n",
      "Transformed 5000/7929 items\n",
      "Transformed 6000/7929 items\n",
      "Transformed 7000/7929 items\n",
      "Processing feature 2/14\n",
      "Fitted on 1000/7929 items\n",
      "Fitted on 2000/7929 items\n",
      "Fitted on 3000/7929 items\n",
      "Fitted on 4000/7929 items\n",
      "Fitted on 5000/7929 items\n",
      "Fitted on 6000/7929 items\n",
      "Fitted on 7000/7929 items\n",
      "Transformed 1000/7929 items\n",
      "Transformed 2000/7929 items\n",
      "Transformed 3000/7929 items\n",
      "Transformed 4000/7929 items\n",
      "Transformed 5000/7929 items\n",
      "Transformed 6000/7929 items\n",
      "Transformed 7000/7929 items\n",
      "Processing feature 3/14\n",
      "Fitted on 1000/7929 items\n",
      "Fitted on 2000/7929 items\n",
      "Fitted on 3000/7929 items\n",
      "Fitted on 4000/7929 items\n",
      "Fitted on 5000/7929 items\n",
      "Fitted on 6000/7929 items\n",
      "Fitted on 7000/7929 items\n",
      "Transformed 1000/7929 items\n",
      "Transformed 2000/7929 items\n",
      "Transformed 3000/7929 items\n",
      "Transformed 4000/7929 items\n",
      "Transformed 5000/7929 items\n",
      "Transformed 6000/7929 items\n",
      "Transformed 7000/7929 items\n",
      "Processing feature 4/14\n",
      "Fitted on 1000/7929 items\n",
      "Fitted on 2000/7929 items\n",
      "Fitted on 3000/7929 items\n",
      "Fitted on 4000/7929 items\n",
      "Fitted on 5000/7929 items\n",
      "Fitted on 6000/7929 items\n",
      "Fitted on 7000/7929 items\n",
      "Transformed 1000/7929 items\n",
      "Transformed 2000/7929 items\n",
      "Transformed 3000/7929 items\n",
      "Transformed 4000/7929 items\n",
      "Transformed 5000/7929 items\n",
      "Transformed 6000/7929 items\n",
      "Transformed 7000/7929 items\n",
      "Processing feature 5/14\n",
      "Fitted on 1000/7929 items\n",
      "Fitted on 2000/7929 items\n",
      "Fitted on 3000/7929 items\n",
      "Fitted on 4000/7929 items\n",
      "Fitted on 5000/7929 items\n",
      "Fitted on 6000/7929 items\n",
      "Fitted on 7000/7929 items\n",
      "Transformed 1000/7929 items\n",
      "Transformed 2000/7929 items\n",
      "Transformed 3000/7929 items\n",
      "Transformed 4000/7929 items\n",
      "Transformed 5000/7929 items\n",
      "Transformed 6000/7929 items\n",
      "Transformed 7000/7929 items\n",
      "Processing feature 6/14\n",
      "Fitted on 1000/7929 items\n",
      "Fitted on 2000/7929 items\n",
      "Fitted on 3000/7929 items\n",
      "Fitted on 4000/7929 items\n",
      "Fitted on 5000/7929 items\n",
      "Fitted on 6000/7929 items\n",
      "Fitted on 7000/7929 items\n",
      "Transformed 1000/7929 items\n",
      "Transformed 2000/7929 items\n",
      "Transformed 3000/7929 items\n",
      "Transformed 4000/7929 items\n",
      "Transformed 5000/7929 items\n",
      "Transformed 6000/7929 items\n",
      "Transformed 7000/7929 items\n",
      "Processing feature 7/14\n",
      "Fitted on 1000/7929 items\n",
      "Fitted on 2000/7929 items\n",
      "Fitted on 3000/7929 items\n",
      "Fitted on 4000/7929 items\n",
      "Fitted on 5000/7929 items\n",
      "Fitted on 6000/7929 items\n",
      "Fitted on 7000/7929 items\n",
      "Transformed 1000/7929 items\n",
      "Transformed 2000/7929 items\n",
      "Transformed 3000/7929 items\n",
      "Transformed 4000/7929 items\n",
      "Transformed 5000/7929 items\n",
      "Transformed 6000/7929 items\n",
      "Transformed 7000/7929 items\n",
      "Processing feature 8/14\n",
      "Fitted on 1000/7929 items\n",
      "Fitted on 2000/7929 items\n",
      "Fitted on 3000/7929 items\n",
      "Fitted on 4000/7929 items\n",
      "Fitted on 5000/7929 items\n",
      "Fitted on 6000/7929 items\n",
      "Fitted on 7000/7929 items\n",
      "Transformed 1000/7929 items\n",
      "Transformed 2000/7929 items\n",
      "Transformed 3000/7929 items\n",
      "Transformed 4000/7929 items\n",
      "Transformed 5000/7929 items\n",
      "Transformed 6000/7929 items\n",
      "Transformed 7000/7929 items\n",
      "Processing feature 9/14\n",
      "Fitted on 1000/7929 items\n",
      "Fitted on 2000/7929 items\n",
      "Fitted on 3000/7929 items\n",
      "Fitted on 4000/7929 items\n",
      "Fitted on 5000/7929 items\n",
      "Fitted on 6000/7929 items\n",
      "Fitted on 7000/7929 items\n",
      "Transformed 1000/7929 items\n",
      "Transformed 2000/7929 items\n",
      "Transformed 3000/7929 items\n",
      "Transformed 4000/7929 items\n",
      "Transformed 5000/7929 items\n",
      "Transformed 6000/7929 items\n",
      "Transformed 7000/7929 items\n",
      "Processing feature 10/14\n",
      "Fitted on 1000/7929 items\n",
      "Fitted on 2000/7929 items\n",
      "Fitted on 3000/7929 items\n",
      "Fitted on 4000/7929 items\n",
      "Fitted on 5000/7929 items\n",
      "Fitted on 6000/7929 items\n",
      "Fitted on 7000/7929 items\n",
      "Transformed 1000/7929 items\n"
     ]
    }
   ],
   "source": [
    "def normalize_dataset(dataset_input, directory_path):\n",
    "    try:\n",
    "        print(f\"Starting normalization for {len(dataset_input)} items\")\n",
    "        dataset = copy_subset(dataset_input)\n",
    "        print(\"Dataset copied successfully\")\n",
    "        \n",
    "        dataset = normalize_x_values(dataset, directory_path)\n",
    "        print(\"X values normalized successfully\")\n",
    "        \n",
    "        dataset = normalize_positional_features(dataset, directory_path)\n",
    "        print(\"Positional features normalized successfully\")\n",
    "        \n",
    "        dataset = normalize_mode_stats(dataset, directory_path)\n",
    "        print(\"Mode stats normalized successfully\")\n",
    "        \n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error in normalize_dataset: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    \n",
    "    \n",
    "import torch\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalize_x_values(dataset, directory_path, batch_size=1000):\n",
    "    try:\n",
    "        # Get the shape of x from the first item\n",
    "        first_item = dataset[0]\n",
    "        shape_of_x = first_item.x.shape[1]\n",
    "        print(f\"Shape of x: {shape_of_x}\")\n",
    "        \n",
    "        list_of_scalers_to_save = []\n",
    "\n",
    "        for i in range(shape_of_x):\n",
    "            print(f\"Processing feature {i}/{shape_of_x}\")\n",
    "            scaler = StandardScaler()\n",
    "            \n",
    "            # Fit scaler\n",
    "            for j in range(len(dataset)):\n",
    "                data = dataset[j]\n",
    "                feature = replace_invalid_values(data.x[:, i].reshape(-1, 1)).numpy()\n",
    "                scaler.partial_fit(feature)\n",
    "                \n",
    "                if (j + 1) % batch_size == 0:\n",
    "                    print(f\"Fitted on {j + 1}/{len(dataset)} items\")\n",
    "            \n",
    "            list_of_scalers_to_save.append(scaler)\n",
    "\n",
    "            # Transform data\n",
    "            for j in range(len(dataset)):\n",
    "                data = dataset[j]\n",
    "                data_x_dim = replace_invalid_values(data.x[:, i].reshape(-1, 1))\n",
    "                normalized_x_dim = torch.tensor(scaler.transform(data_x_dim.numpy()), dtype=torch.float)\n",
    "                if i == 0:\n",
    "                    normalized_x = normalized_x_dim\n",
    "                else:\n",
    "                    normalized_x = torch.cat((normalized_x, normalized_x_dim), dim=1)\n",
    "                \n",
    "                if (j + 1) % batch_size == 0:\n",
    "                    print(f\"Transformed {j + 1}/{len(dataset)} items\")\n",
    "                \n",
    "                # Update the x attribute directly\n",
    "                data.x = normalized_x\n",
    "\n",
    "        print(\"Saving scalers...\")\n",
    "        joblib.dump(list_of_scalers_to_save, (directory_path + 'x_scaler.pkl'))\n",
    "        print(\"Scalers saved successfully\")\n",
    "\n",
    "        print(\"Dataset x values updated\")\n",
    "\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error in normalize_x_values: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "\n",
    "# Function to copy a Subset\n",
    "def copy_subset(subset):\n",
    "    return Subset(copy.deepcopy(subset.dataset), copy.deepcopy(subset.indices))\n",
    "\n",
    "def normalize_positional_features(dataset, directory_path, batch_size=1000):\n",
    "    try:\n",
    "        # Get the shape of pos from the first item\n",
    "        first_item = dataset[0]\n",
    "        shape_of_pos = first_item.pos.shape\n",
    "        print(f\"Shape of pos: {shape_of_pos}\")\n",
    "        \n",
    "        list_of_scalers_to_save = []\n",
    "        num_pos_features = shape_of_pos[1] * shape_of_pos[2]  # Assuming pos is 3D\n",
    "\n",
    "        for i in range(num_pos_features):\n",
    "            print(f\"Processing positional feature {i}/{num_pos_features}\")\n",
    "            scaler = StandardScaler()\n",
    "            \n",
    "            # Fit scaler\n",
    "            for j in range(len(dataset)):\n",
    "                data = dataset[j]\n",
    "                feature = replace_invalid_values(data.pos.reshape(data.pos.shape[0], -1)[:, i].reshape(-1, 1)).numpy()\n",
    "                scaler.partial_fit(feature)\n",
    "                \n",
    "                if (j + 1) % batch_size == 0:\n",
    "                    print(f\"Fitted on {j + 1}/{len(dataset)} items\")\n",
    "            \n",
    "            list_of_scalers_to_save.append(scaler)\n",
    "\n",
    "            # Transform data\n",
    "            for j in range(len(dataset)):\n",
    "                data = dataset[j]\n",
    "                pos_reshaped = data.pos.reshape(data.pos.shape[0], -1)\n",
    "                data_pos_dim = replace_invalid_values(pos_reshaped[:, i].reshape(-1, 1))\n",
    "                normalized_pos_dim = torch.tensor(scaler.transform(data_pos_dim.numpy()), dtype=torch.float)\n",
    "                pos_reshaped[:, i] = normalized_pos_dim.squeeze()\n",
    "                data.pos = pos_reshaped.reshape(shape_of_pos)\n",
    "                \n",
    "                if (j + 1) % batch_size == 0:\n",
    "                    print(f\"Transformed {j + 1}/{len(dataset)} items\")\n",
    "\n",
    "        print(\"Saving positional scalers...\")\n",
    "        joblib.dump(list_of_scalers_to_save, (directory_path + 'pos_scaler.pkl'))\n",
    "        print(\"Positional scalers saved successfully\")\n",
    "\n",
    "        print(\"Dataset pos values updated\")\n",
    "\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error in normalize_positional_features: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def normalize_mode_stats(dataset, directory_path, batch_size=1000):\n",
    "    try:\n",
    "        print(\"Starting mode stats normalization...\")\n",
    "        # Initialize 12 StandardScalers for 6 sets of 2 dimensions\n",
    "        scalers = [[StandardScaler() for _ in range(2)] for _ in range(6)]\n",
    "\n",
    "        # Fit the scalers\n",
    "        for i in range(6):  # Iterate over the first dimension (6 sets)\n",
    "            for j in range(2):  # Iterate over the second dimension (2D vectors)\n",
    "                print(f\"Processing mode stats dimension {i}, {j}\")\n",
    "                \n",
    "                # Fit the scaler in batches\n",
    "                for k in range(0, len(dataset), batch_size):\n",
    "                    batch = [dataset[idx] for idx in range(k, min(k+batch_size, len(dataset)))]\n",
    "                    values = np.vstack([replace_invalid_values(data.mode_stats[i, j].reshape(-1, 1)) for data in batch])\n",
    "                    scalers[i][j].partial_fit(values)\n",
    "                    \n",
    "                    if (k + batch_size) % (batch_size * 10) == 0:\n",
    "                        print(f\"Fitted on {k + batch_size}/{len(dataset)} items\")\n",
    "\n",
    "        # Transform the data\n",
    "        for k in range(0, len(dataset), batch_size):\n",
    "            batch = [dataset[idx] for idx in range(k, min(k+batch_size, len(dataset)))]\n",
    "            for data in batch:\n",
    "                normalized_mode_stats = data.mode_stats.clone()\n",
    "                for i in range(6):\n",
    "                    for j in range(2):\n",
    "                        data_mode_stats_dim = replace_invalid_values(data.mode_stats[i, j].reshape(-1, 1))\n",
    "                        transformed = scalers[i][j].transform(data_mode_stats_dim).flatten()\n",
    "                        normalized_mode_stats[i, j] = torch.tensor(transformed, dtype=torch.float32)\n",
    "                data.mode_stats = normalized_mode_stats\n",
    "            \n",
    "            if (k + batch_size) % (batch_size * 10) == 0:\n",
    "                print(f\"Transformed {k + batch_size}/{len(dataset)} items\")\n",
    "\n",
    "        print(\"Saving mode stats scalers...\")\n",
    "        # Save the scalers using joblib\n",
    "        for i in range(6):\n",
    "            for j in range(2):\n",
    "                scaler_path = directory_path + f'scaler_mode_stats_{i}_{j}.pkl'\n",
    "                joblib.dump(scalers[i][j], scaler_path)\n",
    "\n",
    "        print(\"Mode stats scalers saved and dataset standardized.\")\n",
    "        return dataset\n",
    "    except Exception as e:\n",
    "        print(f\"Error in normalize_mode_stats: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "def replace_invalid_values(tensor):\n",
    "    # print(f\"Input tensor shape: {tensor.shape}\")\n",
    "    # nan_count = torch.isnan(tensor).sum().item()\n",
    "    # inf_count = torch.isinf(tensor).sum().item()\n",
    "    # print(f\"NaN count: {nan_count}, Inf count: {inf_count}\")\n",
    "    \n",
    "    tensor[torch.isnan(tensor)] = 0  # replace NaNs with 0\n",
    "    tensor[torch.isinf(tensor)] = 0  # replace inf and -inf with 0\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def prepare_data_with_graph_features(datalist, batch_size, path_to_save_dataloader):\n",
    "    print(f\"Starting prepare_data_with_graph_features with {len(datalist)} items\")\n",
    "    \n",
    "    try:\n",
    "        print(\"Splitting into subsets...\")\n",
    "        train_set, valid_set, test_set = gio.split_into_subsets(dataset=datalist, train_ratio=0.8, val_ratio=0.15, test_ratio=0.05)\n",
    "        print(f\"Split complete. Train: {len(train_set)}, Valid: {len(valid_set)}, Test: {len(test_set)}\")\n",
    "        \n",
    "        print(\"Normalizing train set...\")\n",
    "        train_set_normalized = normalize_dataset(dataset_input=train_set, directory_path=path_to_save_dataloader + \"train_\")\n",
    "        print(\"Train set normalized\")\n",
    "        \n",
    "        print(\"Normalizing validation set...\")\n",
    "        valid_set_normalized = normalize_dataset(dataset_input=valid_set, directory_path=path_to_save_dataloader + \"valid_\")\n",
    "        print(\"Validation set normalized\")\n",
    "        \n",
    "        print(\"Creating train loader...\")\n",
    "        train_loader = DataLoader(dataset=train_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, prefetch_factor=2, pin_memory=True, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "        print(\"Train loader created\")\n",
    "        \n",
    "        print(\"Creating validation loader...\")\n",
    "        val_loader = DataLoader(dataset=valid_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "        print(\"Validation loader created\")\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    except Exception as e:\n",
    "        print(f\"Error in prepare_data_with_graph_features: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "train_dl, valid_dl = prepare_data_with_graph_features(datalist=data_loader.dataset, batch_size= params['batch_size'], path_to_save_dataloader= path_to_save_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outgoing edges from node 0: [9477, 9478, 9479]\n",
      "Incoming edges to node 0: [9479, 23978]\n",
      "The graph is unidirectional\n"
     ]
    }
   ],
   "source": [
    "def check_directionality(data):\n",
    "    # Get all edges starting from node 0\n",
    "    outgoing = data.edge_index[1, data.edge_index[0] == 25318].tolist()\n",
    "    \n",
    "    # Get all edges ending at node 0\n",
    "    incoming = data.edge_index[0, data.edge_index[1] == 25318].tolist()\n",
    "    \n",
    "    # Check if all outgoing edges have a corresponding incoming edge\n",
    "    bidirectional = all(node in incoming for node in outgoing) and len(outgoing) == len(incoming)\n",
    "    \n",
    "    print(f\"Outgoing edges from node 0: {outgoing}\")\n",
    "    print(f\"Incoming edges to node 0: {incoming}\")\n",
    "    print(f\"The graph is {'bidirectional' if bidirectional else 'unidirectional'}\")\n",
    "    \n",
    "    return bidirectional\n",
    "\n",
    "# Use the function on your data object\n",
    "data = train_dl.dataset[0]\n",
    "is_bidirectional = check_directionality(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 59135], num_nodes=31140, x=[31140, 14], pos=[31140, 3, 2], y=[31140, 1], mode_stats=[6, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU 0 with CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33menatterer\u001b[0m (\u001b[33mtum-traffic-engineering\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/enatterer/Development/gnn_predicting_effects_of_traffic_policies/scripts/training/wandb/run-20241011_172213-gohhtx3f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tum-traffic-engineering/test/runs/gohhtx3f' target=\"_blank\">ethereal-night-71</a></strong> to <a href='https://wandb.ai/tum-traffic-engineering/test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tum-traffic-engineering/test' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tum-traffic-engineering/test/runs/gohhtx3f' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/test/runs/gohhtx3f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized\n",
      "MyGnn(\n",
      "  (point_net_conv_1): PointNetConv(local_nn=Sequential(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "  ), global_nn=Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (point_net_conv_2): PointNetConv(local_nn=Sequential(\n",
      "    (0): Linear(in_features=66, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "  ), global_nn=Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (point_net_conv_3): PointNetConv(local_nn=Sequential(\n",
      "    (0): Linear(in_features=66, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "  ), global_nn=Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (graph_predictor): Sequential(\n",
      "    (0): Linear(in_features=140, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      "  (gat_graph_layers): Sequential(\n",
      "    (0) - GATConv(128, 256, heads=1): x, edge_index -> x\n",
      "    (1) - ReLU(inplace=True): x -> x\n",
      "    (2) - GATConv(256, 256, heads=1): x, edge_index -> x\n",
      "    (3) - ReLU(inplace=True): x -> x\n",
      "    (4) - GATConv(256, 128, heads=1): x, edge_index -> x\n",
      "    (5) - ReLU(inplace=True): x -> x\n",
      "    (6) - GATConv(128, 1, heads=1): x, edge_index -> x\n",
      "  )\n",
      "  (gat_graph_layers_local): Sequential(\n",
      "    (0) - GATConv(128, 256, heads=1): x, edge_index -> x\n",
      "    (1) - ReLU(inplace=True): x -> x\n",
      "    (2) - GATConv(256, 256, heads=1): x, edge_index -> x\n",
      "    (3) - ReLU(inplace=True): x -> x\n",
      "    (4) - GATConv(256, 128, heads=1): x, edge_index -> x\n",
      "    (5) - ReLU(inplace=True): x -> x\n",
      "    (6) - GATConv(128, 1, heads=1): x, edge_index -> x\n",
      "  )\n",
      "  (graph_mlp): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "baseline loss mean 3.097710609436035\n",
      "baseline loss no  3.106675863265991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:   0%|          | 0/2 [00:00<?, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "Epoch 1/1000:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 8 but got size 48 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbaseline loss no  \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(baseline_loss) )\n\u001b[1;32m     37\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m gio\u001b[38;5;241m.\u001b[39mEarlyStopping(patience\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly_stopping_patience\u001b[39m\u001b[38;5;124m'\u001b[39m], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 38\u001b[0m best_val_loss, best_epoch \u001b[38;5;241m=\u001b[39m \u001b[43mgarch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloss_fct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdamW\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m            \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_gradient_clipping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler_warmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler_cosine_decay_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest model saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \n",
      "File \u001b[0;32m~/Development/gnn_predicting_effects_of_traffic_policies/scripts/gnn_architectures_improved.py:319\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, config, loss_fct, optimizer, train_dl, valid_dl, device, early_stopping, accumulation_steps, model_save_path, use_gradient_clipping, lr_scheduler_warmup_steps, lr_scheduler_cosine_decay_rate)\u001b[0m\n\u001b[1;32m    315\u001b[0m targets \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m     predicted \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m loss_fct(predicted, targets)\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Development/gnn_predicting_effects_of_traffic_policies/scripts/gnn_architectures_improved.py:118\u001b[0m, in \u001b[0;36mMyGnn.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    115\u001b[0m graph_x \u001b[38;5;241m=\u001b[39m geo_nn\u001b[38;5;241m.\u001b[39mglobal_mean_pool(x, data\u001b[38;5;241m.\u001b[39mbatch)  \u001b[38;5;66;03m# Aggregate node features\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Combine aggregated node features with mode_stats\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m combined_graph_features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgraph_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_stats\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Make graph-level prediction\u001b[39;00m\n\u001b[1;32m    121\u001b[0m graph_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_predictor(combined_graph_features)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 8 but got size 48 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "gpus = hf.get_available_gpus()\n",
    "best_gpu = hf.select_best_gpu(gpus)\n",
    "hf.set_cuda_visible_device(best_gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = hf.setup_wandb(params['project_name'], {\n",
    "    \"epochs\": params['num_epochs'],\n",
    "    \"batch_size\": params['batch_size'],\n",
    "    \"lr\": params['lr'],\n",
    "    \"gradient_accumulation_steps\": params['gradient_accumulation_steps'],\n",
    "    \"early_stopping_patience\": params['early_stopping_patience'],\n",
    "    \"point_net_conv_local_mlp\": params['point_net_conv_layer_structure_local_mlp'],\n",
    "    \"point_net_conv_global_mlp\": params['point_net_conv_layer_structure_global_mlp'],\n",
    "    \"gat_conv_layer_structure\": params['gat_conv_layer_structure'],\n",
    "    \"graph_mlp_layer_structure\": params['graph_mlp_layer_structure'],\n",
    "    \"in_channels\": params['in_channels'],\n",
    "    \"out_channels\": params['out_channels'],\n",
    "    \"dropout\": params['dropout'],\n",
    "    \"use_dropout\": params['use_dropout']\n",
    "})\n",
    "\n",
    "model = garch.MyGnn(in_channels=config.in_channels, out_channels=config.out_channels, point_net_conv_layer_structure_local_mlp=config.point_net_conv_local_mlp,\n",
    "                            point_net_conv_layer_structure_global_mlp=config.point_net_conv_global_mlp,\n",
    "                            gat_conv_layer_structure=config.gat_conv_layer_structure,\n",
    "                            graph_mlp_layer_structure=config.graph_mlp_layer_structure,\n",
    "                            dropout=config.dropout, use_dropout=config.use_dropout)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_fct = torch.nn.MSELoss()\n",
    "\n",
    "baseline_loss_mean_target = gio.compute_baseline_of_mean_target(dataset=train_dl, loss_fct=loss_fct)\n",
    "baseline_loss = gio.compute_baseline_of_no_policies(dataset=train_dl, loss_fct=loss_fct)\n",
    "print(\"baseline loss mean \" + str(baseline_loss_mean_target))\n",
    "print(\"baseline loss no  \" +str(baseline_loss) )\n",
    "\n",
    "early_stopping = gio.EarlyStopping(patience=params['early_stopping_patience'], verbose=True)\n",
    "best_val_loss, best_epoch = garch.train(model=model, \n",
    "            config=config, \n",
    "            loss_fct=loss_fct,\n",
    "            optimizer=torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=1e-4),\n",
    "            train_dl=train_dl,  \n",
    "            valid_dl=valid_dl,\n",
    "            device=device, \n",
    "            early_stopping=early_stopping,\n",
    "            accumulation_steps=config.gradient_accumulation_steps,\n",
    "            model_save_path=model_save_path,\n",
    "            use_gradient_clipping=True,\n",
    "            lr_scheduler_warmup_steps=20000,\n",
    "            lr_scheduler_cosine_decay_rate=0.2)\n",
    "print(f'Best model saved to {model_save_path} with validation loss: {best_val_loss} at epoch {best_epoch}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call this function during training without the scalars and with the directory path, and during the testing with the saved scalars and without a directory path to save.\n",
    "# def normalize_dataset(dataset_input, directory_path):\n",
    "#     dataset = copy_subset(dataset_input)\n",
    "#     dataset = normalize_x_values(dataset, directory_path)\n",
    "#     dataset = normalize_positional_features(dataset, directory_path)\n",
    "#     dataset = normalize_mode_stats(dataset, directory_path)\n",
    "#     return dataset\n",
    "    \n",
    "    \n",
    "# def normalize_x_values(dataset, directory_path):\n",
    "#     try:\n",
    "#         shape_of_x = dataset[0].x.shape[1]\n",
    "#         print(f\"Shape of x: {shape_of_x}\")\n",
    "        \n",
    "#         list_of_scalers_to_save = []\n",
    "#         print(\"Processing x values...\")\n",
    "\n",
    "#         # Process in batches\n",
    "#         batch_size = 100  # Adjust this value based on your available memory\n",
    "#         for i in range(shape_of_x):\n",
    "#             print(f\"Processing feature {i}/{shape_of_x}\")\n",
    "#             scaler = StandardScaler()\n",
    "            \n",
    "#             # Fit scaler in batches\n",
    "#             for j in range(0, len(dataset), batch_size):\n",
    "#                 batch = dataset[j:j+batch_size]\n",
    "#                 print(f\"Processing batch {j//batch_size + 1}/{len(dataset)//batch_size + 1}\")\n",
    "#                 batch_x_values = torch.cat([data.x[:, i].reshape(-1, 1) for data in batch], dim=0)\n",
    "#                 batch_x_values = replace_invalid_values(batch_x_values)\n",
    "#                 scaler.partial_fit(batch_x_values.numpy())\n",
    "\n",
    "#             list_of_scalers_to_save.append(scaler)\n",
    "\n",
    "#             # Transform data\n",
    "#             for j, data in enumerate(dataset):\n",
    "#                 if j % 100 == 0:\n",
    "#                     print(f\"Transforming data point {j}/{len(dataset)}\")\n",
    "#                 data_x_dim = replace_invalid_values(data.x[:, i].reshape(-1, 1))\n",
    "#                 normalized_x_dim = torch.tensor(scaler.transform(data_x_dim.numpy()), dtype=torch.float)\n",
    "#                 if i == 0:\n",
    "#                     data.normalized_x = normalized_x_dim\n",
    "#                 else:\n",
    "#                     data.normalized_x = torch.cat((data.normalized_x, normalized_x_dim), dim=1)\n",
    "\n",
    "#         print(\"Saving scalers...\")\n",
    "#         joblib.dump(list_of_scalers_to_save, (directory_path + 'x_scaler.pkl'))\n",
    "#         print(\"Scalers saved successfully\")\n",
    "\n",
    "#         print(\"Updating x values in dataset...\")\n",
    "#         for data in dataset:\n",
    "#             data.x = data.normalized_x\n",
    "#             del data.normalized_x\n",
    "#         print(\"Dataset x values updated\")\n",
    "\n",
    "#         return dataset\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in normalize_x_values: {str(e)}\")\n",
    "#         traceback.print_exc()\n",
    "#         raise   \n",
    "\n",
    "\n",
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from collections import Counter\n",
    "\n",
    "# def check_scale_free_distribution(edge_index, num_nodes):\n",
    "#     # Create a NetworkX graph from the edge_index\n",
    "#     G = nx.Graph()\n",
    "#     G.add_nodes_from(range(num_nodes))\n",
    "#     edge_list = edge_index.t().tolist()\n",
    "#     G.add_edges_from(edge_list)\n",
    "\n",
    "#     # Calculate degree for each node\n",
    "#     degrees = [d for n, d in G.degree()]\n",
    "#     degree_counts = Counter(degrees)\n",
    "\n",
    "#     # Sort the degree counts\n",
    "#     sorted_degree_counts = sorted(degree_counts.items())\n",
    "#     x = [k for k, v in sorted_degree_counts]\n",
    "#     y = [v for k, v in sorted_degree_counts]\n",
    "\n",
    "#     # Plot degree distribution on log-log scale\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.loglog(x, y, 'bo-')\n",
    "#     plt.xlabel('Degree (log scale)')\n",
    "#     plt.ylabel('Count (log scale)')\n",
    "#     plt.title('Degree Distribution (Log-Log Scale)')\n",
    "#     plt.grid(True)\n",
    "\n",
    "#     # Fit a power law distribution\n",
    "#     x_log = np.log(x)\n",
    "#     y_log = np.log(y)\n",
    "#     coeffs = np.polyfit(x_log[1:], y_log[1:], 1)\n",
    "#     power_law_exponent = -coeffs[0]\n",
    "\n",
    "#     # Plot the fitted line\n",
    "#     x_fit = np.logspace(np.log10(min(x)), np.log10(max(x)), 100)\n",
    "#     y_fit = np.exp(coeffs[1]) * x_fit**(-power_law_exponent)\n",
    "#     plt.loglog(x_fit, y_fit, 'r--', label=f'Power Law Fit (γ ≈ {power_law_exponent:.2f})')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     print(f\"Estimated power law exponent: γ ≈ {power_law_exponent:.2f}\")\n",
    "    \n",
    "#     if 2 < power_law_exponent < 3:\n",
    "#         print(\"The network shows characteristics of a scale-free network.\")\n",
    "#     else:\n",
    "#         print(\"The network may not be scale-free.\")\n",
    "\n",
    "#     return power_law_exponent\n",
    "\n",
    "# # Usage example:\n",
    "# # Assuming you have a PyTorch Geometric Data object called 'data'\n",
    "# exponent = check_scale_free_distribution(data.edge_index, data.num_nodes)\n",
    "\n",
    "# from torch_geometric.utils import to_undirected, is_undirected\n",
    "\n",
    "# # Assuming you're working with the first graph in your dataset\n",
    "# data = train_dl.dataset[0]\n",
    "\n",
    "# # Check if the graph is already undirected\n",
    "# # if not is_undirected(data.edge_index):\n",
    "# #     # If it's directed, convert it to undirected\n",
    "# #     data.edge_index = to_undirected(data.edge_index)\n",
    "# #     print(\"Graph has been converted to undirected.\")\n",
    "# # else:\n",
    "# #     print(\"Graph is already undirected.\")\n",
    "\n",
    "# # Verify that the graph is now undirected\n",
    "# print(f\"Is the graph undirected? {is_undirected(data.edge_index)}\")\n",
    "\n",
    "\n",
    "# def normalize_mode_stats(dataset, directory_path):\n",
    "#     # Initialize 12 StandardScalers for 6 sets of 2 dimensions\n",
    "#     scalers = [[StandardScaler() for _ in range(2)] for _ in range(6)]\n",
    "\n",
    "#     # Standardize the data\n",
    "#     for i in range(6):  # Iterate over the first dimension (6 sets)\n",
    "#         for j in range(2):  # Iterate over the second dimension (2D vectors)\n",
    "#             values = np.vstack([data.mode_stats[i, j].numpy().reshape(-1, 1) for data in dataset])\n",
    "#             # Fit the corresponding scaler on the extracted values\n",
    "#             scalers[i][j].fit(values)\n",
    "#             for data in dataset:\n",
    "#                 transformed = scalers[i][j].transform(data.mode_stats[i, j].numpy().reshape(-1, 1)).flatten()\n",
    "#                 # Convert the transformed NumPy array back into a torch tensor\n",
    "#                 data.mode_stats[i, j] = torch.tensor(transformed, dtype=torch.float32)\n",
    "    \n",
    "#     # Save the scalers using joblib\n",
    "#     for i in range(6):\n",
    "#         for j in range(2):\n",
    "#             # Dump the scalers with meaningful names to differentiate them\n",
    "#             scaler_path = directory_path + f'scaler_mode_stats_{i}_{j}.pkl'\n",
    "#             joblib.dump(scalers[i][j], scaler_path)\n",
    "\n",
    "#     print(\"Mode stats scalers saved and dataset standardized.\")\n",
    "#     return dataset\n",
    "\n",
    "# def replace_invalid_values(tensor):\n",
    "#     tensor[tensor != tensor] = 0  # replace NaNs with 0\n",
    "#     tensor[tensor == float('inf')] = 0  # replace inf with 0\n",
    "#     tensor[tensor == float('-inf')] = 0  # replace -inf with 0\n",
    "#     return tensor\n",
    "\n",
    "\n",
    "\n",
    "# def prepare_data_with_graph_features(datalist, batch_size, path_to_save_dataloader):\n",
    "#     # datalist = [Data(x=d['x'], edge_index=d['edge_index'], edge_attr=d['edge_attr'], pos=d['pos'], y=d['y'], mode_stats=d['mode_stats']) for d in data_dict_list]\n",
    "#     train_set, valid_set, test_set = gio.split_into_subsets(dataset=datalist, train_ratio=0.8, val_ratio=0.15, test_ratio=0.05)\n",
    "    \n",
    "#     train_set_normalized = normalize_dataset(dataset_input = train_set, directory_path=path_to_save_dataloader + \"train_\")\n",
    "#     valid_set_normalized = normalize_dataset(dataset_input = valid_set, directory_path=path_to_save_dataloader + \"valid_\")\n",
    "#     # # test_set_normalized = normalize_dataset(dataset_input = test_set, directory_path=path_to_save_dataloader + \"test_\")\n",
    "        \n",
    "#     train_loader = DataLoader(dataset=train_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, prefetch_factor=2, pin_memory=True, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "#     val_loader = DataLoader(dataset=valid_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "#     # test_loader = DataLoader(dataset=test_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "#     # gio.save_dataloader(test_loader, path_to_save_dataloader + 'test_dl.pt')\n",
    "#     # gio.save_dataloader_params(test_loader, path_to_save_dataloader + 'test_loader_params.json')\n",
    "    \n",
    "#     return train_loader, val_loader\n",
    "\n",
    "\n",
    "# def normalize_x_values(dataset, directory_path):\n",
    "#     shape_of_x = dataset[0].x.shape[1]\n",
    "#     list_of_scalers_to_save = []\n",
    "#     x_values = torch.cat([data.x for data in dataset], dim=0)\n",
    "\n",
    "#     for i in range(shape_of_x):\n",
    "#         all_node_features = replace_invalid_values(x_values[:, i].reshape(-1, 1)).numpy()\n",
    "        \n",
    "#         scaler = StandardScaler()\n",
    "#         print(f\"Scaler created for x values at index {i}: {scaler}\")\n",
    "#         scaler.fit(all_node_features)\n",
    "#         list_of_scalers_to_save.append(scaler)\n",
    "\n",
    "#         for data in dataset:\n",
    "#             data_x_dim = replace_invalid_values(data.x[:, i].reshape(-1, 1))\n",
    "#             normalized_x_dim = torch.tensor(scaler.transform(data_x_dim.numpy()), dtype=torch.float)\n",
    "#             if i == 0:\n",
    "#                 data.normalized_x = normalized_x_dim\n",
    "#             else:\n",
    "#                 data.normalized_x = torch.cat((data.normalized_x, normalized_x_dim), dim=1)\n",
    "\n",
    "#     joblib.dump(list_of_scalers_to_save, (directory_path + 'x_scaler.pkl'))\n",
    "#     for data in dataset:\n",
    "#         data.x = data.normalized_x\n",
    "#         del data.normalized_x\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "# def normalize_positional_features(dataset, directory_path):\n",
    "#     # Initialize 6 StandardScalers for 3 sets of 2 dimensions\n",
    "#     scalers = [[StandardScaler() for _ in range(2)] for _ in range(3)]\n",
    "\n",
    "#     # Standardize the data\n",
    "#     for i in range(3):  # Iterate over the second dimension (3 sets)\n",
    "#         for j in range(2):  # Iterate over the third dimension (2D vectors)\n",
    "#             values = np.vstack([data.pos[:, i, j].numpy() for data in dataset]).reshape(-1, 1)\n",
    "#             # Fit the corresponding scaler on the extracted values\n",
    "#             scalers[i][j].fit(values)\n",
    "#             for data in dataset:\n",
    "#                 transformed = scalers[i][j].transform(data.pos[:, i, j].numpy().reshape(-1, 1)).flatten()\n",
    "#                 # Convert the transformed NumPy array back into a torch tensor\n",
    "#                 data.pos[:, i, j] = torch.tensor(transformed, dtype=torch.float32)\n",
    "#     # Save the scalers using joblib\n",
    "#     for i in range(3):\n",
    "#         for j in range(2):\n",
    "#             # Dump the scalers with meaningful names to differentiate them\n",
    "#             scaler_path = directory_path + f'scaler_pos_{i}_{j}.pkl'\n",
    "#             joblib.dump(scalers[i][j], scaler_path)\n",
    "\n",
    "#     print(\"Postional scalers saved and dataset standardized.\")\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "\n",
    "# working version, but only up to 2000 datapoints\n",
    "# def get_combined_data(dataset_path, max_batches=None):\n",
    "#     data_list = []\n",
    "#     batch_num = 1\n",
    "#     while max_batches is None or batch_num <= max_batches:\n",
    "#         try:\n",
    "#             batch_file = os.path.join(dataset_path, f'datalist_batch_{batch_num}.pt')\n",
    "#             batch_data = torch.load(batch_file, map_location='cpu')\n",
    "#             print(f\"Batch {batch_num} type: {type(batch_data)}, length: {len(batch_data)}\")\n",
    "            \n",
    "#             if isinstance(batch_data, list):\n",
    "#                 for idx, item in enumerate(batch_data):\n",
    "#                     try:\n",
    "#                         # print(f\"Item {idx} type: {type(item)}\")\n",
    "#                         if isinstance(item, Data):\n",
    "#                             required_attrs = ['x', 'edge_index', 'pos', 'y', 'mode_stats']\n",
    "#                             missing_attrs = [attr for attr in required_attrs if not hasattr(item, attr)]\n",
    "#                             if not missing_attrs:\n",
    "#                                 data_list.append(item)\n",
    "#                                 # print(f\"Added item {idx} to data_list\")\n",
    "#                             else:\n",
    "#                                 print(f\"Skipping invalid item {idx} in batch {batch_num}. Missing attributes: {missing_attrs}\")\n",
    "#                         else:\n",
    "#                             print(f\"Skipping non-Data item {idx} in batch {batch_num}.\")\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error processing item {idx} in batch {batch_num}: {str(e)}\")\n",
    "#             else:\n",
    "#                 print(f\"Unexpected batch data type in batch {batch_num}: {type(batch_data)}\")\n",
    "            \n",
    "#             batch_num += 1\n",
    "#             print(f\"Loaded batch {batch_num-1}, current total: {len(data_list)} items\")\n",
    "            \n",
    "#             if len(data_list) % 1000 == 0:\n",
    "#                 if psutil.virtual_memory().percent > 90:\n",
    "#                     print(\"Memory usage high, stopping data loading\")\n",
    "#                     break\n",
    "#         except FileNotFoundError:\n",
    "#             print(f\"Finished loading {batch_num-1} batches\")\n",
    "#             break\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading batch {batch_num}: {str(e)}\")\n",
    "#             batch_num += 1\n",
    "\n",
    "#     print(f\"Successfully loaded {len(data_list)} data points\")\n",
    "#     return data_list\n",
    "\n",
    "# def get_combined_data(dataset_path, max_data_points=10000):\n",
    "#     data_list = []\n",
    "#     batch_num = 1\n",
    "#     while len(data_list) < max_data_points:\n",
    "#         try:\n",
    "#             batch_file = os.path.join(dataset_path, f'datalist_batch_{batch_num}.pt')\n",
    "#             batch_data = torch.load(batch_file, map_location='cpu')\n",
    "#             print(f\"Batch {batch_num} type: {type(batch_data)}, length: {len(batch_data)}\")\n",
    "            \n",
    "#             if isinstance(batch_data, list):\n",
    "#                 for idx, item in enumerate(batch_data):\n",
    "#                     if len(data_list) >= max_data_points:\n",
    "#                         break\n",
    "#                     try:\n",
    "#                         if isinstance(item, Data):\n",
    "#                             required_attrs = ['x', 'edge_index', 'pos', 'y', 'mode_stats']\n",
    "#                             missing_attrs = [attr for attr in required_attrs if not hasattr(item, attr)]\n",
    "#                             if not missing_attrs:\n",
    "#                                 data_list.append(item)\n",
    "#                             else:\n",
    "#                                 print(f\"Skipping invalid item {idx} in batch {batch_num}. Missing attributes: {missing_attrs}\")\n",
    "#                         else:\n",
    "#                             print(f\"Skipping non-Data item {idx} in batch {batch_num}.\")\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error processing item {idx} in batch {batch_num}: {str(e)}\")\n",
    "#             else:\n",
    "#                 print(f\"Unexpected batch data type in batch {batch_num}: {type(batch_data)}\")\n",
    "            \n",
    "#             batch_num += 1\n",
    "#             print(f\"Loaded batch {batch_num-1}, current total: {len(data_list)} items\")\n",
    "            \n",
    "#             if psutil.virtual_memory().percent > 90:\n",
    "#                 print(\"Memory usage high, stopping data loading\")\n",
    "#                 break\n",
    "#         except FileNotFoundError:\n",
    "#             print(f\"Finished loading {batch_num-1} batches\")\n",
    "#             break\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading batch {batch_num}: {str(e)}\")\n",
    "#             batch_num += 1\n",
    "\n",
    "#     print(f\"Successfully loaded {len(data_list)} data points\")\n",
    "#     return data_list\n",
    "\n",
    "# # Usage\n",
    "# try:\n",
    "#     dataset_path = '../../data/train_data/sim_output_1pm_capacity_reduction_10k_11_10_2024/'\n",
    "#     data_list = get_combined_data(dataset_path)  # Let's look at the first two batches\n",
    "#     print(f\"Final count: Successfully loaded {len(data_list)} data points\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# def normalize_x_values(dataset, directory_path):\n",
    "#     try:\n",
    "#         shape_of_x = dataset[0].x.shape[1]\n",
    "#         print(f\"Shape of x: {shape_of_x}\")\n",
    "        \n",
    "#         list_of_scalers_to_save = []\n",
    "#         print(\"Concatenating x values...\")\n",
    "#         x_values = torch.cat([data.x for data in dataset], dim=0)\n",
    "#         print(f\"Concatenated x_values shape: {x_values.shape}\")\n",
    "\n",
    "#         for i in range(shape_of_x):\n",
    "#             print(f\"Processing feature {i}/{shape_of_x}\")\n",
    "#             all_node_features = replace_invalid_values(x_values[:, i].reshape(-1, 1)).numpy()\n",
    "            \n",
    "#             scaler = StandardScaler()\n",
    "#             scaler.fit(all_node_features)\n",
    "#             list_of_scalers_to_save.append(scaler)\n",
    "\n",
    "#             for j, data in enumerate(dataset):\n",
    "#                 if j % 100 == 0:\n",
    "#                     print(f\"Processing data point {j}/{len(dataset)}\")\n",
    "#                 data_x_dim = replace_invalid_values(data.x[:, i].reshape(-1, 1))\n",
    "#                 normalized_x_dim = torch.tensor(scaler.transform(data_x_dim.numpy()), dtype=torch.float)\n",
    "#                 if i == 0:\n",
    "#                     data.normalized_x = normalized_x_dim\n",
    "#                 else:\n",
    "#                     data.normalized_x = torch.cat((data.normalized_x, normalized_x_dim), dim=1)\n",
    "\n",
    "#         joblib.dump(list_of_scalers_to_save, (directory_path + 'x_scaler.pkl'))\n",
    "#         for data in dataset:\n",
    "#             data.x = data.normalized_x\n",
    "#             del data.normalized_x\n",
    "#         return dataset\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in normalize_x_values: {str(e)}\")\n",
    "#         traceback.print_exc()\n",
    "#         raise\n",
    "\n",
    "# def normalize_positional_features(dataset, directory_path):\n",
    "#     try:\n",
    "#         shape_of_pos = dataset[0].pos.shape\n",
    "#         print(f\"Shape of pos: {shape_of_pos}\")\n",
    "        \n",
    "#         list_of_scalers_to_save = []\n",
    "#         print(\"Concatenating positional values...\")\n",
    "#         pos_values = torch.cat([data.pos.reshape(data.pos.shape[0], -1) for data in dataset], dim=0)\n",
    "#         print(f\"Concatenated pos_values shape: {pos_values.shape}\")\n",
    "\n",
    "#         for i in range(pos_values.shape[1]):\n",
    "#             print(f\"Processing positional feature {i}/{pos_values.shape[1]}\")\n",
    "#             all_pos_features = replace_invalid_values(pos_values[:, i].reshape(-1, 1)).numpy()\n",
    "            \n",
    "#             scaler = StandardScaler()\n",
    "#             scaler.fit(all_pos_features)\n",
    "#             list_of_scalers_to_save.append(scaler)\n",
    "\n",
    "#             for j, data in enumerate(dataset):\n",
    "#                 if j % 100 == 0:\n",
    "#                     print(f\"Processing data point {j}/{len(dataset)}\")\n",
    "#                 data_pos_dim = replace_invalid_values(data.pos.reshape(data.pos.shape[0], -1)[:, i].reshape(-1, 1))\n",
    "#                 normalized_pos_dim = torch.tensor(scaler.transform(data_pos_dim.numpy()), dtype=torch.float)\n",
    "#                 if i == 0:\n",
    "#                     data.normalized_pos = normalized_pos_dim\n",
    "#                 else:\n",
    "#                     data.normalized_pos = torch.cat((data.normalized_pos, normalized_pos_dim), dim=1)\n",
    "\n",
    "#         print(\"Saving positional scalers...\")\n",
    "#         joblib.dump(list_of_scalers_to_save, (directory_path + 'pos_scaler.pkl'))\n",
    "#         print(\"Positional scalers saved successfully\")\n",
    "\n",
    "#         print(\"Updating pos values in dataset...\")\n",
    "#         for data in dataset:\n",
    "#             data.pos = data.normalized_pos.reshape(shape_of_pos)\n",
    "#             del data.normalized_pos\n",
    "#         print(\"Dataset pos values updated\")\n",
    "\n",
    "#         return dataset\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in normalize_positional_features: {str(e)}\")\n",
    "#         traceback.print_exc()\n",
    "#         raise\n",
    "\n",
    "# def normalize_mode_stats(dataset, directory_path):\n",
    "#     try:\n",
    "#         print(\"Starting mode stats normalization...\")\n",
    "#         # Initialize 12 StandardScalers for 6 sets of 2 dimensions\n",
    "#         scalers = [[StandardScaler() for _ in range(2)] for _ in range(6)]\n",
    "\n",
    "#         # Standardize the data\n",
    "#         for i in range(6):  # Iterate over the first dimension (6 sets)\n",
    "#             for j in range(2):  # Iterate over the second dimension (2D vectors)\n",
    "#                 print(f\"Processing mode stats dimension {i}, {j}\")\n",
    "#                 values = np.vstack([replace_invalid_values(data.mode_stats[i, j].reshape(-1, 1)) for data in dataset])\n",
    "#                 print(f\"Collected values shape: {values.shape}\")\n",
    "                \n",
    "#                 # Fit the corresponding scaler on the extracted values\n",
    "#                 scalers[i][j].fit(values)\n",
    "                \n",
    "#                 for k, data in enumerate(dataset):\n",
    "#                     if k % 100 == 0:\n",
    "#                         print(f\"Transforming data point {k}/{len(dataset)} for dimension {i}, {j}\")\n",
    "#                     data_mode_stats_dim = replace_invalid_values(data.mode_stats[i, j].reshape(-1, 1))\n",
    "#                     transformed = scalers[i][j].transform(data_mode_stats_dim).flatten()\n",
    "#                     # Convert the transformed NumPy array back into a torch tensor\n",
    "#                     data.mode_stats[i, j] = torch.tensor(transformed, dtype=torch.float32)\n",
    "\n",
    "#         print(\"Saving mode stats scalers...\")\n",
    "#         # Save the scalers using joblib\n",
    "#         for i in range(6):\n",
    "#             for j in range(2):\n",
    "#                 # Dump the scalers with meaningful names to differentiate them\n",
    "#                 scaler_path = directory_path + f'scaler_mode_stats_{i}_{j}.pkl'\n",
    "#                 joblib.dump(scalers[i][j], scaler_path)\n",
    "\n",
    "#         print(\"Mode stats scalers saved and dataset standardized.\")\n",
    "#         return dataset\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in normalize_mode_stats: {str(e)}\")\n",
    "#         traceback.print_exc()\n",
    "#         raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenhao-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import psutil\n",
    "import random\n",
    "import signal\n",
    "import subprocess\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch_geometric.data import Batch, Data\n",
    "from torch_geometric.utils import to_undirected\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import help_functions as hf\n",
    "import joblib\n",
    "import wandb\n",
    "\n",
    "# Adding scripts path to sys.path if not already included\n",
    "scripts_path = os.path.abspath(os.path.join('..'))\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "\n",
    "import gnn_architectures_improved as garch\n",
    "import gnn_io as gio\n",
    "\n",
    "def get_memory_info():\n",
    "        memory_info = psutil.virtual_memory()\n",
    "        total_memory = memory_info.total / (1024 ** 3)  # Convert bytes to GB\n",
    "        available_memory = memory_info.available / (1024 ** 3)  # Convert bytes to GB\n",
    "        used_memory = memory_info.used / (1024 ** 3)  # Convert bytes to GB\n",
    "        return total_memory, available_memory, used_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory: 125.49 GB\n",
      "Available Memory: 28.61 GB\n",
      "Used Memory: 95.67 GB\n"
     ]
    }
   ],
   "source": [
    "total_memory, available_memory, used_memory = get_memory_info()\n",
    "print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "print(f\"Available Memory: {available_memory:.2f} GB\")\n",
    "print(f\"Used Memory: {used_memory:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch number: 1\n",
      "Processing batch number: 2\n",
      "Processing batch number: 3\n",
      "Processing batch number: 4\n",
      "Processing batch number: 5\n",
      "Processing batch number: 6\n",
      "Processing batch number: 7\n",
      "Processing batch number: 8\n",
      "Processing batch number: 9\n",
      "Processing batch number: 10\n",
      "Processing batch number: 11\n",
      "Processing batch number: 12\n",
      "Processing batch number: 13\n",
      "Processing batch number: 14\n",
      "Processing batch number: 15\n",
      "Processing batch number: 16\n",
      "Processing batch number: 17\n",
      "Processing batch number: 18\n",
      "Processing batch number: 19\n",
      "Processing batch number: 20\n",
      "Processing batch number: 21\n",
      "Processing batch number: 22\n",
      "Processing batch number: 23\n",
      "Processing batch number: 24\n",
      "Processing batch number: 25\n",
      "Processing batch number: 26\n",
      "Processing batch number: 27\n",
      "Processing batch number: 28\n",
      "Processing batch number: 29\n",
      "Processing batch number: 30\n",
      "Processing batch number: 31\n",
      "Processing batch number: 32\n",
      "Processing batch number: 33\n",
      "Processing batch number: 34\n",
      "Processing batch number: 35\n",
      "Processing batch number: 36\n",
      "Processing batch number: 37\n",
      "Processing batch number: 38\n",
      "Processing batch number: 39\n",
      "Processing batch number: 40\n",
      "Processing batch number: 41\n",
      "Processing batch number: 42\n",
      "Processing batch number: 43\n",
      "Processing batch number: 44\n",
      "Processing batch number: 45\n",
      "Processing batch number: 46\n",
      "Processing batch number: 47\n",
      "Processing batch number: 48\n",
      "Processing batch number: 49\n",
      "Processing batch number: 50\n",
      "Processing batch number: 51\n",
      "Processing batch number: 52\n",
      "Processing batch number: 53\n",
      "Processing batch number: 54\n",
      "Processing batch number: 55\n",
      "Processing batch number: 56\n",
      "Processing batch number: 57\n",
      "Processing batch number: 58\n",
      "Processing batch number: 59\n",
      "Processing batch number: 60\n",
      "Processing batch number: 61\n",
      "Processing batch number: 62\n",
      "Processing batch number: 63\n",
      "Processing batch number: 64\n",
      "Processing batch number: 65\n",
      "Processing batch number: 66\n",
      "Processing batch number: 67\n",
      "Processing batch number: 68\n",
      "Processing batch number: 69\n",
      "Processing batch number: 70\n",
      "Processing batch number: 71\n",
      "Processing batch number: 72\n",
      "Processing batch number: 73\n",
      "Processing batch number: 74\n",
      "Processing batch number: 75\n",
      "Processing batch number: 76\n",
      "Processing batch number: 77\n",
      "Processing batch number: 78\n",
      "Processing batch number: 79\n",
      "Processing batch number: 80\n",
      "Processing batch number: 81\n",
      "Processing batch number: 82\n",
      "Processing batch number: 83\n",
      "Processing batch number: 84\n",
      "Processing batch number: 85\n",
      "Processing batch number: 86\n",
      "Processing batch number: 87\n",
      "Processing batch number: 88\n",
      "Processing batch number: 89\n",
      "Processing batch number: 90\n",
      "Processing batch number: 91\n",
      "Processing batch number: 92\n",
      "Processing batch number: 93\n",
      "Processing batch number: 94\n",
      "Processing batch number: 95\n",
      "Processing batch number: 96\n",
      "Processing batch number: 97\n",
      "Processing batch number: 98\n",
      "Processing batch number: 99\n",
      "Loaded 4950 items into datalist\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dataset_path = '../../data/train_data/sim_output_1pm_capacity_reduction_10k_15_10_2024/'\n",
    "    datalist = []\n",
    "    batch_num = 1\n",
    "    # while True and batch_num < 100:\n",
    "    while True:\n",
    "        print(f\"Processing batch number: {batch_num}\")\n",
    "        # total_memory, available_memory, used_memory = get_memory_info()\n",
    "        # print(f\"Total Memory: {total_memory:.2f} GB\")\n",
    "        # print(f\"Available Memory: {available_memory:.2f} GB\")\n",
    "        # print(f\"Used Memory: {used_memory:.2f} GB\")\n",
    "        batch_file = os.path.join(dataset_path, f'datalist_batch_{batch_num}.pt')\n",
    "        if not os.path.exists(batch_file):\n",
    "            break\n",
    "        batch_data = torch.load(batch_file, map_location='cpu')\n",
    "        if isinstance(batch_data, list):\n",
    "            datalist.extend(batch_data)\n",
    "        batch_num += 1\n",
    "    print(f\"Loaded {len(datalist)} items into datalist\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "    \n",
    "# Set parameters here\n",
    "params = {\"project_name\": \"test\",\n",
    "            \"num_epochs\": 1000,\n",
    "            \"batch_size\": 8,\n",
    "            \"point_net_conv_layer_structure_local_mlp\": [64, 128],\n",
    "            \"point_net_conv_layer_structure_global_mlp\": [256, 64],\n",
    "            \"gat_conv_layer_structure\": [128, 256, 256, 128],\n",
    "            \"graph_mlp_layer_structure\": [128, 256, 128],\n",
    "            \"lr\": 0.001,\n",
    "            \"gradient_accumulation_steps\": 3,\n",
    "            \"in_channels\": 15,\n",
    "            \"out_channels\": 1,\n",
    "            \"early_stopping_patience\": 100,\n",
    "            \"unique_model_description\": \"my_test\",\n",
    "            \"dropout\": 0.3,\n",
    "            \"use_dropout\": False\n",
    "        } \n",
    "    \n",
    "base_dir = '../../data/' + params['project_name'] + '/'\n",
    "unique_run_dir = os.path.join(base_dir, params['unique_model_description'])\n",
    "os.makedirs(unique_run_dir, exist_ok=True)\n",
    "\n",
    "# Define the paths here\n",
    "def get_paths(base_dir: str, unique_model_description: str, model_save_path: str = 'trained_model/model.pth'):\n",
    "    data_path = os.path.join(base_dir, unique_model_description)\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    model_save_to = os.path.join(data_path, model_save_path)\n",
    "    path_to_save_dataloader = os.path.join(data_path, 'data_created_during_training/')\n",
    "    os.makedirs(os.path.dirname(model_save_to), exist_ok=True)\n",
    "    os.makedirs(path_to_save_dataloader, exist_ok=True)\n",
    "    return model_save_to, path_to_save_dataloader\n",
    "\n",
    "model_save_path, path_to_save_dataloader = get_paths(base_dir=base_dir, unique_model_description= params['unique_model_description'], model_save_path= 'trained_model/model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking for NaNs: 100%|██████████| 4950/4950 [00:01<00:00, 3706.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN check results:\n",
      "Total items checked: 4950\n",
      "Items with NaNs in x: 0 (0.00%)\n",
      "Items with NaNs in pos: 0 (0.00%)\n",
      "Items with NaNs in mode_stats: 0 (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def check_nans_in_data(datalist):\n",
    "    nan_counts = {'x': 0, 'pos': 0, 'mode_stats': 0}\n",
    "    total_items = len(datalist)\n",
    "\n",
    "    for data in tqdm(datalist, desc=\"Checking for NaNs\"):\n",
    "        if torch.isnan(data.x).any():\n",
    "            nan_counts['x'] += 1\n",
    "        if torch.isnan(data.pos).any():\n",
    "            nan_counts['pos'] += 1\n",
    "        if torch.isnan(data.mode_stats).any():\n",
    "            nan_counts['mode_stats'] += 1\n",
    "\n",
    "    print(f\"NaN check results:\")\n",
    "    print(f\"Total items checked: {total_items}\")\n",
    "    print(f\"Items with NaNs in x: {nan_counts['x']} ({nan_counts['x']/total_items*100:.2f}%)\")\n",
    "    print(f\"Items with NaNs in pos: {nan_counts['pos']} ({nan_counts['pos']/total_items*100:.2f}%)\")\n",
    "    print(f\"Items with NaNs in mode_stats: {nan_counts['mode_stats']} ({nan_counts['mode_stats']/total_items*100:.2f}%)\")\n",
    "\n",
    "    return nan_counts\n",
    "\n",
    "# Usage\n",
    "nan_results = check_nans_in_data(datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 1., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 1., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalist[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.3630, 48.8842],\n",
       "         [ 2.3631, 48.8844],\n",
       "         [ 2.3630, 48.8843]],\n",
       "\n",
       "        [[ 2.3630, 48.8842],\n",
       "         [ 2.3629, 48.8841],\n",
       "         [ 2.3629, 48.8842]],\n",
       "\n",
       "        [[ 2.3614, 48.8810],\n",
       "         [ 2.3614, 48.8811],\n",
       "         [ 2.3614, 48.8810]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.3134, 48.8943],\n",
       "         [ 2.3134, 48.8943],\n",
       "         [ 2.3134, 48.8943]],\n",
       "\n",
       "        [[ 2.2712, 48.8380],\n",
       "         [ 2.2789, 48.8359],\n",
       "         [ 2.2750, 48.8370]],\n",
       "\n",
       "        [[ 2.2712, 48.8380],\n",
       "         [ 2.2712, 48.8380],\n",
       "         [ 2.2712, 48.8380]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalist[0].pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1822e+03, 3.6665e+03],\n",
       "        [1.8033e+03, 4.9954e+03],\n",
       "        [4.8058e+02, 4.4846e+03],\n",
       "        [7.8831e-01, 1.0569e+03],\n",
       "        [1.6104e+03, 5.4706e+03],\n",
       "        [1.0098e+03, 1.2123e+03]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datalist[0].mode_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(dataset_input, directory_path):\n",
    "    data_list = [dataset_input.dataset[idx] for idx in dataset_input.indices]\n",
    "    print(\"LEN DATALIST\")\n",
    "    print(len(data_list))\n",
    "    print(\"Fitting and normalizing x features...\")\n",
    "    normalized_data_list, x_scaler = normalize_x_features_batched(data_list)\n",
    "    print(\"x features normalized\")\n",
    "    print(len(normalized_data_list))\n",
    "    \n",
    "\n",
    "    print(\"Fitting and normalizing pos features...\")\n",
    "    normalized_data_list, pos_scaler = normalize_pos_features_batched(normalized_data_list)\n",
    "    print(\"Pos features normalized\")\n",
    "    \n",
    "    print(\"Fitting and normalizing modestats features...\")\n",
    "    normalized_data_list, modestats_scaler = normalize_modestats_features_batched(normalized_data_list)\n",
    "    print(\"Modestats features normalized\")\n",
    "    \n",
    "    print(\"FINAL LEN\")\n",
    "    print(len(normalized_data_list))\n",
    "    return normalized_data_list, (x_scaler, pos_scaler, modestats_scaler)\n",
    "\n",
    "def normalize_x_features_batched(data_list, batch_size=100):\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # First pass: Fit the scaler\n",
    "    for i in tqdm(range(0, len(data_list), batch_size), desc=\"Fitting scaler\"):\n",
    "        batch = data_list[i:i+batch_size]\n",
    "        batch_x = np.vstack([data.x.numpy() for data in batch])\n",
    "        scaler.partial_fit(batch_x)\n",
    "    \n",
    "    # Second pass: Transform the data\n",
    "    for i in tqdm(range(0, len(data_list), batch_size), desc=\"Normalizing x features\"):\n",
    "        batch = data_list[i:i+batch_size]\n",
    "        batch_x = np.vstack([data.x.numpy() for data in batch])\n",
    "        batch_x_normalized = scaler.transform(batch_x)\n",
    "        for j, data in enumerate(batch):\n",
    "            data.x = torch.tensor(batch_x_normalized[j*31140:(j+1)*31140], dtype=torch.float32)\n",
    "    \n",
    "    return data_list, scaler\n",
    "\n",
    "def normalize_pos_features_batched(data_list, batch_size=1000):\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # First pass: Fit the scaler\n",
    "    for i in tqdm(range(0, len(data_list), batch_size), desc=\"Fitting scaler\"):\n",
    "        batch = data_list[i:i+batch_size]\n",
    "        batch_pos = np.vstack([data.pos.numpy().reshape(-1, 6) for data in batch])\n",
    "        scaler.partial_fit(batch_pos)\n",
    "    \n",
    "    # Second pass: Transform the data\n",
    "    for i in tqdm(range(0, len(data_list), batch_size), desc=\"Normalizing pos features\"):\n",
    "        batch = data_list[i:i+batch_size]\n",
    "        for data in batch:\n",
    "            pos_reshaped = data.pos.numpy().reshape(-1, 6)\n",
    "            pos_normalized = scaler.transform(pos_reshaped)\n",
    "            data.pos = torch.tensor(pos_normalized.reshape(31140, 3, 2), dtype=torch.float32)\n",
    "    \n",
    "    return data_list, scaler\n",
    "\n",
    "def normalize_modestats_features_batched(data_list, batch_size=1000):\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # First pass: Fit the scaler\n",
    "    for i in tqdm(range(0, len(data_list), batch_size), desc=\"Fitting scaler\"):\n",
    "        batch = data_list[i:i+batch_size]\n",
    "        batch_modestats = np.vstack([data.mode_stats.numpy().reshape(1, -1) for data in batch])\n",
    "        scaler.partial_fit(batch_modestats)\n",
    "    \n",
    "    # Second pass: Transform the data\n",
    "    for i in tqdm(range(0, len(data_list), batch_size), desc=\"Normalizing modestats features\"):\n",
    "        batch = data_list[i:i+batch_size]\n",
    "        for data in batch:\n",
    "            modestats_reshaped = data.mode_stats.numpy().reshape(1, -1)\n",
    "            modestats_normalized = scaler.transform(modestats_reshaped)\n",
    "            data.mode_stats = torch.tensor(modestats_normalized.reshape(6, 2), dtype=torch.float32)\n",
    "    \n",
    "    return data_list, scaler\n",
    "\n",
    "def replace_invalid_values(tensor):\n",
    "    # print(f\"Input tensor shape: {tensor.shape}\")\n",
    "    # nan_count = torch.isnan(tensor).sum().item()\n",
    "    # inf_count = torch.isinf(tensor).sum().item()\n",
    "    # print(f\"NaN count: {nan_count}, Inf count: {inf_count}\")\n",
    "    \n",
    "    tensor[torch.isnan(tensor)] = 0  # replace NaNs with 0\n",
    "    tensor[torch.isinf(tensor)] = 0  # replace inf and -inf with 0\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prepare_data_with_graph_features with 4950 items\n",
      "Splitting into subsets...\n",
      "Total dataset length: 4950\n",
      "Training subset length: 3960\n",
      "Validation subset length: 742\n",
      "Test subset length: 248\n",
      "Split complete. Train: 3960, Valid: 742, Test: 248\n",
      "Normalizing train set...\n",
      "LEN DATALIST\n",
      "3960\n",
      "Fitting and normalizing x features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting scaler:   0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting scaler: 100%|██████████| 40/40 [00:39<00:00,  1.01it/s]\n",
      "Normalizing x features: 100%|██████████| 40/40 [00:37<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x features normalized\n",
      "3960\n",
      "Fitting and normalizing pos features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting scaler:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# def prepare_data_with_graph_features(datalist, batch_size, path_to_save_dataloader):\n",
    "#     print(f\"Starting prepare_data_with_graph_features with {len(datalist)} items\")\n",
    "    \n",
    "#     try:\n",
    "#         print(\"Splitting into subsets...\")\n",
    "#         train_set, valid_set, test_set = gio.split_into_subsets(dataset=datalist, train_ratio=0.8, val_ratio=0.15, test_ratio=0.05)\n",
    "#         print(f\"Split complete. Train: {len(train_set)}, Valid: {len(valid_set)}, Test: {len(test_set)}\")\n",
    "        \n",
    "#         print(\"Normalizing train set...\")\n",
    "#         train_set_normalized, scalers_train = normalize_dataset(dataset_input=train_set, directory_path=path_to_save_dataloader + \"train_\")\n",
    "#         print(\"Train set normalized\")\n",
    "        \n",
    "#         print(\"Normalizing validation set...\")\n",
    "#         valid_set_normalized, scalers_validation = normalize_dataset(dataset_input=valid_set, directory_path=path_to_save_dataloader + \"valid_\")\n",
    "#         print(\"Validation set normalized\")\n",
    "#         print(len(valid_set_normalized))\n",
    "        \n",
    "#         print(\"Creating train loader...\")\n",
    "#         train_loader = DataLoader(dataset=train_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, prefetch_factor=2, pin_memory=True, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "#         print(\"Train loader created\")\n",
    "        \n",
    "#         print(\"Creating validation loader...\")\n",
    "#         val_loader = DataLoader(dataset=valid_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "#         print(\"Validation loader created\")\n",
    "        \n",
    "#         return train_loader, val_loader, scalers_train, scalers_validation\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in prepare_data_with_graph_features: {str(e)}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         raise\n",
    "\n",
    "# train_dl, valid_dl, scalers_train, scalers_validation = prepare_data_with_graph_features(datalist=datalist, batch_size= params['batch_size'], path_to_save_dataloader= path_to_save_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting into subsets...\n",
      "Total dataset length: 4950\n",
      "Training subset length: 3960\n",
      "Validation subset length: 742\n",
      "Test subset length: 248\n",
      "Split complete. Train: 3960, Valid: 742, Test: 248\n"
     ]
    }
   ],
   "source": [
    "batch_size = params['batch_size']\n",
    "print(\"Splitting into subsets...\")\n",
    "train_set, valid_set, test_set = gio.split_into_subsets(dataset=datalist, train_ratio=0.8, val_ratio=0.15, test_ratio=0.05)\n",
    "print(f\"Split complete. Train: {len(train_set)}, Valid: {len(valid_set)}, Test: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing train set...\n",
      "LEN DATALIST\n",
      "3960\n",
      "Fitting and normalizing x features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting scaler: 100%|██████████| 40/40 [00:31<00:00,  1.26it/s]\n",
      "Normalizing x features: 100%|██████████| 40/40 [00:15<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x features normalized\n",
      "3960\n",
      "Fitting and normalizing pos features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting scaler: 100%|██████████| 4/4 [00:13<00:00,  3.31s/it]\n",
      "Normalizing pos features: 100%|██████████| 4/4 [00:10<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos features normalized\n",
      "Fitting and normalizing modestats features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting scaler: 100%|██████████| 4/4 [00:00<00:00, 84.29it/s]\n",
      "Normalizing modestats features: 100%|██████████| 4/4 [00:00<00:00,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modestats features normalized\n",
      "FINAL LEN\n",
      "3960\n",
      "Train set normalized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalizing train set...\")\n",
    "train_set_normalized, scalers_train = normalize_dataset(dataset_input=train_set, directory_path=path_to_save_dataloader + \"train_\")\n",
    "print(\"Train set normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing validation set...\n",
      "LEN DATALIST\n",
      "742\n",
      "Fitting and normalizing x features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting scaler: 100%|██████████| 8/8 [00:04<00:00,  1.78it/s]\n",
      "Normalizing x features: 100%|██████████| 8/8 [00:02<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x features normalized\n",
      "742\n",
      "Fitting and normalizing pos features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting scaler: 100%|██████████| 1/1 [00:02<00:00,  2.21s/it]\n",
      "Normalizing pos features: 100%|██████████| 1/1 [00:01<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos features normalized\n",
      "Fitting and normalizing modestats features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting scaler: 100%|██████████| 1/1 [00:00<00:00, 126.19it/s]\n",
      "Normalizing modestats features: 100%|██████████| 1/1 [00:00<00:00, 12.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modestats features normalized\n",
      "FINAL LEN\n",
      "742\n",
      "Validation set normalized\n",
      "742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Normalizing validation set...\")\n",
    "valid_set_normalized, scalers_validation = normalize_dataset(dataset_input=valid_set, directory_path=path_to_save_dataloader + \"valid_\")\n",
    "print(\"Validation set normalized\")\n",
    "print(len(valid_set_normalized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train loader...\n",
      "Train loader created\n",
      "Creating validation loader...\n",
      "Validation loader created\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating train loader...\")\n",
    "train_loader = DataLoader(dataset=train_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, prefetch_factor=2, pin_memory=True, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "print(\"Train loader created\")\n",
    "\n",
    "print(\"Creating validation loader...\")\n",
    "val_loader = DataLoader(dataset=valid_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "print(\"Validation loader created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.5609,  4.5641],\n",
       "        [ 2.4442,  2.2361],\n",
       "        [ 1.5264,  1.4667],\n",
       "        [-2.9498, -2.9884],\n",
       "        [-0.2325, -2.1963],\n",
       "        [-5.2668, -5.2668]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0].mode_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = train_loader\n",
    "valid_dl = val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU 0 with CUDA_VISIBLE_DEVICES=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33menatterer\u001b[0m (\u001b[33mtum-traffic-engineering\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/enatterer/Development/gnn_predicting_effects_of_traffic_policies/scripts/training/wandb/run-20241015_212209-hxbjq1l0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tum-traffic-engineering/test/runs/hxbjq1l0' target=\"_blank\">fine-dust-73</a></strong> to <a href='https://wandb.ai/tum-traffic-engineering/test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tum-traffic-engineering/test' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tum-traffic-engineering/test/runs/hxbjq1l0' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/test/runs/hxbjq1l0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized\n",
      "MyGnn(\n",
      "  (point_net_conv_1): PointNetConv(local_nn=Sequential(\n",
      "    (0): Linear(in_features=16, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "  ), global_nn=Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (point_net_conv_2): PointNetConv(local_nn=Sequential(\n",
      "    (0): Linear(in_features=66, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "  ), global_nn=Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (point_net_conv_3): PointNetConv(local_nn=Sequential(\n",
      "    (0): Linear(in_features=66, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "  ), global_nn=Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (graph_predictor): Sequential(\n",
      "    (0): Linear(in_features=140, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      "  (gat_graph_layers): Sequential(\n",
      "    (0) - GATConv(128, 256, heads=1): x, edge_index -> x\n",
      "    (1) - ReLU(inplace=True): x -> x\n",
      "    (2) - GATConv(256, 256, heads=1): x, edge_index -> x\n",
      "    (3) - ReLU(inplace=True): x -> x\n",
      "    (4) - GATConv(256, 128, heads=1): x, edge_index -> x\n",
      "    (5) - ReLU(inplace=True): x -> x\n",
      "    (6) - GATConv(128, 1, heads=1): x, edge_index -> x\n",
      "  )\n",
      "  (gat_graph_layers_local): Sequential(\n",
      "    (0) - GATConv(128, 256, heads=1): x, edge_index -> x\n",
      "    (1) - ReLU(inplace=True): x -> x\n",
      "    (2) - GATConv(256, 256, heads=1): x, edge_index -> x\n",
      "    (3) - ReLU(inplace=True): x -> x\n",
      "    (4) - GATConv(256, 128, heads=1): x, edge_index -> x\n",
      "    (5) - ReLU(inplace=True): x -> x\n",
      "    (6) - GATConv(128, 1, heads=1): x, edge_index -> x\n",
      "  )\n",
      "  (graph_mlp): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "baseline loss mean 2.9664947986602783\n",
      "baseline loss no  2.9734604358673096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:   0%|          | 0/495 [00:00<?, ?it/s]/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "Epoch 1/1000: 100%|██████████| 495/495 [01:20<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, validation loss: 2.962466355293028, lr: 2.47e-05, r^2: -0.00022220611572265625\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.962466355293028\n",
      "Checkpoint saved to ../../data/test/my_test/trained_model/checkpoints/checkpoint_epoch_0.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, validation loss: 2.931720866951891, lr: 4.945e-05, r^2: 0.009590446949005127\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.931720866951891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/1000: 100%|██████████| 495/495 [01:21<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, validation loss: 2.8703624074177077, lr: 7.42e-05, r^2: 0.030202865600585938\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.8703624074177077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, validation loss: 2.833978147916896, lr: 9.894999999999999e-05, r^2: 0.04276949167251587\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.833978147916896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/1000: 100%|██████████| 495/495 [01:21<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, validation loss: 2.8111383530401413, lr: 0.0001237, r^2: 0.05066913366317749\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.8111383530401413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, validation loss: 2.8192207556898876, lr: 0.00014845, r^2: 0.048224449157714844\n",
      "EarlyStopping counter: 1 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, validation loss: 2.9590860848785727, lr: 0.0001732, r^2: 0.0005736947059631348\n",
      "EarlyStopping counter: 2 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, validation loss: 2.7239293206122612, lr: 0.00019795, r^2: 0.07986128330230713\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.7239293206122612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, validation loss: 2.6757970368990334, lr: 0.00022270000000000002, r^2: 0.09650003910064697\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.6757970368990334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/1000: 100%|██████████| 495/495 [01:21<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, validation loss: 2.67127533881895, lr: 0.00024745, r^2: 0.09760415554046631\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.67127533881895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10, validation loss: 2.659845152208882, lr: 0.0002722, r^2: 0.10166752338409424\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.659845152208882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/1000: 100%|██████████| 495/495 [01:21<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11, validation loss: 2.666088201666391, lr: 0.00029695, r^2: 0.09942156076431274\n",
      "EarlyStopping counter: 1 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12, validation loss: 2.725151836231191, lr: 0.0003217, r^2: 0.07979017496109009\n",
      "EarlyStopping counter: 2 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13, validation loss: 2.6515517004074587, lr: 0.00034645, r^2: 0.10450887680053711\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.6515517004074587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14, validation loss: 2.8467646875689105, lr: 0.00037119999999999997, r^2: 0.03860306739807129\n",
      "EarlyStopping counter: 1 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15, validation loss: 2.7128318996839624, lr: 0.00039595000000000006, r^2: 0.0837705135345459\n",
      "EarlyStopping counter: 2 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/1000: 100%|██████████| 495/495 [01:21<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16, validation loss: 2.5424389198262203, lr: 0.00042070000000000003, r^2: 0.1412370800971985\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.5424389198262203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17, validation loss: 2.5172953810743106, lr: 0.00044545, r^2: 0.14963650703430176\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.5172953810743106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18, validation loss: 2.4832903236471195, lr: 0.0004702, r^2: 0.16138958930969238\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.4832903236471195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19, validation loss: 2.440176543369088, lr: 0.00049495, r^2: 0.17577511072158813\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.440176543369088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20, validation loss: 2.4584474819962696, lr: 0.0005197000000000001, r^2: 0.16999465227127075\n",
      "Checkpoint saved to ../../data/test/my_test/trained_model/checkpoints/checkpoint_epoch_20.pt\n",
      "EarlyStopping counter: 1 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 21, validation loss: 2.423237185324392, lr: 0.00054445, r^2: 0.18165606260299683\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.423237185324392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 22, validation loss: 2.383851102603379, lr: 0.0005692000000000001, r^2: 0.19487369060516357\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.383851102603379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 23, validation loss: 2.3598719361007854, lr: 0.00059395, r^2: 0.20299017429351807\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.3598719361007854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 24, validation loss: 2.3470516127924763, lr: 0.0006187, r^2: 0.20722264051437378\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.3470516127924763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 25, validation loss: 2.4297991721860823, lr: 0.00064345, r^2: 0.17927569150924683\n",
      "EarlyStopping counter: 1 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 26, validation loss: 2.3602595995831233, lr: 0.0006682, r^2: 0.20272880792617798\n",
      "EarlyStopping counter: 2 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 27, validation loss: 2.436246971930227, lr: 0.00069295, r^2: 0.17718863487243652\n",
      "EarlyStopping counter: 3 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 28, validation loss: 2.3464901293477705, lr: 0.0007177, r^2: 0.20759552717208862\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.3464901293477705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 29, validation loss: 2.3018383031250327, lr: 0.0007424500000000001, r^2: 0.2224147915840149\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.3018383031250327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30, validation loss: 2.3086859820991434, lr: 0.0007672, r^2: 0.2199857234954834\n",
      "EarlyStopping counter: 1 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 31, validation loss: 2.316098810524069, lr: 0.00079195, r^2: 0.2179507613182068\n",
      "EarlyStopping counter: 2 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 32, validation loss: 2.3781781581140335, lr: 0.0008167, r^2: 0.19672662019729614\n",
      "EarlyStopping counter: 3 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/1000: 100%|██████████| 495/495 [01:21<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 33, validation loss: 2.333648440658405, lr: 0.0008414500000000001, r^2: 0.21169525384902954\n",
      "EarlyStopping counter: 4 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/1000: 100%|██████████| 495/495 [01:21<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 34, validation loss: 2.3821157486208024, lr: 0.0008662, r^2: 0.19551688432693481\n",
      "EarlyStopping counter: 5 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/1000: 100%|██████████| 495/495 [01:21<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35, validation loss: 2.336846943824522, lr: 0.00089095, r^2: 0.21053647994995117\n",
      "EarlyStopping counter: 6 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 36, validation loss: 2.330545784324728, lr: 0.0009157, r^2: 0.21275633573532104\n",
      "EarlyStopping counter: 7 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 37, validation loss: 2.298000599748345, lr: 0.00094045, r^2: 0.22382193803787231\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.298000599748345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/1000: 100%|██████████| 495/495 [01:20<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 38, validation loss: 2.2879310372055217, lr: 0.0009651999999999999, r^2: 0.2271556258201599\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.2879310372055217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/1000: 100%|██████████| 495/495 [01:21<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 39, validation loss: 2.2904008819210913, lr: 0.00098995, r^2: 0.22657173871994019\n",
      "EarlyStopping counter: 1 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40, validation loss: 2.21142170634321, lr: 0.0003999996218996755, r^2: 0.25312870740890503\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.21142170634321\n",
      "Checkpoint saved to ../../data/test/my_test/trained_model/checkpoints/checkpoint_epoch_40.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/1000: 100%|██████████| 495/495 [01:21<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 41, validation loss: 2.1971981410057313, lr: 0.0003999972768877301, r^2: 0.25789082050323486\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.1971981410057313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/1000: 100%|██████████| 495/495 [01:21<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 42, validation loss: 2.1791428058378157, lr: 0.000399992788261618, r^2: 0.2638503909111023\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.1791428058378157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 43, validation loss: 2.1682041909105036, lr: 0.0003999861560694491, r^2: 0.26755571365356445\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.1682041909105036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 44, validation loss: 2.1602882544199624, lr: 0.0003999773803823088, r^2: 0.2704750895500183\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.1602882544199624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 45, validation loss: 2.1481677255322857, lr: 0.0003999664612942568, r^2: 0.2743532061576843\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.1481677255322857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/1000: 100%|██████████| 495/495 [01:21<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46, validation loss: 2.153240002611632, lr: 0.00039995339892232615, r^2: 0.27270740270614624\n",
      "EarlyStopping counter: 1 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/1000: 100%|██████████| 495/495 [01:21<00:00,  6.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 47, validation loss: 2.1460842637605566, lr: 0.00039993819340652226, r^2: 0.27527791261672974\n",
      "Best model saved to ../../data/test/my_test/trained_model/model.pth with validation loss: 2.1460842637605566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/1000:  62%|██████▏   | 306/495 [00:49<00:32,  5.76it/s]"
     ]
    }
   ],
   "source": [
    "gpus = hf.get_available_gpus()\n",
    "best_gpu = hf.select_best_gpu(gpus)\n",
    "hf.set_cuda_visible_device(best_gpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config = hf.setup_wandb(params['project_name'], {\n",
    "    \"epochs\": params['num_epochs'],\n",
    "    \"batch_size\": params['batch_size'],\n",
    "    \"lr\": params['lr'],\n",
    "    \"gradient_accumulation_steps\": params['gradient_accumulation_steps'],\n",
    "    \"early_stopping_patience\": params['early_stopping_patience'],\n",
    "    \"point_net_conv_local_mlp\": params['point_net_conv_layer_structure_local_mlp'],\n",
    "    \"point_net_conv_global_mlp\": params['point_net_conv_layer_structure_global_mlp'],\n",
    "    \"gat_conv_layer_structure\": params['gat_conv_layer_structure'],\n",
    "    \"graph_mlp_layer_structure\": params['graph_mlp_layer_structure'],\n",
    "    \"in_channels\": params['in_channels'],\n",
    "    \"out_channels\": params['out_channels'],\n",
    "    \"dropout\": params['dropout'],\n",
    "    \"use_dropout\": params['use_dropout']\n",
    "})\n",
    "\n",
    "model = garch.MyGnn(in_channels=config.in_channels, out_channels=config.out_channels, point_net_conv_layer_structure_local_mlp=config.point_net_conv_local_mlp,\n",
    "                            point_net_conv_layer_structure_global_mlp=config.point_net_conv_global_mlp,\n",
    "                            gat_conv_layer_structure=config.gat_conv_layer_structure,\n",
    "                            graph_mlp_layer_structure=config.graph_mlp_layer_structure,\n",
    "                            dropout=config.dropout, use_dropout=config.use_dropout)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_fct = torch.nn.MSELoss()\n",
    "\n",
    "baseline_loss_mean_target = gio.compute_baseline_of_mean_target(dataset=train_dl, loss_fct=loss_fct)\n",
    "baseline_loss = gio.compute_baseline_of_no_policies(dataset=train_dl, loss_fct=loss_fct)\n",
    "print(\"baseline loss mean \" + str(baseline_loss_mean_target))\n",
    "print(\"baseline loss no  \" +str(baseline_loss) )\n",
    "\n",
    "early_stopping = gio.EarlyStopping(patience=params['early_stopping_patience'], verbose=True)\n",
    "best_val_loss, best_epoch = garch.train(model=model, \n",
    "            config=config, \n",
    "            loss_fct=loss_fct,\n",
    "            optimizer=torch.optim.AdamW(model.parameters(), lr=config.lr, weight_decay=1e-4),\n",
    "            train_dl=train_dl,  \n",
    "            valid_dl=valid_dl,\n",
    "            device=device, \n",
    "            early_stopping=early_stopping,\n",
    "            accumulation_steps=config.gradient_accumulation_steps,\n",
    "            model_save_path=model_save_path,\n",
    "            use_gradient_clipping=True,\n",
    "            lr_scheduler_warmup_steps=20000,\n",
    "            lr_scheduler_cosine_decay_rate=0.2)\n",
    "print(f'Best model saved to {model_save_path} with validation loss: {best_val_loss} at epoch {best_epoch}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call this function during training without the scalars and with the directory path, and during the testing with the saved scalars and without a directory path to save.\n",
    "# def normalize_dataset(dataset_input, directory_path):\n",
    "#     dataset = copy_subset(dataset_input)\n",
    "#     dataset = normalize_x_values(dataset, directory_path)\n",
    "#     dataset = normalize_positional_features(dataset, directory_path)\n",
    "#     dataset = normalize_mode_stats(dataset, directory_path)\n",
    "#     return dataset\n",
    "    \n",
    "    \n",
    "# def normalize_x_values(dataset, directory_path):\n",
    "#     try:\n",
    "#         shape_of_x = dataset[0].x.shape[1]\n",
    "#         print(f\"Shape of x: {shape_of_x}\")\n",
    "        \n",
    "#         list_of_scalers_to_save = []\n",
    "#         print(\"Processing x values...\")\n",
    "\n",
    "#         # Process in batches\n",
    "#         batch_size = 100  # Adjust this value based on your available memory\n",
    "#         for i in range(shape_of_x):\n",
    "#             print(f\"Processing feature {i}/{shape_of_x}\")\n",
    "#             scaler = StandardScaler()\n",
    "            \n",
    "#             # Fit scaler in batches\n",
    "#             for j in range(0, len(dataset), batch_size):\n",
    "#                 batch = dataset[j:j+batch_size]\n",
    "#                 print(f\"Processing batch {j//batch_size + 1}/{len(dataset)//batch_size + 1}\")\n",
    "#                 batch_x_values = torch.cat([data.x[:, i].reshape(-1, 1) for data in batch], dim=0)\n",
    "#                 batch_x_values = replace_invalid_values(batch_x_values)\n",
    "#                 scaler.partial_fit(batch_x_values.numpy())\n",
    "\n",
    "#             list_of_scalers_to_save.append(scaler)\n",
    "\n",
    "#             # Transform data\n",
    "#             for j, data in enumerate(dataset):\n",
    "#                 if j % 100 == 0:\n",
    "#                     print(f\"Transforming data point {j}/{len(dataset)}\")\n",
    "#                 data_x_dim = replace_invalid_values(data.x[:, i].reshape(-1, 1))\n",
    "#                 normalized_x_dim = torch.tensor(scaler.transform(data_x_dim.numpy()), dtype=torch.float)\n",
    "#                 if i == 0:\n",
    "#                     data.normalized_x = normalized_x_dim\n",
    "#                 else:\n",
    "#                     data.normalized_x = torch.cat((data.normalized_x, normalized_x_dim), dim=1)\n",
    "\n",
    "#         print(\"Saving scalers...\")\n",
    "#         joblib.dump(list_of_scalers_to_save, (directory_path + 'x_scaler.pkl'))\n",
    "#         print(\"Scalers saved successfully\")\n",
    "\n",
    "#         print(\"Updating x values in dataset...\")\n",
    "#         for data in dataset:\n",
    "#             data.x = data.normalized_x\n",
    "#             del data.normalized_x\n",
    "#         print(\"Dataset x values updated\")\n",
    "\n",
    "#         return dataset\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in normalize_x_values: {str(e)}\")\n",
    "#         traceback.print_exc()\n",
    "#         raise   \n",
    "\n",
    "\n",
    "# import networkx as nx\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from collections import Counter\n",
    "\n",
    "# def check_scale_free_distribution(edge_index, num_nodes):\n",
    "#     # Create a NetworkX graph from the edge_index\n",
    "#     G = nx.Graph()\n",
    "#     G.add_nodes_from(range(num_nodes))\n",
    "#     edge_list = edge_index.t().tolist()\n",
    "#     G.add_edges_from(edge_list)\n",
    "\n",
    "#     # Calculate degree for each node\n",
    "#     degrees = [d for n, d in G.degree()]\n",
    "#     degree_counts = Counter(degrees)\n",
    "\n",
    "#     # Sort the degree counts\n",
    "#     sorted_degree_counts = sorted(degree_counts.items())\n",
    "#     x = [k for k, v in sorted_degree_counts]\n",
    "#     y = [v for k, v in sorted_degree_counts]\n",
    "\n",
    "#     # Plot degree distribution on log-log scale\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.loglog(x, y, 'bo-')\n",
    "#     plt.xlabel('Degree (log scale)')\n",
    "#     plt.ylabel('Count (log scale)')\n",
    "#     plt.title('Degree Distribution (Log-Log Scale)')\n",
    "#     plt.grid(True)\n",
    "\n",
    "#     # Fit a power law distribution\n",
    "#     x_log = np.log(x)\n",
    "#     y_log = np.log(y)\n",
    "#     coeffs = np.polyfit(x_log[1:], y_log[1:], 1)\n",
    "#     power_law_exponent = -coeffs[0]\n",
    "\n",
    "#     # Plot the fitted line\n",
    "#     x_fit = np.logspace(np.log10(min(x)), np.log10(max(x)), 100)\n",
    "#     y_fit = np.exp(coeffs[1]) * x_fit**(-power_law_exponent)\n",
    "#     plt.loglog(x_fit, y_fit, 'r--', label=f'Power Law Fit (γ ≈ {power_law_exponent:.2f})')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "\n",
    "#     print(f\"Estimated power law exponent: γ ≈ {power_law_exponent:.2f}\")\n",
    "    \n",
    "#     if 2 < power_law_exponent < 3:\n",
    "#         print(\"The network shows characteristics of a scale-free network.\")\n",
    "#     else:\n",
    "#         print(\"The network may not be scale-free.\")\n",
    "\n",
    "#     return power_law_exponent\n",
    "\n",
    "# # Usage example:\n",
    "# # Assuming you have a PyTorch Geometric Data object called 'data'\n",
    "# exponent = check_scale_free_distribution(data.edge_index, data.num_nodes)\n",
    "\n",
    "# from torch_geometric.utils import to_undirected, is_undirected\n",
    "\n",
    "# # Assuming you're working with the first graph in your dataset\n",
    "# data = train_dl.dataset[0]\n",
    "\n",
    "# # Check if the graph is already undirected\n",
    "# # if not is_undirected(data.edge_index):\n",
    "# #     # If it's directed, convert it to undirected\n",
    "# #     data.edge_index = to_undirected(data.edge_index)\n",
    "# #     print(\"Graph has been converted to undirected.\")\n",
    "# # else:\n",
    "# #     print(\"Graph is already undirected.\")\n",
    "\n",
    "# # Verify that the graph is now undirected\n",
    "# print(f\"Is the graph undirected? {is_undirected(data.edge_index)}\")\n",
    "\n",
    "\n",
    "# def normalize_mode_stats(dataset, directory_path):\n",
    "#     # Initialize 12 StandardScalers for 6 sets of 2 dimensions\n",
    "#     scalers = [[StandardScaler() for _ in range(2)] for _ in range(6)]\n",
    "\n",
    "#     # Standardize the data\n",
    "#     for i in range(6):  # Iterate over the first dimension (6 sets)\n",
    "#         for j in range(2):  # Iterate over the second dimension (2D vectors)\n",
    "#             values = np.vstack([data.mode_stats[i, j].numpy().reshape(-1, 1) for data in dataset])\n",
    "#             # Fit the corresponding scaler on the extracted values\n",
    "#             scalers[i][j].fit(values)\n",
    "#             for data in dataset:\n",
    "#                 transformed = scalers[i][j].transform(data.mode_stats[i, j].numpy().reshape(-1, 1)).flatten()\n",
    "#                 # Convert the transformed NumPy array back into a torch tensor\n",
    "#                 data.mode_stats[i, j] = torch.tensor(transformed, dtype=torch.float32)\n",
    "    \n",
    "#     # Save the scalers using joblib\n",
    "#     for i in range(6):\n",
    "#         for j in range(2):\n",
    "#             # Dump the scalers with meaningful names to differentiate them\n",
    "#             scaler_path = directory_path + f'scaler_mode_stats_{i}_{j}.pkl'\n",
    "#             joblib.dump(scalers[i][j], scaler_path)\n",
    "\n",
    "#     print(\"Mode stats scalers saved and dataset standardized.\")\n",
    "#     return dataset\n",
    "\n",
    "# def replace_invalid_values(tensor):\n",
    "#     tensor[tensor != tensor] = 0  # replace NaNs with 0\n",
    "#     tensor[tensor == float('inf')] = 0  # replace inf with 0\n",
    "#     tensor[tensor == float('-inf')] = 0  # replace -inf with 0\n",
    "#     return tensor\n",
    "\n",
    "\n",
    "\n",
    "# def prepare_data_with_graph_features(datalist, batch_size, path_to_save_dataloader):\n",
    "#     # datalist = [Data(x=d['x'], edge_index=d['edge_index'], edge_attr=d['edge_attr'], pos=d['pos'], y=d['y'], mode_stats=d['mode_stats']) for d in data_dict_list]\n",
    "#     train_set, valid_set, test_set = gio.split_into_subsets(dataset=datalist, train_ratio=0.8, val_ratio=0.15, test_ratio=0.05)\n",
    "    \n",
    "#     train_set_normalized = normalize_dataset(dataset_input = train_set, directory_path=path_to_save_dataloader + \"train_\")\n",
    "#     valid_set_normalized = normalize_dataset(dataset_input = valid_set, directory_path=path_to_save_dataloader + \"valid_\")\n",
    "#     # # test_set_normalized = normalize_dataset(dataset_input = test_set, directory_path=path_to_save_dataloader + \"test_\")\n",
    "        \n",
    "#     train_loader = DataLoader(dataset=train_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, prefetch_factor=2, pin_memory=True, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "#     val_loader = DataLoader(dataset=valid_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "#     # test_loader = DataLoader(dataset=test_set_normalized, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "#     # gio.save_dataloader(test_loader, path_to_save_dataloader + 'test_dl.pt')\n",
    "#     # gio.save_dataloader_params(test_loader, path_to_save_dataloader + 'test_loader_params.json')\n",
    "    \n",
    "#     return train_loader, val_loader\n",
    "\n",
    "\n",
    "# def normalize_x_values(dataset, directory_path):\n",
    "#     shape_of_x = dataset[0].x.shape[1]\n",
    "#     list_of_scalers_to_save = []\n",
    "#     x_values = torch.cat([data.x for data in dataset], dim=0)\n",
    "\n",
    "#     for i in range(shape_of_x):\n",
    "#         all_node_features = replace_invalid_values(x_values[:, i].reshape(-1, 1)).numpy()\n",
    "        \n",
    "#         scaler = StandardScaler()\n",
    "#         print(f\"Scaler created for x values at index {i}: {scaler}\")\n",
    "#         scaler.fit(all_node_features)\n",
    "#         list_of_scalers_to_save.append(scaler)\n",
    "\n",
    "#         for data in dataset:\n",
    "#             data_x_dim = replace_invalid_values(data.x[:, i].reshape(-1, 1))\n",
    "#             normalized_x_dim = torch.tensor(scaler.transform(data_x_dim.numpy()), dtype=torch.float)\n",
    "#             if i == 0:\n",
    "#                 data.normalized_x = normalized_x_dim\n",
    "#             else:\n",
    "#                 data.normalized_x = torch.cat((data.normalized_x, normalized_x_dim), dim=1)\n",
    "\n",
    "#     joblib.dump(list_of_scalers_to_save, (directory_path + 'x_scaler.pkl'))\n",
    "#     for data in dataset:\n",
    "#         data.x = data.normalized_x\n",
    "#         del data.normalized_x\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "# def normalize_positional_features(dataset, directory_path):\n",
    "#     # Initialize 6 StandardScalers for 3 sets of 2 dimensions\n",
    "#     scalers = [[StandardScaler() for _ in range(2)] for _ in range(3)]\n",
    "\n",
    "#     # Standardize the data\n",
    "#     for i in range(3):  # Iterate over the second dimension (3 sets)\n",
    "#         for j in range(2):  # Iterate over the third dimension (2D vectors)\n",
    "#             values = np.vstack([data.pos[:, i, j].numpy() for data in dataset]).reshape(-1, 1)\n",
    "#             # Fit the corresponding scaler on the extracted values\n",
    "#             scalers[i][j].fit(values)\n",
    "#             for data in dataset:\n",
    "#                 transformed = scalers[i][j].transform(data.pos[:, i, j].numpy().reshape(-1, 1)).flatten()\n",
    "#                 # Convert the transformed NumPy array back into a torch tensor\n",
    "#                 data.pos[:, i, j] = torch.tensor(transformed, dtype=torch.float32)\n",
    "#     # Save the scalers using joblib\n",
    "#     for i in range(3):\n",
    "#         for j in range(2):\n",
    "#             # Dump the scalers with meaningful names to differentiate them\n",
    "#             scaler_path = directory_path + f'scaler_pos_{i}_{j}.pkl'\n",
    "#             joblib.dump(scalers[i][j], scaler_path)\n",
    "\n",
    "#     print(\"Postional scalers saved and dataset standardized.\")\n",
    "#     return dataset\n",
    "\n",
    "\n",
    "\n",
    "# working version, but only up to 2000 datapoints\n",
    "# def get_combined_data(dataset_path, max_batches=None):\n",
    "#     data_list = []\n",
    "#     batch_num = 1\n",
    "#     while max_batches is None or batch_num <= max_batches:\n",
    "#         try:\n",
    "#             batch_file = os.path.join(dataset_path, f'datalist_batch_{batch_num}.pt')\n",
    "#             batch_data = torch.load(batch_file, map_location='cpu')\n",
    "#             print(f\"Batch {batch_num} type: {type(batch_data)}, length: {len(batch_data)}\")\n",
    "            \n",
    "#             if isinstance(batch_data, list):\n",
    "#                 for idx, item in enumerate(batch_data):\n",
    "#                     try:\n",
    "#                         # print(f\"Item {idx} type: {type(item)}\")\n",
    "#                         if isinstance(item, Data):\n",
    "#                             required_attrs = ['x', 'edge_index', 'pos', 'y', 'mode_stats']\n",
    "#                             missing_attrs = [attr for attr in required_attrs if not hasattr(item, attr)]\n",
    "#                             if not missing_attrs:\n",
    "#                                 data_list.append(item)\n",
    "#                                 # print(f\"Added item {idx} to data_list\")\n",
    "#                             else:\n",
    "#                                 print(f\"Skipping invalid item {idx} in batch {batch_num}. Missing attributes: {missing_attrs}\")\n",
    "#                         else:\n",
    "#                             print(f\"Skipping non-Data item {idx} in batch {batch_num}.\")\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error processing item {idx} in batch {batch_num}: {str(e)}\")\n",
    "#             else:\n",
    "#                 print(f\"Unexpected batch data type in batch {batch_num}: {type(batch_data)}\")\n",
    "            \n",
    "#             batch_num += 1\n",
    "#             print(f\"Loaded batch {batch_num-1}, current total: {len(data_list)} items\")\n",
    "            \n",
    "#             if len(data_list) % 1000 == 0:\n",
    "#                 if psutil.virtual_memory().percent > 90:\n",
    "#                     print(\"Memory usage high, stopping data loading\")\n",
    "#                     break\n",
    "#         except FileNotFoundError:\n",
    "#             print(f\"Finished loading {batch_num-1} batches\")\n",
    "#             break\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading batch {batch_num}: {str(e)}\")\n",
    "#             batch_num += 1\n",
    "\n",
    "#     print(f\"Successfully loaded {len(data_list)} data points\")\n",
    "#     return data_list\n",
    "\n",
    "# def get_combined_data(dataset_path, max_data_points=10000):\n",
    "#     data_list = []\n",
    "#     batch_num = 1\n",
    "#     while len(data_list) < max_data_points:\n",
    "#         try:\n",
    "#             batch_file = os.path.join(dataset_path, f'datalist_batch_{batch_num}.pt')\n",
    "#             batch_data = torch.load(batch_file, map_location='cpu')\n",
    "#             print(f\"Batch {batch_num} type: {type(batch_data)}, length: {len(batch_data)}\")\n",
    "            \n",
    "#             if isinstance(batch_data, list):\n",
    "#                 for idx, item in enumerate(batch_data):\n",
    "#                     if len(data_list) >= max_data_points:\n",
    "#                         break\n",
    "#                     try:\n",
    "#                         if isinstance(item, Data):\n",
    "#                             required_attrs = ['x', 'edge_index', 'pos', 'y', 'mode_stats']\n",
    "#                             missing_attrs = [attr for attr in required_attrs if not hasattr(item, attr)]\n",
    "#                             if not missing_attrs:\n",
    "#                                 data_list.append(item)\n",
    "#                             else:\n",
    "#                                 print(f\"Skipping invalid item {idx} in batch {batch_num}. Missing attributes: {missing_attrs}\")\n",
    "#                         else:\n",
    "#                             print(f\"Skipping non-Data item {idx} in batch {batch_num}.\")\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error processing item {idx} in batch {batch_num}: {str(e)}\")\n",
    "#             else:\n",
    "#                 print(f\"Unexpected batch data type in batch {batch_num}: {type(batch_data)}\")\n",
    "            \n",
    "#             batch_num += 1\n",
    "#             print(f\"Loaded batch {batch_num-1}, current total: {len(data_list)} items\")\n",
    "            \n",
    "#             if psutil.virtual_memory().percent > 90:\n",
    "#                 print(\"Memory usage high, stopping data loading\")\n",
    "#                 break\n",
    "#         except FileNotFoundError:\n",
    "#             print(f\"Finished loading {batch_num-1} batches\")\n",
    "#             break\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading batch {batch_num}: {str(e)}\")\n",
    "#             batch_num += 1\n",
    "\n",
    "#     print(f\"Successfully loaded {len(data_list)} data points\")\n",
    "#     return data_list\n",
    "\n",
    "# # Usage\n",
    "# try:\n",
    "#     dataset_path = '../../data/train_data/sim_output_1pm_capacity_reduction_10k_11_10_2024/'\n",
    "#     data_list = get_combined_data(dataset_path)  # Let's look at the first two batches\n",
    "#     print(f\"Final count: Successfully loaded {len(data_list)} data points\")\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "# def normalize_x_values(dataset, directory_path):\n",
    "#     try:\n",
    "#         shape_of_x = dataset[0].x.shape[1]\n",
    "#         print(f\"Shape of x: {shape_of_x}\")\n",
    "        \n",
    "#         list_of_scalers_to_save = []\n",
    "#         print(\"Concatenating x values...\")\n",
    "#         x_values = torch.cat([data.x for data in dataset], dim=0)\n",
    "#         print(f\"Concatenated x_values shape: {x_values.shape}\")\n",
    "\n",
    "#         for i in range(shape_of_x):\n",
    "#             print(f\"Processing feature {i}/{shape_of_x}\")\n",
    "#             all_node_features = replace_invalid_values(x_values[:, i].reshape(-1, 1)).numpy()\n",
    "            \n",
    "#             scaler = StandardScaler()\n",
    "#             scaler.fit(all_node_features)\n",
    "#             list_of_scalers_to_save.append(scaler)\n",
    "\n",
    "#             for j, data in enumerate(dataset):\n",
    "#                 if j % 100 == 0:\n",
    "#                     print(f\"Processing data point {j}/{len(dataset)}\")\n",
    "#                 data_x_dim = replace_invalid_values(data.x[:, i].reshape(-1, 1))\n",
    "#                 normalized_x_dim = torch.tensor(scaler.transform(data_x_dim.numpy()), dtype=torch.float)\n",
    "#                 if i == 0:\n",
    "#                     data.normalized_x = normalized_x_dim\n",
    "#                 else:\n",
    "#                     data.normalized_x = torch.cat((data.normalized_x, normalized_x_dim), dim=1)\n",
    "\n",
    "#         joblib.dump(list_of_scalers_to_save, (directory_path + 'x_scaler.pkl'))\n",
    "#         for data in dataset:\n",
    "#             data.x = data.normalized_x\n",
    "#             del data.normalized_x\n",
    "#         return dataset\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in normalize_x_values: {str(e)}\")\n",
    "#         traceback.print_exc()\n",
    "#         raise\n",
    "\n",
    "# def normalize_positional_features(dataset, directory_path):\n",
    "#     try:\n",
    "#         shape_of_pos = dataset[0].pos.shape\n",
    "#         print(f\"Shape of pos: {shape_of_pos}\")\n",
    "        \n",
    "#         list_of_scalers_to_save = []\n",
    "#         print(\"Concatenating positional values...\")\n",
    "#         pos_values = torch.cat([data.pos.reshape(data.pos.shape[0], -1) for data in dataset], dim=0)\n",
    "#         print(f\"Concatenated pos_values shape: {pos_values.shape}\")\n",
    "\n",
    "#         for i in range(pos_values.shape[1]):\n",
    "#             print(f\"Processing positional feature {i}/{pos_values.shape[1]}\")\n",
    "#             all_pos_features = replace_invalid_values(pos_values[:, i].reshape(-1, 1)).numpy()\n",
    "            \n",
    "#             scaler = StandardScaler()\n",
    "#             scaler.fit(all_pos_features)\n",
    "#             list_of_scalers_to_save.append(scaler)\n",
    "\n",
    "#             for j, data in enumerate(dataset):\n",
    "#                 if j % 100 == 0:\n",
    "#                     print(f\"Processing data point {j}/{len(dataset)}\")\n",
    "#                 data_pos_dim = replace_invalid_values(data.pos.reshape(data.pos.shape[0], -1)[:, i].reshape(-1, 1))\n",
    "#                 normalized_pos_dim = torch.tensor(scaler.transform(data_pos_dim.numpy()), dtype=torch.float)\n",
    "#                 if i == 0:\n",
    "#                     data.normalized_pos = normalized_pos_dim\n",
    "#                 else:\n",
    "#                     data.normalized_pos = torch.cat((data.normalized_pos, normalized_pos_dim), dim=1)\n",
    "\n",
    "#         print(\"Saving positional scalers...\")\n",
    "#         joblib.dump(list_of_scalers_to_save, (directory_path + 'pos_scaler.pkl'))\n",
    "#         print(\"Positional scalers saved successfully\")\n",
    "\n",
    "#         print(\"Updating pos values in dataset...\")\n",
    "#         for data in dataset:\n",
    "#             data.pos = data.normalized_pos.reshape(shape_of_pos)\n",
    "#             del data.normalized_pos\n",
    "#         print(\"Dataset pos values updated\")\n",
    "\n",
    "#         return dataset\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in normalize_positional_features: {str(e)}\")\n",
    "#         traceback.print_exc()\n",
    "#         raise\n",
    "\n",
    "# def normalize_mode_stats(dataset, directory_path):\n",
    "#     try:\n",
    "#         print(\"Starting mode stats normalization...\")\n",
    "#         # Initialize 12 StandardScalers for 6 sets of 2 dimensions\n",
    "#         scalers = [[StandardScaler() for _ in range(2)] for _ in range(6)]\n",
    "\n",
    "#         # Standardize the data\n",
    "#         for i in range(6):  # Iterate over the first dimension (6 sets)\n",
    "#             for j in range(2):  # Iterate over the second dimension (2D vectors)\n",
    "#                 print(f\"Processing mode stats dimension {i}, {j}\")\n",
    "#                 values = np.vstack([replace_invalid_values(data.mode_stats[i, j].reshape(-1, 1)) for data in dataset])\n",
    "#                 print(f\"Collected values shape: {values.shape}\")\n",
    "                \n",
    "#                 # Fit the corresponding scaler on the extracted values\n",
    "#                 scalers[i][j].fit(values)\n",
    "                \n",
    "#                 for k, data in enumerate(dataset):\n",
    "#                     if k % 100 == 0:\n",
    "#                         print(f\"Transforming data point {k}/{len(dataset)} for dimension {i}, {j}\")\n",
    "#                     data_mode_stats_dim = replace_invalid_values(data.mode_stats[i, j].reshape(-1, 1))\n",
    "#                     transformed = scalers[i][j].transform(data_mode_stats_dim).flatten()\n",
    "#                     # Convert the transformed NumPy array back into a torch tensor\n",
    "#                     data.mode_stats[i, j] = torch.tensor(transformed, dtype=torch.float32)\n",
    "\n",
    "#         print(\"Saving mode stats scalers...\")\n",
    "#         # Save the scalers using joblib\n",
    "#         for i in range(6):\n",
    "#             for j in range(2):\n",
    "#                 # Dump the scalers with meaningful names to differentiate them\n",
    "#                 scaler_path = directory_path + f'scaler_mode_stats_{i}_{j}.pkl'\n",
    "#                 joblib.dump(scalers[i][j], scaler_path)\n",
    "\n",
    "#         print(\"Mode stats scalers saved and dataset standardized.\")\n",
    "#         return dataset\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in normalize_mode_stats: {str(e)}\")\n",
    "#         traceback.print_exc()\n",
    "#         raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenhao-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

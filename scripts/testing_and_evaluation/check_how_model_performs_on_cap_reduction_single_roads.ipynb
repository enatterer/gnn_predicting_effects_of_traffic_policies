{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ABSTRACT -->\n",
    "\n",
    "The goal of this script is to check how well the model performs on the test set. For this, we will look at the overall test set, as well as some specific cases, that we will visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import joblib\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "import help_functions as hf\n",
    "import pandas as pd\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from gnn_architecture import MyGnn\n",
    "import gnn_io as gio\n",
    "import help_functions as hf\n",
    "from training.help_functions import normalize_dataset, seed_worker, normalize_x_features_with_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "run_path = '../../data/runs_01_2025/1pct_10k_shuffle/'\n",
    "districts = gpd.read_file(\"../../data/visualisation/districts_paris.geojson\")\n",
    "base_case_path = '../../data/links_and_stats/pop_1pct_basecase_average_output_links.geojson'\n",
    "result_path = './results/'\n",
    "\n",
    "# Parameters\n",
    "point_net_conv_layer_structure_local_mlp = \"256\"\n",
    "point_net_conv_layer_structure_global_mlp = \"512\"\n",
    "gat_conv_layer_structure = \"128,256,512,256\" \n",
    "dropout = 0.3\n",
    "use_dropout = False\n",
    "predict_mode_stats = False\n",
    "in_channels = 5\n",
    "out_channels = 1\n",
    "\n",
    "links_base_case = gpd.read_file(base_case_path, crs=\"EPSG:4326\")\n",
    "data_created_during_training = run_path + 'data_created_during_training/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "### Load test data from the run itself! ###\n",
    "###########################################\n",
    "\n",
    "# # Load scalers\n",
    "# scaler_x = joblib.load(data_created_during_training + 'test_x_scaler.pkl')\n",
    "# scaler_pos = joblib.load(data_created_during_training + 'test_pos_scaler.pkl')\n",
    "\n",
    "# # Load the test dataset created during training\n",
    "# test_set_dl = torch.load(data_created_during_training + 'test_dl.pt')\n",
    "\n",
    "# # Load the DataLoader parameters\n",
    "# with open(data_created_during_training + 'test_loader_params.json', 'r') as f:\n",
    "#     test_set_dl_loader_params = json.load(f)\n",
    "    \n",
    "# # Remove or correct collate_fn if it is incorrectly specified\n",
    "# if 'collate_fn' in test_set_dl_loader_params and isinstance(test_set_dl_loader_params['collate_fn'], str):\n",
    "#     del test_set_dl_loader_params['collate_fn']  # Remove it to use the default collate function\n",
    "    \n",
    "# test_set_loader = torch.utils.data.DataLoader(test_set_dl, **test_set_dl_loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "### Load separate test data! ###\n",
    "################################\n",
    "\n",
    "# dataset_path = \"../../data/test_data/general_pst_roads/\"\n",
    "dataset_path = \"../../data/test_data/pst_roads_in_inner_districts/\"\n",
    "\n",
    "datalist = []\n",
    "batch_num = 1\n",
    "while True:\n",
    "    print(f\"Processing batch number: {batch_num}\")\n",
    "    batch_file = os.path.join(dataset_path, f'datalist_batch_{batch_num}.pt')\n",
    "    if not os.path.exists(batch_file):\n",
    "        break\n",
    "    batch_data = torch.load(batch_file, map_location='cpu')\n",
    "    if isinstance(batch_data, list):\n",
    "        datalist.extend(batch_data)\n",
    "    batch_num += 1\n",
    "print(f\"Loaded {len(datalist)} items into datalist\")\n",
    "\n",
    "dataset_length = len(datalist)\n",
    "test_indices = range(dataset_length)\n",
    "test_subset = Subset(datalist, test_indices)\n",
    "\n",
    "node_features = [\"VOL_BASE_CASE\",\n",
    "                 \"CAPACITY_BASE_CASE\",\n",
    "                 \"CAPACITY_REDUCTION\",\n",
    "                 \"FREESPEED\",\n",
    "                 \"LENGTH\"]\n",
    "\n",
    "### Use a new scaler! ###\n",
    "# test_set_normalized, scalers_test = normalize_dataset(dataset_input=test_subset, node_features=node_features)\n",
    "# test_set_loader = DataLoader(dataset=test_set_normalized, batch_size=8,\n",
    "#                              shuffle=True, num_workers=4, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "\n",
    "# scaler_x = scalers_test['x_scaler']\n",
    "# scaler_pos = scalers_test['pos_scaler']\n",
    "##########################\n",
    "\n",
    "### Load scalers for consistency! ###\n",
    "scaler_x = joblib.load(data_created_during_training + 'test_x_scaler.pkl')\n",
    "data_list = [copy.deepcopy(test_subset.dataset[idx]) for idx in test_subset.indices]\n",
    "test_set_normalized = normalize_x_features_with_scaler(data_list, node_features, scaler_x)\n",
    "test_set_loader = DataLoader(dataset=test_set_normalized, batch_size=8,\n",
    "                             shuffle=True, num_workers=4, collate_fn=gio.collate_fn, worker_init_fn=seed_worker)\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_net_conv_layer_structure_local_mlp = [int(x) for x in point_net_conv_layer_structure_local_mlp.split(',')]\n",
    "point_net_conv_layer_structure_global_mlp = [int(x) for x in point_net_conv_layer_structure_global_mlp.split(',')]\n",
    "gat_conv_layer_structure = [int(x) for x in gat_conv_layer_structure.split(',')]\n",
    "\n",
    "model = MyGnn(in_channels=in_channels, out_channels=out_channels,\n",
    "              point_net_conv_layer_structure_local_mlp=point_net_conv_layer_structure_local_mlp, \n",
    "              point_net_conv_layer_structure_global_mlp = point_net_conv_layer_structure_global_mlp,\n",
    "              gat_conv_layer_structure=gat_conv_layer_structure,\n",
    "              dropout=dropout,\n",
    "              use_dropout=use_dropout,\n",
    "              predict_mode_stats=predict_mode_stats)\n",
    "\n",
    "# Load the model state dictionary\n",
    "model_path = run_path + 'trained_model/model.pth'\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fct = torch.nn.MSELoss().to(dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, r_squared, actual_vals, predictions, baseline_loss = hf.validate_model_on_test_set(model, test_set_loader.dataset, loss_fct, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"Baseline Loss: {baseline_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next, we will look at single elements of the test set and visualize the performance of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_norm_max = 50\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    if i > 0:\n",
    "        continue\n",
    "    \n",
    "    my_test_data = test_set_loader.dataset[i]\n",
    "    my_test_x = test_set_loader.dataset[i].x\n",
    "    my_test_x = my_test_x.to('cpu')\n",
    "    \n",
    "    test_loss_my_test_data, r_squared_my_test_data, actual_vals_my_test_data, predictions_my_test_data, baseline_loss_my_test_data = hf.validate_model_on_test_set(model, my_test_data, loss_fct, device)\n",
    "    print(f\"Test {i}\")\n",
    "    print(f\"Test Loss: {test_loss_my_test_data}\")\n",
    "    print(f\"R-squared: {r_squared_my_test_data}\")\n",
    "    print(f\"Baseline Loss: {baseline_loss_my_test_data}\")\n",
    "\n",
    "    inversed_x = scaler_x.inverse_transform(my_test_x)\n",
    "\n",
    "    gdf_with_og_values = hf.data_to_geodataframe_with_og_values(data=my_test_data, original_gdf=links_base_case, predicted_values=predictions_my_test_data, inversed_x=inversed_x)\n",
    "    gdf_with_og_values['capacity_reduction_rounded'] = gdf_with_og_values['capacity_reduction'].round(decimals=3)\n",
    "    gdf_with_og_values['highway'] = gdf_with_og_values['highway'].map(hf.highway_mapping)\n",
    "\n",
    "    hf.plot_combined_output(gdf_input=gdf_with_og_values, column_to_plot=\"vol_car_change_predicted\", \n",
    "                            save_it=True, number_to_plot=i, zone_to_plot = \"this zone\", is_predicted=True, alpha=0, use_fixed_norm=False, \n",
    "                            fixed_norm_max = fixed_norm_max,\n",
    "                            known_districts = False, buffer = 0.0005, districts_of_interest=None,\n",
    "                            plot_contour_lines=False, plot_policy_roads=True, result_path=result_path)\n",
    "    \n",
    "    hf.plot_combined_output(gdf_input=gdf_with_og_values, column_to_plot=\"vol_car_change_actual\", save_it=True, \n",
    "                            number_to_plot=i, zone_to_plot = \"this zone\",is_predicted=False,alpha=10,use_fixed_norm=False, \n",
    "                            fixed_norm_max = fixed_norm_max,\n",
    "                            known_districts = False, buffer = 0.0005, districts_of_interest=None,\n",
    "                            plot_contour_lines=False, plot_policy_roads=True, result_path=result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_with_og_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalize the creation of gdfs for a range of indices\n",
    "gdfs = []\n",
    "for i in range(len(test_set_loader.dataset)):\n",
    "    my_test_data = test_set_loader.dataset[i]\n",
    "    my_test_x = test_set_loader.dataset[i].x\n",
    "    my_test_x = my_test_x.to('cpu')\n",
    "\n",
    "    test_loss_my_test_data, r_squared_my_test_data, actual_vals_my_test_data, predictions_my_test_data, baseline_loss_my_test_data = hf.validate_model_on_test_set(model, my_test_data, loss_fct, device)\n",
    "    \n",
    "    inversed_x = scaler_x.inverse_transform(my_test_x)\n",
    "\n",
    "    gdf = hf.data_to_geodataframe_with_og_values(data=my_test_data, original_gdf=links_base_case, predicted_values=predictions_my_test_data, inversed_x=inversed_x)\n",
    "    gdf['capacity_reduction_rounded'] = gdf['capacity_reduction'].round(decimals=3)\n",
    "    gdf['highway'] = gdf['highway'].map(hf.highway_mapping)\n",
    "    \n",
    "    gdfs.append(gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For absolute differences in number of vehicles\n",
    "\n",
    "# discrete_thresholds=(20, 100)\n",
    "# discrete_thresholds=(20, 40, 60, 80, 100)\n",
    "discrete_thresholds=(10, 20, 30, 40, 50, 60, 70, 80, 90, 100)\n",
    "\n",
    "result_gdf = hf.plot_average_prediction_differences(\n",
    "    gdf_inputs=gdfs,\n",
    "    scale_type=\"discrete\",\n",
    "    discrete_thresholds=discrete_thresholds,\n",
    "    save_it=True,\n",
    "    use_fixed_norm=True,\n",
    "    fixed_norm_max=100,\n",
    "    use_absolute_value_of_difference=True,\n",
    "    use_percentage=True,\n",
    "    disagreement_threshold=None,\n",
    "    result_path=result_path,\n",
    "    loss_fct=\"l1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots in percentage\n",
    "\n",
    "# fixed_norm_max=100\n",
    "fixed_norm_max=10\n",
    "\n",
    "result_gdf = hf.plot_average_prediction_differences(\n",
    "    gdf_inputs=gdfs,\n",
    "    use_fixed_norm=True,\n",
    "    fixed_norm_max=fixed_norm_max,\n",
    "    use_absolute_value_of_difference=True,\n",
    "    use_percentage=True,\n",
    "    disagreement_threshold=0.01,\n",
    "    result_path=result_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_roads_with_highway_primary_sec_tertiary = gdf_with_og_values[gdf_with_og_values['highway'].isin([1,2,3])].index\n",
    "indices_roads_with_highway_primary_ = gdf_with_og_values[gdf_with_og_values['highway'].isin([1])].index\n",
    "indices_roads_with_highway_sec = gdf_with_og_values[gdf_with_og_values['highway'].isin([2])].index\n",
    "indices_roads_with_highway_tertiary = gdf_with_og_values[gdf_with_og_values['highway'].isin([3])].index\n",
    "indices_roads_with_highway_not_primary_sec_tertiary = gdf_with_og_values[~gdf_with_og_values['highway'].isin([1, 2, 3])].index\n",
    "\n",
    "tolerance = 1e-3\n",
    "indices_roads_with_cap_reduction = gdf_with_og_values[gdf_with_og_values['capacity_reduction_rounded'] < -1e-3].index\n",
    "indices_roads_with_no_cap_reduction = gdf_with_og_values[gdf_with_og_values['capacity_reduction_rounded'] >= -1e-3].index\n",
    "\n",
    "indices_roads_with_highway_primary_sec_tertiary_and_cap_reduction = gdf_with_og_values[\n",
    "    (gdf_with_og_values['highway'].isin([1, 2, 3])) & \n",
    "    (gdf_with_og_values['capacity_reduction_rounded'] < -1e-3)].index\n",
    "\n",
    "indices_roads_with_highway_primary_sec_tertiary_and_not_cap_reduction = gdf_with_og_values[\n",
    "    (gdf_with_og_values['highway'].isin([1, 2, 3])) & \n",
    "    (gdf_with_og_values['capacity_reduction_rounded'] >= -1e-3)].index\n",
    "\n",
    "indices_roads_with_highway_trunk = gdf_with_og_values[gdf_with_og_values['highway'].isin([0])].index\n",
    "indices_roads_with_highway_residential_street = gdf_with_og_values[gdf_with_og_values['highway'].isin([4])].index\n",
    "indices_roads_with_highway_living_street = gdf_with_og_values[gdf_with_og_values['highway'].isin([5])].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def compute_correlations_scipy(predictions, targets):\n",
    "    \"\"\"\n",
    "    Compute correlations using scipy (for verification)\n",
    "    \"\"\"\n",
    "    pred_np = predictions.detach().cpu().numpy()\n",
    "    target_np = targets.detach().cpu().numpy()\n",
    "    \n",
    "    spearman_corr, _ = stats.spearmanr(pred_np, target_np)\n",
    "    pearson_corr, _ = stats.pearsonr(pred_np, target_np)\n",
    "    \n",
    "    return spearman_corr, pearson_corr\n",
    "\n",
    "def validate_model_with_interpretable_error(indices, gdf, loss_fct, tolerance):\n",
    "    loss_fct_l1 = torch.nn.L1Loss()\n",
    "    base_car_vol = gdf.loc[indices, 'vol_base_case']\n",
    "    actual_vals = gdf.loc[indices, 'vol_car_change_actual']\n",
    "    predicted_vals = gdf.loc[indices, 'vol_car_change_predicted']\n",
    "    \n",
    "    actual_vals = actual_vals.to_numpy()\n",
    "    predicted_vals = predicted_vals.to_numpy()\n",
    "    actual_mean = torch.mean(torch.tensor(actual_vals))\n",
    "    \n",
    "    baseline_vals = torch.full_like(torch.tensor(actual_vals), actual_mean)\n",
    "    r_squared = hf.compute_r2_torch(preds=torch.tensor(predicted_vals), targets=torch.tensor(actual_vals))\n",
    "    r_squared = round(r_squared.item(), 2)\n",
    "    \n",
    "    baseline_vals_np = baseline_vals.numpy()\n",
    "    base_car_vol_np = base_car_vol.to_numpy()\n",
    "    \n",
    "    baseline_car_vol = base_car_vol_np + baseline_vals_np\n",
    "    actual_car_vol = base_car_vol + actual_vals\n",
    "    predicted_car_vol = base_car_vol + predicted_vals\n",
    "    \n",
    "    mse_loss = loss_fct(torch.tensor(actual_vals), torch.tensor(predicted_vals))\n",
    "    l1_loss = loss_fct_l1(torch.tensor(actual_vals), torch.tensor(predicted_vals))\n",
    "\n",
    "    baseline_mse = loss_fct(torch.tensor(actual_vals), torch.full_like(torch.tensor(actual_vals), actual_mean))\n",
    "    baseline_l1 = loss_fct_l1(torch.tensor(actual_vals), torch.full_like(torch.tensor(actual_vals), actual_mean))\n",
    "    \n",
    "    error_normalized_by_mean_squared = mse_loss /   torch.mean(torch.tensor(actual_vals)).pow(2)\n",
    "    baseline_normalized_by_mean_squared = baseline_mse / torch.mean(torch.tensor(actual_vals)).pow(2)\n",
    "    \n",
    "    variance_actual_car_vol = torch.var(torch.tensor(actual_vals))\n",
    "    error_normalized_by_variance = mse_loss / variance_actual_car_vol\n",
    "    baseline_normalized_by_variance = baseline_mse / variance_actual_car_vol\n",
    "    \n",
    "    spearman_corr, pearson_corr = compute_correlations_scipy(torch.tensor(actual_vals), torch.tensor(predicted_vals))\n",
    "    \n",
    "    print(f\"Spearman Correlation: {spearman_corr:.4f}\")\n",
    "    print(f\"Pearson Correlation: {pearson_corr:.4f}\")\n",
    "    mean_relative_errors, mean_filtered_relative_errors = compute_relative_error_and_relative_filtered_error(actual_car_vol, predicted_car_vol, tolerance)\n",
    "    mean_relative_errors_baseline, mean_filtered_relative_errors_baseline = compute_relative_error_and_relative_filtered_error(actual_car_vol, baseline_car_vol, tolerance)\n",
    "    \n",
    "    print(f\"R-squared: {r_squared}\")\n",
    "    print(f\"MSE Loss: {mse_loss}\")\n",
    "    print(f\"Baseline Loss: {baseline_mse}\")\n",
    "    print(f\"L1 Loss: {l1_loss}\")\n",
    "    print(f\"Baseline L1 loss: {baseline_l1}\")\n",
    "    print(f\"Error normalized by mean squared: {error_normalized_by_mean_squared:.4f}\")\n",
    "    print(f\"Baseline normalized by mean squared: {baseline_normalized_by_mean_squared:.4f}\")\n",
    "    print(f\"Error normalized by variance: {error_normalized_by_variance:.4f}\")\n",
    "    print(f\"Baseline normalized by variance: {baseline_normalized_by_variance:.4f}\")\n",
    "    print(f\"Mean Relative Error: {mean_relative_errors:.4f}\")\n",
    "    print(f\"Mean Filtered Relative Error: {mean_filtered_relative_errors:.4f}\")\n",
    "    print(f\"Baseline Mean Relative Error: {mean_relative_errors_baseline:.4f}\")\n",
    "    print(f\"Baseline Mean Filtered Relative Error: {mean_filtered_relative_errors_baseline:.4f}\")\n",
    "    print(\" \")\n",
    "    \n",
    "    return\n",
    "\n",
    "def compute_relative_error_and_relative_filtered_error(actual_car_vol, car_vol_to_compare, tolerance):\n",
    "    actual_car_vol[actual_car_vol == 0] = 1e-10\n",
    "    absolute_errors = torch.abs(torch.tensor(car_vol_to_compare) - torch.tensor(actual_car_vol))\n",
    "    relative_errors = absolute_errors / torch.tensor(actual_car_vol)\n",
    "    filtered_relative_errors = relative_errors[(relative_errors <= tolerance) & (relative_errors >= -tolerance)]\n",
    "    mean_relative_errors = torch.mean(relative_errors)\n",
    "    mean_filtered_relative_errors = torch.mean(filtered_relative_errors)\n",
    "    return mean_relative_errors,mean_filtered_relative_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All Roads\")\n",
    "validate_model_with_interpretable_error(gdf_with_og_values.index, gdf_with_og_values, loss_fct, 10000)\n",
    "\n",
    "print(\"Trunk Roads\")\n",
    "validate_model_with_interpretable_error(pd.RangeIndex(start=0, stop=len(indices_roads_with_highway_trunk), step=1), gdf_with_og_values, loss_fct, 10000)\n",
    "\n",
    "print(\"Residential Streets\")\n",
    "validate_model_with_interpretable_error(pd.RangeIndex(start=0, stop=len(indices_roads_with_highway_residential_street), step=1), gdf_with_og_values, loss_fct, 10000)\n",
    "\n",
    "print(\"Living Streets\")\n",
    "validate_model_with_interpretable_error(pd.RangeIndex(start=0, stop=len(indices_roads_with_highway_living_street), step=1), gdf_with_og_values, loss_fct, 10000)\n",
    "\n",
    "print(\"Primary Roads\")\n",
    "validate_model_with_interpretable_error(pd.RangeIndex(start=0, stop=len(indices_roads_with_highway_primary_), step=1), gdf_with_og_values, loss_fct, 10000)\n",
    "print(\"Secondary Roads\")\n",
    "validate_model_with_interpretable_error(pd.RangeIndex(start=0, stop=len(indices_roads_with_highway_sec), step=1), gdf_with_og_values, loss_fct, 10000)\n",
    "print(\"Tertiary Roads\")\n",
    "validate_model_with_interpretable_error(pd.RangeIndex(start=0, stop=len(indices_roads_with_highway_tertiary), step=1), gdf_with_og_values, loss_fct, 10000)\n",
    "# print(\"Primary, Secondary, Tertiary Roads\")\n",
    "# validate_model_with_interpretable_error(pd.RangeIndex(start=0, stop=len(indices_roads_with_highway_primary_sec_tertiary), step=1), gdf_with_og_values, loss_fct, 10000)\n",
    "\n",
    "# print(\"Non-Primary, Non-Secondary, Non-Tertiary Roads\")\n",
    "# validate_model_with_interpretable_error(pd.RangeIndex(start=0, stop=len(indices_roads_with_highway_not_primary_sec_tertiary), step=1), gdf_with_og_values, loss_fct, 10000)\n",
    "# print(\"Capacity Reduction Roads\")\n",
    "# validate_model_with_interpretable_error(pd.RangeIndex(start=0, stop=len(indices_roads_with_cap_reduction), step=1), gdf_with_og_values, loss_fct, 10000)\n",
    "# print(\"No Capacity Reduction Roads\")\n",
    "# validate_model_with_interpretable_error(pd.RangeIndex(start=0, stop=len(indices_roads_with_no_cap_reduction), step=1), gdf_with_og_values, loss_fct, 10000)\n",
    "# print(\"Primary, Secondary, Tertiary Roads with Capacity Reduction\")\n",
    "# validate_model_with_interpretable_error(pd.RangeIndex(start=0, stop=len(indices_roads_with_highway_primary_sec_tertiary_and_cap_reduction), step=1), gdf_with_og_values, loss_fct, 10000)\n",
    "# print(\"Primary, Secondary, Tertiary Roads with No Capacity Reduction\")\n",
    "# validate_model_with_interpretable_error(pd.RangeIndex(start=0, stop=len(indices_roads_with_highway_primary_sec_tertiary_and_not_cap_reduction), step=1), gdf_with_og_values, loss_fct, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the GeoDataFrame to the appropriate coordinate reference system (CRS) for length calculation\n",
    "gdf_in_meters = gdf_with_og_values.to_crs(\"EPSG:32633\")\n",
    "gdf_in_meters['length'] = gdf_in_meters.length\n",
    "total_length = gdf_in_meters['length'].sum() / 1000\n",
    "print(f\"Total length of the street network: {total_length:.2f} km\")\n",
    "gdf_with_reductions = gdf_in_meters.loc[indices_roads_with_cap_reduction]\n",
    "total_length_with_reductions = gdf_with_reductions['length'].sum() / 1000\n",
    "print(f\"Total length of the street network with capacity reductions: {total_length_with_reductions:.2f} km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for the plot\n",
    "road_types = [\n",
    "    \"Trunk Roads\",\n",
    "    \"Primary Roads\",\n",
    "    \"Secondary Roads\",\n",
    "    \"Tertiary Roads\",\n",
    "    \"Residential Streets\",\n",
    "    \"Living Streets\",\n",
    "]\n",
    "\n",
    "rmse = [5.16, 5.64, 5.46, 5.58, 5.40, 4.65]\n",
    "std_dev = [5.12, 3.50, 2.85, 2.65, 1.46, 0.72]\n",
    "naive_rmse = [10.52, 11.81, 10.47, 10.71, 12.10, 11.13]\n",
    "mean_car_volume = [1060.73, 117.58, 51.46, 35.37, 13.27, 4.81]\n",
    "\n",
    "x = np.arange(len(road_types))  # x locations for the bars\n",
    "\n",
    "# Set font to Times New Roman\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "\n",
    "# Plot for RMSE with Mean Car Volume overlay\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Bar plot for RMSE, Std. Dev., and Naive RMSE\n",
    "bar_width = 0.2\n",
    "ax1.bar(x - bar_width, rmse, width=bar_width, label=\"RMSE\", color=\"blue\")\n",
    "ax1.bar(x, std_dev, width=bar_width, label=\"Standard Deviation\", color=\"green\")\n",
    "ax1.bar(x + bar_width, naive_rmse, width=bar_width, label=\"Naive RMSE\", color=\"orange\")\n",
    "ax1.set_ylabel(\"RMSE\", fontsize=12)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(road_types, rotation=45, ha=\"right\")\n",
    "ax1.legend(loc=\"upper left\", fontsize=10, bbox_to_anchor=(0, 1.2))  # Adjusted position to make the legend box higher\n",
    "\n",
    "# Second y-axis for Mean Car Volume\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(x, mean_car_volume, label=\"Mean Car Volume\", color=\"purple\", marker=\"o\", linestyle=\"dashed\", linewidth=2)\n",
    "ax2.set_ylabel(\"Mean Car Volume\", fontsize=12, color=\"purple\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"purple\")\n",
    "ax2.set_ylim(0, max(mean_car_volume) * 1.1)  # Ensure the secondary y-axis starts at 0\n",
    "ax2.legend(loc=\"upper right\", fontsize=10)\n",
    "ax2.legend(loc=\"upper right\", fontsize=10, bbox_to_anchor=(1, 1.2))  # Adjusted position to make the legend box higher\n",
    "\n",
    "\n",
    "plt.savefig(\"rmse_with_mean_car_volume.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Tight layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenhao-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

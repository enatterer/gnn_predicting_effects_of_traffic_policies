{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import wandb\n",
    "import pickle\n",
    "import os\n",
    "import shapely.wkt as wkt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "from torch_geometric.transforms import LineGraph\n",
    "\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import processing_io as pio\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from shapely.geometry import Point, LineString, box\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "from shapely.ops import unary_union\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from torch_geometric.data import Data, Batch\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import alphashape\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "districts = gpd.read_file(\"../../data/visualisation/districts_paris.geojson\")\n",
    "\n",
    "# Add the 'scripts' directory to the Python path\n",
    "scripts_path = os.path.abspath(os.path.join('..'))\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "\n",
    "import gnn_io as gio\n",
    "import gnn_architectures as garch\n",
    "\n",
    "# Assuming 'highway_mapping' and 'encode_modes' are defined as in your context\n",
    "highway_mapping = {\n",
    "    'residential': 0, 'tertiary': 1, 'living_street': 2, 'secondary': 3, \n",
    "    'primary': 4, 'trunk_link': 5, 'primary_link': 6, 'motorway': 7, \n",
    "    'service': 8, 'unclassified': 9, 'secondary_link': 10, \n",
    "    'pedestrian': 11, 'trunk': 12, 'motorway_link': 13, \n",
    "    'construction': 14, 'tertiary_link': 15, np.nan: -1\n",
    "}\n",
    "\n",
    "def validate_model(model, valid_dl, loss_func, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    actual_vals = []\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for idx, data in enumerate(valid_dl):\n",
    "            input_node_features, targets = data.x.to(device), data.y.to(device)\n",
    "            predicted = model(data.to(device))\n",
    "            actual_vals.append(targets)\n",
    "            predictions.append(predicted)\n",
    "            val_loss += loss_func(predicted, targets).item()\n",
    "            num_batches += 1\n",
    "            \n",
    "    actual_vals_cat = torch.cat(actual_vals)\n",
    "    predictions_cat = torch.cat(predictions)\n",
    "    r_squared = compute_r2_torch(preds=predictions_cat, targets=actual_vals_cat)\n",
    "    return val_loss / num_batches if num_batches > 0 else 0, r_squared, actual_vals, predictions\n",
    "\n",
    "def validate_one_model(model, data, loss_func, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        # for idx, data in enumerate(valid_dl):\n",
    "        input_node_features, targets = data.x.to(device), data.y.to(device)\n",
    "        predicted = model(data.to(device))\n",
    "        val_loss += loss_func(predicted, targets).item()\n",
    "        num_batches += 1\n",
    "    r_squared = compute_r2_torch(preds=predicted, targets=targets)\n",
    "    return val_loss / num_batches if num_batches > 0 else 0, r_squared, targets, predicted\n",
    "\n",
    "def compute_r2_torch(preds, targets):\n",
    "    \"\"\"Compute R^2 score using PyTorch.\"\"\"\n",
    "    mean_targets = torch.mean(targets)\n",
    "    ss_tot = torch.sum((targets - mean_targets) ** 2)\n",
    "    ss_res = torch.sum((targets - preds) ** 2)\n",
    "    r2 = 1 - ss_res / ss_tot\n",
    "    return r2\n",
    "\n",
    "def data_to_geodataframe(data, original_gdf, predicted_values):\n",
    "    # Extract the edge index and node features\n",
    "    node_features = data.x.cpu().numpy()\n",
    "    target_values = data.y.cpu().numpy()\n",
    "    predicted_values = predicted_values.cpu().numpy() if isinstance(predicted_values, torch.Tensor) else predicted_values\n",
    "\n",
    "    # Create edge data\n",
    "    edge_data = {\n",
    "        'from_node': original_gdf[\"from_node\"].values,\n",
    "        'to_node': original_gdf[\"to_node\"].values,\n",
    "        'vol_base_case': node_features[:, 0],  # Assuming capacity is the first feature, and so on\n",
    "        'capacity_base_case': node_features[:, 1],  \n",
    "        'capacity_reduction': node_features[:, 2],  \n",
    "        'highway': node_features[:, 3],  \n",
    "        'vol_car_change_actual': target_values.squeeze(),  # Assuming target values are car volumes\n",
    "        'vol_car_change_predicted': predicted_values.squeeze()\n",
    "    }\n",
    "    # Convert to DataFrame\n",
    "    edge_df = pd.DataFrame(edge_data)\n",
    "    # Create LineString geometry\n",
    "    edge_df['geometry'] = original_gdf[\"geometry\"].values\n",
    "    # Create GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(edge_df, geometry='geometry')\n",
    "    return gdf\n",
    "\n",
    "def map_to_original_values(input_gdf: gpd.GeoDataFrame):\n",
    "    gdf = input_gdf.copy()\n",
    "    original_values_vol_car_change_actual = scaler_y.inverse_transform(gdf['vol_car_change_actual'].values.reshape(-1, 1))\n",
    "    original_values_vol_car_change_predicted = scaler_y.inverse_transform(gdf['vol_car_change_predicted'].values.reshape(-1, 1))\n",
    "    original_values_vol_base_case = scaler_x[0].inverse_transform(gdf['vol_base_case'].values.reshape(-1, 1))\n",
    "    original_values_capacity_base_case = scaler_x[1].inverse_transform(gdf['capacity_base_case'].values.reshape(-1, 1))\n",
    "    original_values_capacity_new = scaler_x[2].inverse_transform(gdf['capacity_reduction'].values.reshape(-1, 1))\n",
    "    original_values_highway = scaler_x[3].inverse_transform(gdf['highway'].values.reshape(-1, 1))\n",
    "\n",
    "    gdf['og_highway'] = original_values_highway\n",
    "    gdf['og_vol_car_change_actual'] = original_values_vol_car_change_actual\n",
    "    gdf['og_vol_car_change_predicted'] = original_values_vol_car_change_predicted\n",
    "    gdf['og_vol_base_case'] = original_values_vol_base_case\n",
    "    gdf['og_capacity_base_case'] = original_values_capacity_base_case\n",
    "    gdf['og_capacity_reduction'] = original_values_capacity_new\n",
    "    return gdf\n",
    "\n",
    "def list_to_string(integers, delimiter=', '):\n",
    "    \"\"\"\n",
    "    Converts a list of integers into a string, with each integer separated by the specified delimiter.\n",
    "\n",
    "    Parameters:\n",
    "    integers (list of int): The list of integers to convert.\n",
    "    delimiter (str): The delimiter to use between integers in the string.\n",
    "\n",
    "    Returns:\n",
    "    str: A string representation of the list of integers.\n",
    "    \"\"\"\n",
    "    return delimiter.join(map(str, integers))\n",
    "\n",
    "def plot_combined_output(gdf_input: gpd.GeoDataFrame, column_to_plot: str, font: str = 'DejaVu Serif', save_it: bool = False, number_to_plot: int = 0, is_predicted: bool = False):\n",
    "    gdf = gdf_input.copy()\n",
    "    x_min = gdf.total_bounds[0] + 0.05\n",
    "    y_min = gdf.total_bounds[1] + 0.05\n",
    "    x_max = gdf.total_bounds[2]\n",
    "    y_max = gdf.total_bounds[3]\n",
    "    bbox = box(x_min, y_min, x_max, y_max)\n",
    "\n",
    "    # Filter the network to include only the data within the bounding box\n",
    "    gdf = gdf[gdf.intersects(bbox)]\n",
    "\n",
    "    # Set up the plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "    gdf = gdf[gdf[\"og_highway\"].isin([1, 2, 3])]\n",
    "\n",
    "    # Round og_capacity_reduction and filter\n",
    "    gdf['og_capacity_reduction_rounded'] = gdf['og_capacity_reduction'].round(decimals=3)\n",
    "    tolerance = 1e-3\n",
    "    edges_with_capacity_reduction = gdf[np.abs(gdf['og_capacity_reduction_rounded']) > tolerance]\n",
    "\n",
    "\n",
    "    norm = TwoSlopeNorm(vmin=-20, vcenter=0, vmax=20)\n",
    "    gdf.plot(ax=ax, column=column_to_plot, cmap='coolwarm', linewidth=4, legend=False, norm=norm, zorder=1, label=\"Higher order street network\")\n",
    "    \n",
    "    # Extract the coordinates of the geometries\n",
    "    coords = [(x, y) for geom in edges_with_capacity_reduction.geometry for x, y in zip(geom.xy[0], geom.xy[1])]\n",
    "\n",
    "    # Compute the alpha shape\n",
    "    alpha = 100  # Adjust this parameter to make the shape tighter or looser\n",
    "    alpha_shape = alphashape.alphashape(coords, alpha)\n",
    "\n",
    "    # Create a GeoSeries for the alpha shape with the same CRS\n",
    "    alpha_shape_gs = gpd.GeoSeries([alpha_shape], crs=gdf.crs)\n",
    "\n",
    "    # Plot the alpha shape\n",
    "    alpha_shape_gs.plot(ax=ax, edgecolor='black', linewidth=2, facecolor='None', zorder=2, label = 'Capacity was decreased in this section')\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel(\"Longitude\", fontname=font, fontsize=15)\n",
    "    plt.ylabel(\"Latitude\", fontname=font, fontsize=15)\n",
    "\n",
    "    # Customize tick labels\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontname(font)\n",
    "        label.set_fontsize(15)\n",
    "    \n",
    "    # Create custom legend\n",
    "    custom_lines = [Line2D([0], [0], color='grey', lw=4, label='Higher order street network'),# Add more lines for other labels as needed\n",
    "                    Line2D([0], [0], color='black', lw=2, label='Capacity was decreased in this section')]\n",
    "\n",
    "    ax.legend(handles=custom_lines, prop={'family': font, 'size': 15})\n",
    "    ax.set_position([0.1, 0.1, 0.75, 0.75])\n",
    "    cax = fig.add_axes([0.87, 0.22, 0.03, 0.5])  # Manually position the color bar\n",
    "    \n",
    "    # Create the color bar\n",
    "    sm = plt.cm.ScalarMappable(cmap='coolwarm', norm=norm)\n",
    "    sm._A = []\n",
    "    cbar = plt.colorbar(sm, cax=cax)\n",
    "\n",
    "    # Set color bar font properties\n",
    "    cbar.ax.tick_params(labelsize=15)\n",
    "    for t in cbar.ax.get_yticklabels():\n",
    "        t.set_fontname(font)\n",
    "    cbar.ax.yaxis.label.set_fontname(font)\n",
    "    cbar.ax.yaxis.label.set_size(15)\n",
    "    cbar.set_label('Car volume: Difference to base case (%)', fontname=font, fontsize=15)\n",
    "    if save_it:\n",
    "        p = \"predicted\" if is_predicted else \"actual\"\n",
    "        plt.savefig(\"results/gdf_\" + str(number_to_plot) + \"_\" + p, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_difference_output(gdf_input: gpd.GeoDataFrame, column1: str, column2: str, diff_column: str = 'difference', font: str = 'DejaVu Serif', save_it: bool = False, number_to_plot: int = 0):\n",
    "    gdf = gdf_input.copy()\n",
    "    \n",
    "    # Calculate the difference between the two columns\n",
    "    gdf[diff_column] = gdf[column1] - gdf[column2]\n",
    "    \n",
    "    x_min = gdf.total_bounds[0] + 0.05\n",
    "    y_min = gdf.total_bounds[1] + 0.05\n",
    "    x_max = gdf.total_bounds[2]\n",
    "    y_max = gdf.total_bounds[3]\n",
    "    bbox = box(x_min, y_min, x_max, y_max)\n",
    "\n",
    "    # Filter the network to include only the data within the bounding box\n",
    "    gdf = gdf[gdf.intersects(bbox)]\n",
    "\n",
    "    # Set up the plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "    gdf = gdf[gdf[\"og_highway\"].isin([1, 2, 3])]\n",
    "\n",
    "    # Round og_capacity_reduction and filter\n",
    "    gdf['og_capacity_reduction_rounded'] = gdf['og_capacity_reduction'].round(decimals=3)\n",
    "    tolerance = 1e-3\n",
    "    edges_with_capacity_reduction = gdf[np.abs(gdf['og_capacity_reduction_rounded']) > tolerance]\n",
    "    \n",
    "    norm = TwoSlopeNorm(vmin=gdf[diff_column].min(), vcenter=gdf[diff_column].median(), vmax=gdf[diff_column].max())\n",
    "\n",
    "    # norm = TwoSlopeNorm(vmin=-20, vcenter=0, vmax=20)\n",
    "    gdf.plot(ax=ax, column=diff_column, cmap='coolwarm', linewidth=4, legend=False, norm=norm, zorder=1, label=\"Higher order street network\")\n",
    "    \n",
    "    # Extract the coordinates of the geometries\n",
    "    coords = [(x, y) for geom in edges_with_capacity_reduction.geometry for x, y in zip(geom.xy[0], geom.xy[1])]\n",
    "\n",
    "    # Compute the alpha shape\n",
    "    alpha = 100  # Adjust this parameter to make the shape tighter or looser\n",
    "    alpha_shape = alphashape.alphashape(coords, alpha)\n",
    "\n",
    "    # Create a GeoSeries for the alpha shape with the same CRS\n",
    "    alpha_shape_gs = gpd.GeoSeries([alpha_shape], crs=gdf.crs)\n",
    "\n",
    "    # Plot the alpha shape\n",
    "    alpha_shape_gs.plot(ax=ax, edgecolor='black', linewidth=2, facecolor='None', zorder=2, label='Capacity was decreased in this section')\n",
    "\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.xlabel(\"Longitude\", fontname=font, fontsize=15)\n",
    "    plt.ylabel(\"Latitude\", fontname=font, fontsize=15)\n",
    "\n",
    "    # Customize tick labels\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontname(font)\n",
    "        label.set_fontsize(15)\n",
    "    \n",
    "    # Create custom legend\n",
    "    custom_lines = [Line2D([0], [0], color='grey', lw=4, label='Higher order street network'),# Add more lines for other labels as needed\n",
    "                    Line2D([0], [0], color='black', lw=2, label='Capacity was decreased in this section')]\n",
    "\n",
    "    ax.legend(handles=custom_lines, prop={'family': font, 'size': 15})\n",
    "    ax.set_position([0.1, 0.1, 0.75, 0.75])\n",
    "    cax = fig.add_axes([0.87, 0.22, 0.03, 0.5])  # Manually position the color bar\n",
    "    \n",
    "    # Create the color bar\n",
    "    sm = plt.cm.ScalarMappable(cmap='coolwarm', norm=norm)\n",
    "    sm._A = []\n",
    "    cbar = plt.colorbar(sm, cax=cax)\n",
    "\n",
    "    # Set color bar font properties\n",
    "    cbar.ax.tick_params(labelsize=15)\n",
    "    for t in cbar.ax.get_yticklabels():\n",
    "        t.set_fontname(font)\n",
    "    cbar.ax.yaxis.label.set_fontname(font)\n",
    "    cbar.ax.yaxis.label.set_size(15)\n",
    "    cbar.set_label('Difference between predicted and actual (%)', fontname=font, fontsize=15)\n",
    "    if save_it:\n",
    "        # p = \"predicted_vs_actual\" if is_predicted else \"actual_vs_predicted\"\n",
    "        plt.savefig(\"results/gdf_difference_\" + str(number_to_plot), bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to define\n",
    "run_path = '/home/enatterer/Development/gnn_predicting_effects_of_traffic_policies/data/runs_1234/hidden_64_hidden_layer_str_[64_64]_/'\n",
    "model_path = run_path +  'trained_model/model.pth'\n",
    "data_created_during_training = run_path + 'data_created_during_training/'\n",
    "indices_of_datasets_to_use = [0, 1, 3, 4]\n",
    "\n",
    "scaler_y = joblib.load(data_created_during_training + 'y_scaler.pkl')\n",
    "scaler_x = joblib.load(data_created_during_training + 'x_scaler.pkl')\n",
    "scaler_pos = joblib.load(data_created_during_training + 'pos_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset created during training\n",
    "test_set_dl = torch.load(data_created_during_training + 'test_dl.pt')\n",
    "\n",
    "# Load the DataLoader parameters\n",
    "with open(data_created_during_training + 'test_loader_params.json', 'r') as f:\n",
    "    test_set_dl_loader_params = json.load(f)\n",
    "    \n",
    "# Remove or correct collate_fn if it is incorrectly specified\n",
    "if 'collate_fn' in test_set_dl_loader_params and isinstance(test_set_dl_loader_params['collate_fn'], str):\n",
    "    del test_set_dl_loader_params['collate_fn']  # Remove it to use the default collate function\n",
    "    \n",
    "test_set_loader = torch.utils.data.DataLoader(test_set_dl, **test_set_dl_loader_params)\n",
    "correct_test_dataset= test_set_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.2547e-09)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_test_dataset.y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = \"../../data/test_data/pop_1pm_policy_in_zone_1.geojson\"\n",
    "test_data = gpd.read_file(test_data)\n",
    "base_case = \"../../data/test_data/gdf_basecase_mean_pop_1pm.geojson\"\n",
    "base_case = gpd.read_file(base_case)\n",
    "\n",
    "test_input_linegraph = pio.create_test_data_object(base_case=base_case, test_data = test_data) # check this function if there have been changes in the features of the data\n",
    "test_data_list = [test_input_linegraph] # we do it for just one test data object, for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler 1 Mean: [0.14182423]\n",
      "Scaler 1 Scale: [2.06741494]\n"
     ]
    }
   ],
   "source": [
    "print(\"Scaler 1 Mean:\", scaler_y.mean_)\n",
    "print(\"Scaler 1 Scale:\", scaler_y.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  7.0741, 480.0000, 480.0000,   0.0000,   4.0000],\n",
       "        [  9.1481, 480.0000, 480.0000,   0.0000,   3.0000],\n",
       "        [  2.0000, 960.0000, 960.0000,   0.0000,   3.0000],\n",
       "        [  7.1852, 960.0000, 960.0000,   0.0000,   4.0000],\n",
       "        [  8.1111, 480.0000, 480.0000,   0.0000,   4.0000],\n",
       "        [  5.4444, 480.0000, 480.0000,   0.0000,   4.0000],\n",
       "        [  5.4444, 480.0000, 480.0000,   0.0000,   4.0000],\n",
       "        [  0.0000, 240.0000, 240.0000,   0.0000,   5.0000],\n",
       "        [ 17.1111, 480.0000, 480.0000,   0.0000,   3.0000],\n",
       "        [  6.7037, 480.0000, 480.0000,   0.0000,   4.0000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_linegraph.x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all y values shape: \n",
      "(31216, 1)\n",
      "og\n",
      "torch.Size([31216, 1])\n",
      "tensor([[-0.1741],\n",
      "        [-1.2481],\n",
      "        [-0.8000],\n",
      "        [-1.0852],\n",
      "        [-1.0111],\n",
      "        [-0.9444],\n",
      "        [-0.9444],\n",
      "        [ 0.0000],\n",
      "        [-1.3111],\n",
      "        [-0.4037]])\n",
      "scaler data\n",
      "[[-0.1527987 ]\n",
      " [-0.6723239 ]\n",
      " [-0.45555648]\n",
      " ...\n",
      " [-0.06859979]\n",
      " [-0.06859979]\n",
      " [-0.06859979]]\n",
      "tensor([[-0.1528],\n",
      "        [-0.6723],\n",
      "        [-0.4556],\n",
      "        [-0.5935],\n",
      "        [-0.5577],\n",
      "        [-0.5254],\n",
      "        [-0.5254],\n",
      "        [-0.0686],\n",
      "        [-0.7028],\n",
      "        [-0.2639]])\n",
      "torch.Size([31216, 1])\n",
      "Total dataset length: 1\n"
     ]
    }
   ],
   "source": [
    "dataset_only_relevant_dimensions = gio.cut_dimensions(dataset=test_data_list, indices_of_dimensions_to_keep=indices_of_datasets_to_use)\n",
    "dataset_normalized = gio.normalize_dataset(dataset_only_relevant_dimensions, y_scalar=scaler_y, x_scalar_list=scaler_x, pos_scalar=scaler_pos, directory_path=None)\n",
    "test_dl = gio.create_dataloader(dataset=dataset_normalized, is_train=False, batch_size=16, train_ratio=0, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0552)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_test_input = test_dl.dataset[0]\n",
    "normalized_test_input.y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing PointNetConv(local_nn=Sequential(\n",
      "  (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "), global_nn=Sequential(\n",
      "  (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "))\n",
      "Initializing 0.weight with kaiming_normal\n",
      "Initializing 0.bias with zeros\n",
      "Initializing 2.weight with kaiming_normal\n",
      "Initializing 2.bias with zeros\n",
      "Initializing 0.weight with kaiming_normal\n",
      "Initializing 0.bias with zeros\n",
      "Initializing 2.weight with kaiming_normal\n",
      "Initializing 2.bias with zeros\n",
      "Initializing 4.weight with kaiming_normal\n",
      "Initializing 4.bias with zeros\n",
      "Initializing Linear(in_features=6, out_features=64, bias=True)\n",
      "Initializing Linear(in_features=64, out_features=64, bias=True)\n",
      "Initializing Linear(in_features=64, out_features=32, bias=True)\n",
      "Initializing Linear(in_features=32, out_features=128, bias=True)\n",
      "Initializing Linear(in_features=128, out_features=64, bias=True)\n",
      "Initializing GATConv(64, 64, heads=1)\n",
      "Initializing GATConv(64, 1, heads=1)\n",
      "Model initialized\n",
      "MyGnn(\n",
      "  (pointLayer): PointNetConv(local_nn=Sequential(\n",
      "    (0): Linear(in_features=6, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  ), global_nn=Sequential(\n",
      "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "  ))\n",
      "  (graph_layers): Sequential(\n",
      "    (0) - GATConv(64, 64, heads=1): x, edge_index -> x\n",
      "    (1) - ReLU(inplace=True): x -> x\n",
      "    (2) - GATConv(64, 1, heads=1): x, edge_index -> x\n",
      "  )\n",
      ")\n",
      "Test Loss: 3.8289902210235596\n",
      "r_squared: -29.410734176635742\n",
      "actual_vals shape: 31216\n",
      "predictions shape: 31216\n"
     ]
    }
   ],
   "source": [
    "in_channels = 6  # Example value\n",
    "out_channels = 1  # Example value\n",
    "hidden_layers_base_for_point_net_conv = 64\n",
    "hidden_layer_structure = [64,64]  # Example value\n",
    "\n",
    "# Initialize the model\n",
    "model = garch.MyGnn(in_channels=in_channels, out_channels=out_channels, hidden_layers_base_for_point_net_conv=hidden_layers_base_for_point_net_conv, hidden_layer_structure=hidden_layer_structure)\n",
    "\n",
    "# Load the model state dictionary\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "loss_fct = torch.nn.MSELoss()\n",
    "test_loss, r_squared, actual_vals, predictions = validate_one_model(model, test_dl.dataset[0], loss_fct, device)\n",
    "print(f'Test Loss: {test_loss}')\n",
    "print(f'r_squared: {r_squared}')\n",
    "print(f'actual_vals shape: {len(actual_vals)}')\n",
    "print(f'predictions shape: {len(predictions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m i\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m gdf \u001b[39m=\u001b[39m data_to_geodataframe(data\u001b[39m=\u001b[39;49mtest_dl, original_gdf\u001b[39m=\u001b[39;49mtest_data, predicted_values\u001b[39m=\u001b[39;49mpredictions)\n\u001b[1;32m      3\u001b[0m gdf_with_og_values \u001b[39m=\u001b[39m map_to_original_values(input_gdf\u001b[39m=\u001b[39mgdf)\n\u001b[1;32m      4\u001b[0m plot_combined_output(gdf_input\u001b[39m=\u001b[39mgdf_with_og_values, column_to_plot\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mog_vol_car_change_predicted\u001b[39m\u001b[39m\"\u001b[39m, save_it\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, number_to_plot\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, is_predicted\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[1], line 105\u001b[0m, in \u001b[0;36mdata_to_geodataframe\u001b[0;34m(data, original_gdf, predicted_values)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdata_to_geodataframe\u001b[39m(data, original_gdf, predicted_values):\n\u001b[1;32m    104\u001b[0m     \u001b[39m# Extract the edge index and node features\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     node_features \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mx\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    106\u001b[0m     target_values \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39my\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    107\u001b[0m     predicted_values \u001b[39m=\u001b[39m predicted_values\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(predicted_values, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m predicted_values\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'x'"
     ]
    }
   ],
   "source": [
    "i= 0\n",
    "gdf = data_to_geodataframe(data=test_dl, original_gdf=test_data, predicted_values=predictions)\n",
    "gdf_with_og_values = map_to_original_values(input_gdf=gdf)\n",
    "plot_combined_output(gdf_input=gdf_with_og_values, column_to_plot=\"og_vol_car_change_predicted\", save_it=True, number_to_plot=1, is_predicted=True)\n",
    "plot_combined_output(gdf_input=gdf_with_og_values, column_to_plot=\"og_vol_car_change_actual\", save_it=True, number_to_plot=1, is_predicted=False)\n",
    "plot_difference_output(gdf_input=gdf_with_og_values, column1=\"og_vol_car_change_predicted\", column2=\"og_vol_car_change_actual\", save_it=True, number_to_plot=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenhao-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import wandb\n",
    "import pickle\n",
    "import os\n",
    "import shapely.wkt as wkt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "from torch_geometric.transforms import LineGraph\n",
    "\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import processing_io as pio\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from shapely.geometry import Point, LineString, box\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "from shapely.ops import unary_union\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from torch_geometric.data import Data, Batch\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import help_functions as hf\n",
    "\n",
    "districts = gpd.read_file(\"../../data/visualisation/districts_paris.geojson\")\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "from gnn_architecture import MyGnn  # or whatever you need to import\n",
    "import help_functions as hf\n",
    "import gnn_io as gio\n",
    "from enum import IntEnum\n",
    "\n",
    "# highway_mapping = {\n",
    "#     'trunk': 0, 'trunk_link': 0, 'motorway_link': 0,\n",
    "#     'primary': 1, 'primary_link': 1,\n",
    "#     'secondary': 2, 'secondary_link': 2,\n",
    "#     'tertiary': 3, 'tertiary_link': 3,\n",
    "#     'residential': 4, 'living_street': 5,\n",
    "#     'pedestrian': 6, 'service': 7,\n",
    "#     'construction': 8, 'unclassified': 9,\n",
    "#     'np.nan': -1\n",
    "# }\n",
    "\n",
    "def compute_r2_torch_with_mean_targets(mean_targets, preds, targets):\n",
    "    ss_tot = torch.sum((targets - mean_targets) ** 2)\n",
    "    ss_res = torch.sum((targets - preds) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import wandb\n",
    "import pickle\n",
    "import os\n",
    "import shapely.wkt as wkt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString\n",
    "from torch_geometric.transforms import LineGraph\n",
    "\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import processing_io as pio\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from shapely.geometry import Point, LineString, box\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "\n",
    "from shapely.ops import unary_union\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from torch_geometric.data import Data, Batch\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import alphashape\n",
    "from matplotlib.lines import Line2D\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_path = '/home/enatterer/Development/gnn_predicting_effects_of_traffic_policies/data/runs_21_10_2024/'\n",
    "point_net_conv_layer_structure_local_mlp = \"64,128\"\n",
    "point_net_conv_layer_structure_global_mlp = \"256,64\"\n",
    "gat_conv_layer_structure = \"128,256,512,256\"\n",
    "dropout = 0.3\n",
    "use_dropout = False \n",
    "predict_mode_stats = False\n",
    "in_channels = 13\n",
    "out_channels = 1 \n",
    "\n",
    "pnc_l_string = point_net_conv_layer_structure_local_mlp.replace(',', '_')\n",
    "pnc_g_string = point_net_conv_layer_structure_global_mlp.replace(',', '_') \n",
    "gat_string = gat_conv_layer_structure.replace(',', '_')\n",
    "\n",
    "unique_model_description = f\"pnc_local_[{pnc_l_string}]_\" + \\\n",
    "f\"pnc_global_[{pnc_g_string}]_\" + \\\n",
    "f\"gat_conv_[{gat_string}]_\" + \\\n",
    "f\"use_dropout_{use_dropout}_\" + \\\n",
    "f\"dropout_{dropout}_\" + \\\n",
    "f\"predict_mode_stats_{predict_mode_stats}\" + \"/\"\n",
    "        \n",
    "run_path = run_path + unique_model_description\n",
    "\n",
    "base_case_path = '../../data/test_data/pop_1pm_basecase_mean_links_NEW.geojson'\n",
    "links_base_case = gpd.read_file(base_case_path, crs=\"EPSG:4326\")\n",
    "    \n",
    "model_path = run_path +  'trained_model/model.pth'\n",
    "data_created_during_training = run_path + 'data_created_during_training/'\n",
    "\n",
    "scaler_x = joblib.load(data_created_during_training + 'test_x_scaler.pkl')\n",
    "scaler_pos = joblib.load(data_created_during_training + 'test_pos_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset created during training\n",
    "test_set_dl = torch.load(data_created_during_training + 'test_dl.pt')\n",
    "\n",
    "# Load the DataLoader parameters\n",
    "with open(data_created_during_training + 'test_loader_params.json', 'r') as f:\n",
    "    test_set_dl_loader_params = json.load(f)\n",
    "    \n",
    "# Remove or correct collate_fn if it is incorrectly specified\n",
    "if 'collate_fn' in test_set_dl_loader_params and isinstance(test_set_dl_loader_params['collate_fn'], str):\n",
    "    del test_set_dl_loader_params['collate_fn']  # Remove it to use the default collate function\n",
    "    \n",
    "test_set_loader = torch.utils.data.DataLoader(test_set_dl, **test_set_dl_loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.46146918e+01, 1.26642926e+03, 1.21392798e+03, 2.89878194e+02,\n",
       "       3.94645961e+00, 2.13702735e+00, 1.10319280e+02, 3.11184999e-01,\n",
       "       4.98455147e-01, 4.97435285e-01, 1.73808681e-01, 2.01475470e-01,\n",
       "       1.96966252e-01])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_x.scale_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_set_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_net_conv_layer_structure_local_mlp: [64, 128]\n",
      "point_net_conv_layer_structure_global_mlp: [256, 64]\n",
      "gat_conv_layer_structure: [128, 256, 512, 256]\n"
     ]
    }
   ],
   "source": [
    "point_net_conv_layer_structure_local_mlp = [int(x) for x in point_net_conv_layer_structure_local_mlp.split(',')]\n",
    "point_net_conv_layer_structure_global_mlp = [int(x) for x in point_net_conv_layer_structure_global_mlp.split(',')]\n",
    "gat_conv_layer_structure = [int(x) for x in gat_conv_layer_structure.split(',')]\n",
    "\n",
    "print(\"point_net_conv_layer_structure_local_mlp:\", point_net_conv_layer_structure_local_mlp)\n",
    "print(\"point_net_conv_layer_structure_global_mlp:\", point_net_conv_layer_structure_global_mlp)\n",
    "print(\"gat_conv_layer_structure:\", gat_conv_layer_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized\n",
      "MyGnn(\n",
      "  (point_net_conv_1): PointNetConv(local_nn=Sequential(\n",
      "    (0): Linear(in_features=15, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "  ), global_nn=Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (point_net_conv_2): PointNetConv(local_nn=Sequential(\n",
      "    (0): Linear(in_features=66, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "  ), global_nn=Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (point_net_conv_3): PointNetConv(local_nn=Sequential(\n",
      "    (0): Linear(in_features=66, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "  ), global_nn=Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "  ))\n",
      "  (read_out_node_predictions): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (gat_graph_layers): Sequential(\n",
      "    (0) - TransformerConv(128, 256, heads=1): x, edge_index -> x\n",
      "    (1) - ReLU(inplace=True): x -> x\n",
      "    (2) - TransformerConv(256, 512, heads=1): x, edge_index -> x\n",
      "    (3) - ReLU(inplace=True): x -> x\n",
      "    (4) - TransformerConv(512, 256, heads=1): x, edge_index -> x\n",
      "    (5) - ReLU(inplace=True): x -> x\n",
      "    (6) - GATConv(256, 64, heads=1): x, edge_index -> x\n",
      "  )\n",
      "  (mode_stat_predictor): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      "  (additional_predictor): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "model = MyGnn(in_channels=in_channels, out_channels=out_channels, \n",
    "                    point_net_conv_layer_structure_local_mlp=point_net_conv_layer_structure_local_mlp, \n",
    "                    point_net_conv_layer_structure_global_mlp = point_net_conv_layer_structure_global_mlp,\n",
    "                    gat_conv_layer_structure=gat_conv_layer_structure,\n",
    "                    dropout=dropout,\n",
    "                    use_dropout=use_dropout, \n",
    "                    predict_mode_stats=predict_mode_stats)\n",
    "\n",
    "# Load the model state dictionary\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.60782257963714\n",
      "R-squared: 0.44928133487701416\n",
      "Baseline Loss: 2.919499158859253\n"
     ]
    }
   ],
   "source": [
    "def validate_model_on_test_set(model, dataset, loss_func, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        if isinstance(dataset, list):\n",
    "            for data in dataset:\n",
    "                input_node_features, targets = data.x.to(device), data.y.to(device)\n",
    "                predicted = model(data.to(device))\n",
    "                loss = loss_func(predicted, targets).item()\n",
    "                total_loss += loss\n",
    "                all_preds.append(predicted)\n",
    "                all_targets.append(targets)\n",
    "        else:\n",
    "            input_node_features, targets = dataset.x.to(device), dataset.y.to(device)\n",
    "            predicted = model(dataset.to(device))\n",
    "            loss = loss_func(predicted, targets).item()\n",
    "            total_loss += loss\n",
    "            all_preds.append(predicted)\n",
    "            all_targets.append(targets)\n",
    "    \n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    \n",
    "    mean_targets = torch.mean(all_targets)\n",
    "    r_squared = compute_r2_torch_with_mean_targets(mean_targets=mean_targets, preds=all_preds, targets=all_targets)\n",
    "    baseline_loss = loss_func(all_targets, torch.full_like(all_preds, mean_targets))\n",
    "    \n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    \n",
    "    return avg_loss, r_squared, all_targets, all_preds, baseline_loss\n",
    "\n",
    "# Example usage:\n",
    "loss_fct = torch.nn.MSELoss()\n",
    "test_loss, r_squared, actual_vals, predictions, baseline_loss = validate_model_on_test_set(model, test_set_loader.dataset, loss_fct, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"Baseline Loss: {baseline_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_x = test_set_loader.dataset[0].x\n",
    "my_test_x = my_test_x.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4045, -0.8099, -0.7885,  0.2366, -2.0563,  0.6009, -0.6492, -2.8644,\n",
       "        -0.9243,  1.1068,  5.5740,  4.7530, -0.2053])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_test_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.46146918e+01 1.26642926e+03 1.21392798e+03 2.89878194e+02\n",
      " 3.94645961e+00 2.13702735e+00 1.10319280e+02 3.11184999e-01\n",
      " 4.98455147e-01 4.97435285e-01 1.73808681e-01 2.01475470e-01\n",
      " 1.96966252e-01]\n",
      "[ 5.91181886e+00  1.02571066e+03  9.57135617e+02 -6.85750444e+01\n",
      "  8.11514308e+00  2.71579961e+00  9.16523221e+01  8.91361593e-01\n",
      "  4.60725755e-01  4.49421965e-01  3.11817598e-02  4.23892100e-02\n",
      "  4.04303147e-02]\n",
      "[2.13589215e+02 1.60384307e+06 1.47362113e+06 8.40293671e+04\n",
      " 1.55745435e+01 4.56688589e+00 1.21703436e+04 9.68361037e-02\n",
      " 2.48457534e-01 2.47441862e-01 3.02094577e-02 4.05923649e-02\n",
      " 3.87957044e-02]\n"
     ]
    }
   ],
   "source": [
    "print(scaler_x.scale_)\n",
    "print(scaler_x.mean_)\n",
    "print(scaler_x.var_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "inversed_values = scaler_x.inverse_transform(my_test_x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original value at index 0: 5.911818374951527\n",
      "Original value at index 1: 1025.751403219784\n",
      "Original value at index 2: 957.1264463457123\n",
      "Original value at index 3: -68.57455599426434\n",
      "Original value at index 4: 8.115142678632898\n",
      "Original value at index 5: 11.263908940781429\n",
      "Original value at index 6: 2302.1307728289835\n",
      "Original value at index 7: 0.8913615836835325\n",
      "Original value at index 8: 0.4607257548492381\n",
      "Original value at index 9: 0.9468572407759313\n",
      "Original value at index 10: 0.20499044624495127\n",
      "Original value at index 11: 0.24386467170578763\n",
      "Original value at index 12: 0.0404303144672288\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(inversed_values)):\n",
    "    original_value = inversed_values[i] * scaler_x.scale_[i] + scaler_x.mean_[i]\n",
    "    print(f\"Original value at index {i}: {original_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.30035164e-08,  3.21705227e-05, -7.55467022e-06,  1.68471824e-06,\n",
       "       -1.02007496e-07,  3.99999997e+00,  2.00371000e+01, -2.93174381e-08,\n",
       "        3.86890686e-10,  9.99999982e-01,  1.00000003e+00,  9.99999960e-01,\n",
       "       -1.22123741e-09])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_x_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.22439918915430704\n",
      "R-squared: 0.5001441240310669\n",
      "Baseline Loss: 2.6935665607452393\n"
     ]
    }
   ],
   "source": [
    "test_loss, r_squared, actual_vals, predictions, baseline_loss = validate_model_on_test_set(model, test_set_loader.dataset[0], loss_fct, device)\n",
    "\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "print(f\"Baseline Loss: {baseline_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = test_set_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "class EdgeFeatures(IntEnum):\n",
    "    VOL_BASE_CASE = 0\n",
    "    CAPACITY_BASE_CASE = 1\n",
    "    CAPACITIES_NEW = 2\n",
    "    CAPACITY_REDUCTION = 3\n",
    "    FREESPEED = 4\n",
    "    HIGHWAY = 5\n",
    "    LENGTH = 6\n",
    "    ALLOWED_MODE_CAR = 7\n",
    "    ALLOWED_MODE_BUS = 8\n",
    "    ALLOWED_MODE_PT = 9\n",
    "    ALLOWED_MODE_TRAIN = 10\n",
    "    ALLOWED_MODE_RAIL = 11\n",
    "    ALLOWED_MODE_SUBWAY = 12\n",
    "    \n",
    "def data_to_geodataframe(data, original_gdf, predicted_values):\n",
    "    # Extract the edge index and node features\n",
    "    node_features = data.x.cpu().numpy()\n",
    "    target_values = data.y.cpu().numpy()\n",
    "    predicted_values = predicted_values.cpu().numpy() if isinstance(predicted_values, torch.Tensor) else predicted_values\n",
    "    \n",
    "    edge_data = {\n",
    "        'from_node': original_gdf[\"from_node\"].values,\n",
    "        'to_node': original_gdf[\"to_node\"].values,\n",
    "        'vol_base_case': node_features[:, EdgeFeatures.VOL_BASE_CASE],  \n",
    "        'capacity_base_case': node_features[:, EdgeFeatures.CAPACITY_BASE_CASE],  \n",
    "        'capacities_new': node_features[:, EdgeFeatures.CAPACITIES_NEW],  \n",
    "        'capacity_reduction': node_features[:, EdgeFeatures.CAPACITY_REDUCTION],  \n",
    "        'freespeed': node_features[:, EdgeFeatures.FREESPEED],  \n",
    "        'highway': node_features[:, EdgeFeatures.HIGHWAY],\n",
    "        'length': node_features[:, EdgeFeatures.LENGTH],\n",
    "        'allowed_mode_car': node_features[:, EdgeFeatures.ALLOWED_MODE_CAR],\n",
    "        'allowed_mode_bus': node_features[:, EdgeFeatures.ALLOWED_MODE_BUS],\n",
    "        'allowed_mode_pt': node_features[:, EdgeFeatures.ALLOWED_MODE_PT],\n",
    "        'allowed_mode_train': node_features[:, EdgeFeatures.ALLOWED_MODE_TRAIN],\n",
    "        'allowed_mode_rail': node_features[:, EdgeFeatures.ALLOWED_MODE_RAIL],\n",
    "        'allowed_mode_subway': node_features[:, EdgeFeatures.ALLOWED_MODE_SUBWAY],\n",
    "        'vol_car_change_actual': target_values.squeeze(),  \n",
    "        'vol_car_change_predicted': predicted_values.squeeze()\n",
    "    }\n",
    "    # Convert to DataFrame\n",
    "    edge_df = pd.DataFrame(edge_data)\n",
    "    # Create LineString geometry\n",
    "    edge_df['geometry'] = original_gdf[\"geometry\"].values\n",
    "    # Create GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(edge_df, geometry='geometry')\n",
    "    return gdf\n",
    "\n",
    "gdf = data_to_geodataframe(data=my_data, original_gdf=links_base_case, predicted_values=predictions)\n",
    "\n",
    "def map_to_original_values(input_gdf: gpd.GeoDataFrame, scaler_x):\n",
    "    gdf_og_values = input_gdf.copy()\n",
    "    # gdf_og_values['og_vol_car_change_actual'] = gdf['vol_car_change_actual']\n",
    "    # gdf_og_values['og_vol_car_change_predicted'] = gdf['vol_car_change_predicted']\n",
    "    \n",
    "    edge_feature_columns = [feature.name.lower() for feature in EdgeFeatures]\n",
    "    original_values = scaler_x.inverse_transform(gdf_og_values[edge_feature_columns])\n",
    "    \n",
    "    \n",
    "    \n",
    "    gdf_og_values[edge_feature_columns] = original_values\n",
    "    return gdf_og_values\n",
    "\n",
    "# gdf_with_og_values = map_to_original_values(input_gdf=gdf, scaler_x=scaler_x)\n",
    "# gdf_with_og_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_feature_columns = [feature.name.lower() for feature in EdgeFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vol_base_case</th>\n",
       "      <th>capacity_base_case</th>\n",
       "      <th>capacities_new</th>\n",
       "      <th>capacity_reduction</th>\n",
       "      <th>freespeed</th>\n",
       "      <th>highway</th>\n",
       "      <th>length</th>\n",
       "      <th>allowed_mode_car</th>\n",
       "      <th>allowed_mode_bus</th>\n",
       "      <th>allowed_mode_pt</th>\n",
       "      <th>allowed_mode_train</th>\n",
       "      <th>allowed_mode_rail</th>\n",
       "      <th>allowed_mode_subway</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.404512</td>\n",
       "      <td>-0.809923</td>\n",
       "      <td>-0.788462</td>\n",
       "      <td>0.236565</td>\n",
       "      <td>-2.05631</td>\n",
       "      <td>0.600928</td>\n",
       "      <td>-0.649163</td>\n",
       "      <td>-2.864411</td>\n",
       "      <td>-0.924307</td>\n",
       "      <td>1.106833</td>\n",
       "      <td>5.574050</td>\n",
       "      <td>4.752989</td>\n",
       "      <td>-0.205265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.404512</td>\n",
       "      <td>-0.809923</td>\n",
       "      <td>-0.788462</td>\n",
       "      <td>0.236565</td>\n",
       "      <td>-2.05631</td>\n",
       "      <td>2.940627</td>\n",
       "      <td>-0.742176</td>\n",
       "      <td>-2.864411</td>\n",
       "      <td>-0.924307</td>\n",
       "      <td>1.106833</td>\n",
       "      <td>5.574050</td>\n",
       "      <td>4.752989</td>\n",
       "      <td>-0.205265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.404512</td>\n",
       "      <td>-0.809923</td>\n",
       "      <td>-0.788462</td>\n",
       "      <td>0.236565</td>\n",
       "      <td>-2.05631</td>\n",
       "      <td>0.132989</td>\n",
       "      <td>-0.755253</td>\n",
       "      <td>-2.864411</td>\n",
       "      <td>-0.924307</td>\n",
       "      <td>1.106833</td>\n",
       "      <td>5.574050</td>\n",
       "      <td>4.752989</td>\n",
       "      <td>-0.205265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.404512</td>\n",
       "      <td>-0.809923</td>\n",
       "      <td>-0.788462</td>\n",
       "      <td>0.236565</td>\n",
       "      <td>-2.05631</td>\n",
       "      <td>0.600928</td>\n",
       "      <td>-0.588406</td>\n",
       "      <td>-2.864411</td>\n",
       "      <td>-0.924307</td>\n",
       "      <td>1.106833</td>\n",
       "      <td>5.574050</td>\n",
       "      <td>4.752989</td>\n",
       "      <td>-0.205265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.404512</td>\n",
       "      <td>-0.809923</td>\n",
       "      <td>-0.788462</td>\n",
       "      <td>0.236565</td>\n",
       "      <td>-2.05631</td>\n",
       "      <td>-0.334951</td>\n",
       "      <td>-0.503685</td>\n",
       "      <td>-2.864411</td>\n",
       "      <td>-0.924307</td>\n",
       "      <td>1.106833</td>\n",
       "      <td>5.574050</td>\n",
       "      <td>4.752989</td>\n",
       "      <td>-0.205265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31135</th>\n",
       "      <td>-0.404512</td>\n",
       "      <td>-0.809923</td>\n",
       "      <td>-0.788462</td>\n",
       "      <td>0.236565</td>\n",
       "      <td>-2.05631</td>\n",
       "      <td>-1.738770</td>\n",
       "      <td>-0.649500</td>\n",
       "      <td>-2.864411</td>\n",
       "      <td>-0.924307</td>\n",
       "      <td>-0.903478</td>\n",
       "      <td>-0.179403</td>\n",
       "      <td>-0.210394</td>\n",
       "      <td>4.871747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31136</th>\n",
       "      <td>-0.404512</td>\n",
       "      <td>-0.809923</td>\n",
       "      <td>-0.788462</td>\n",
       "      <td>0.236565</td>\n",
       "      <td>-2.05631</td>\n",
       "      <td>-1.738770</td>\n",
       "      <td>5.513456</td>\n",
       "      <td>-2.864411</td>\n",
       "      <td>-0.924307</td>\n",
       "      <td>-0.903478</td>\n",
       "      <td>-0.179403</td>\n",
       "      <td>-0.210394</td>\n",
       "      <td>4.871747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31137</th>\n",
       "      <td>-0.404512</td>\n",
       "      <td>-0.809923</td>\n",
       "      <td>-0.788462</td>\n",
       "      <td>0.236565</td>\n",
       "      <td>-2.05631</td>\n",
       "      <td>-1.738770</td>\n",
       "      <td>-0.649500</td>\n",
       "      <td>-2.864411</td>\n",
       "      <td>-0.924307</td>\n",
       "      <td>-0.903478</td>\n",
       "      <td>-0.179403</td>\n",
       "      <td>-0.210394</td>\n",
       "      <td>4.871747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31138</th>\n",
       "      <td>-0.404512</td>\n",
       "      <td>-0.809923</td>\n",
       "      <td>-0.788462</td>\n",
       "      <td>0.236565</td>\n",
       "      <td>-2.05631</td>\n",
       "      <td>-1.738770</td>\n",
       "      <td>4.715735</td>\n",
       "      <td>-2.864411</td>\n",
       "      <td>-0.924307</td>\n",
       "      <td>-0.903478</td>\n",
       "      <td>-0.179403</td>\n",
       "      <td>-0.210394</td>\n",
       "      <td>-0.205265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31139</th>\n",
       "      <td>-0.404512</td>\n",
       "      <td>-0.809923</td>\n",
       "      <td>-0.788462</td>\n",
       "      <td>0.236565</td>\n",
       "      <td>-2.05631</td>\n",
       "      <td>-1.738770</td>\n",
       "      <td>-0.649500</td>\n",
       "      <td>-2.864411</td>\n",
       "      <td>-0.924307</td>\n",
       "      <td>-0.903478</td>\n",
       "      <td>-0.179403</td>\n",
       "      <td>-0.210394</td>\n",
       "      <td>-0.205265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31140 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       vol_base_case  capacity_base_case  capacities_new  capacity_reduction  \\\n",
       "0          -0.404512           -0.809923       -0.788462            0.236565   \n",
       "1          -0.404512           -0.809923       -0.788462            0.236565   \n",
       "2          -0.404512           -0.809923       -0.788462            0.236565   \n",
       "3          -0.404512           -0.809923       -0.788462            0.236565   \n",
       "4          -0.404512           -0.809923       -0.788462            0.236565   \n",
       "...              ...                 ...             ...                 ...   \n",
       "31135      -0.404512           -0.809923       -0.788462            0.236565   \n",
       "31136      -0.404512           -0.809923       -0.788462            0.236565   \n",
       "31137      -0.404512           -0.809923       -0.788462            0.236565   \n",
       "31138      -0.404512           -0.809923       -0.788462            0.236565   \n",
       "31139      -0.404512           -0.809923       -0.788462            0.236565   \n",
       "\n",
       "       freespeed   highway    length  allowed_mode_car  allowed_mode_bus  \\\n",
       "0       -2.05631  0.600928 -0.649163         -2.864411         -0.924307   \n",
       "1       -2.05631  2.940627 -0.742176         -2.864411         -0.924307   \n",
       "2       -2.05631  0.132989 -0.755253         -2.864411         -0.924307   \n",
       "3       -2.05631  0.600928 -0.588406         -2.864411         -0.924307   \n",
       "4       -2.05631 -0.334951 -0.503685         -2.864411         -0.924307   \n",
       "...          ...       ...       ...               ...               ...   \n",
       "31135   -2.05631 -1.738770 -0.649500         -2.864411         -0.924307   \n",
       "31136   -2.05631 -1.738770  5.513456         -2.864411         -0.924307   \n",
       "31137   -2.05631 -1.738770 -0.649500         -2.864411         -0.924307   \n",
       "31138   -2.05631 -1.738770  4.715735         -2.864411         -0.924307   \n",
       "31139   -2.05631 -1.738770 -0.649500         -2.864411         -0.924307   \n",
       "\n",
       "       allowed_mode_pt  allowed_mode_train  allowed_mode_rail  \\\n",
       "0             1.106833            5.574050           4.752989   \n",
       "1             1.106833            5.574050           4.752989   \n",
       "2             1.106833            5.574050           4.752989   \n",
       "3             1.106833            5.574050           4.752989   \n",
       "4             1.106833            5.574050           4.752989   \n",
       "...                ...                 ...                ...   \n",
       "31135        -0.903478           -0.179403          -0.210394   \n",
       "31136        -0.903478           -0.179403          -0.210394   \n",
       "31137        -0.903478           -0.179403          -0.210394   \n",
       "31138        -0.903478           -0.179403          -0.210394   \n",
       "31139        -0.903478           -0.179403          -0.210394   \n",
       "\n",
       "       allowed_mode_subway  \n",
       "0                -0.205265  \n",
       "1                -0.205265  \n",
       "2                -0.205265  \n",
       "3                -0.205265  \n",
       "4                -0.205265  \n",
       "...                    ...  \n",
       "31135             4.871747  \n",
       "31136             4.871747  \n",
       "31137             4.871747  \n",
       "31138            -0.205265  \n",
       "31139            -0.205265  \n",
       "\n",
       "[31140 rows x 13 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf[edge_feature_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fct = torch.nn.MSELoss()\n",
    "\n",
    "# test_loss, r_squared, actual_vals, predictions, baseline_loss = validate_trained_model(model, test_set_loader.dataset, loss_fct, device)\n",
    "# print(f'Test Loss: {test_loss}')\n",
    "# print(f'r_squared: {r_squared}')\n",
    "# print(f'baseline_loss: {baseline_loss}')\n",
    "\n",
    "# print(f'actual_vals shape: {len(actual_vals)}')\n",
    "# print(f'predictions shape: {len(predictions)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter list of tensors\n",
    "# def filter_tensors_by_indices(tensor_list, indices):\n",
    "#     return [tensor[indices] for tensor in tensor_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure actual_vals and predictions are tensors\n",
    "# if isinstance(actual_vals, list):\n",
    "#     actual_vals = torch.tensor(actual_vals)\n",
    "# if isinstance(predictions, list):\n",
    "#     predictions = torch.tensor(predictions)\n",
    "# THIS IS FOR WHOLE TEST SET ! \n",
    "\n",
    "\n",
    "# # Filter actual and predicted values\n",
    "# filtered_actual = filter_tensors_by_indices(actual_vals, indices_to_filter_for)\n",
    "# filtered_predicted = filter_tensors_by_indices(predictions, indices_to_filter_for)\n",
    "\n",
    "# # Concatenate filtered tensors for computing metrics\n",
    "# filtered_actual_concat = torch.cat(filtered_actual)\n",
    "# filtered_predicted_concat = torch.cat(filtered_predicted)\n",
    "\n",
    "# filtered_actual_mean = torch.mean(filtered_actual_concat)\n",
    "# # filtered_actual = actual_vals[indices_to_filter_for]\n",
    "# # filtered_actual_mean = torch.mean(filtered_actual)\n",
    "# # filtered_predicted = predictions[indices_to_filter_for]\n",
    "\n",
    "# mse_filtered = loss_fct(filtered_actual_concat, filtered_predicted_concat)\n",
    "# baseline_filtered = loss_fct(filtered_actual_concat, torch.full_like(filtered_actual_concat, filtered_actual_mean))\n",
    "# r_squared = compute_r2_torch(preds=filtered_predicted_concat, targets=filtered_actual_concat)\n",
    "# loss_fct_aux = torch.nn.MSELoss(reduction='none')\n",
    "# variance = torch.var(loss_fct_aux(filtered_actual_concat, torch.full_like(filtered_actual_concat, filtered_actual_mean)))\n",
    "\n",
    "# print(f'variance: {variance}')\n",
    "# print(f'Baseline Loss: {baseline_filtered}')\n",
    "# print(f'Test Loss: {mse_filtered}')\n",
    "# print(f'r_squared: {r_squared}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'no_filter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m indices_to_filter_for \u001b[38;5;241m=\u001b[39m \u001b[43mno_filter\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'no_filter' is not defined"
     ]
    }
   ],
   "source": [
    "indices_to_filter_for = no_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gdf = hf.data_to_geodataframe(data=test_set_loader.dataset[0], original_gdf=test_data, predicted_values=predictions)\n",
    "# # gdf_with_og_values = hf.map_to_original_values(input_gdf=gdf, scaler_x=scaler_x)\n",
    "# # gdf_with_og_values.crs = \"EPSG:2154\"\n",
    "# # gdf_with_og_values.to_crs(\"EPSG:4326\", inplace=True)\n",
    "\n",
    "# gdf_in_meters = gdf_with_og_values.to_crs(\"EPSG:32633\")\n",
    "# gdf_in_meters.length\n",
    "\n",
    "# tolerance = 1e-3\n",
    "# gdf_with_capacity_reduction = gdf_in_meters.loc[indices_to_filter_for]\n",
    "# gdf_with_capacity_reduction['length'] = gdf_with_capacity_reduction.length\n",
    "# total_length = gdf_with_capacity_reduction['length'].sum()/1000\n",
    "# print(total_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/chenhao-gnn/lib/python3.10/site-packages/geopandas/geodataframe.py:1525: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "gdf_in_meters = gdf_with_og_values.to_crs(\"EPSG:32633\")\n",
    "gdf_in_meters.length\n",
    "\n",
    "tolerance = 1e-3\n",
    "gdf_with_capacity_reduction = gdf_in_meters[abs(gdf_in_meters['og_capacity_reduction']) > tolerance]\n",
    "gdf_with_capacity_reduction['length'] = gdf_with_capacity_reduction.length\n",
    "total_length = gdf_with_capacity_reduction['length'].sum()/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4492"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gdf_with_capacity_reduction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenhao-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

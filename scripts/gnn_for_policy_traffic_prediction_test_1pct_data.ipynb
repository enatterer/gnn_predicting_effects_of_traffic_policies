{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import wandb\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from gnn_architectures import MyGnn\n",
    "\n",
    "import gnn_io as gio\n",
    "import gnn_architectures as garch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define model and parameters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of dictionaries\n",
    "data_dict_list = torch.load('../data/dataset_1pct_0_100.pt')\n",
    "\n",
    "# Reconstruct the Data objects\n",
    "datalist = [Data(x=d['x'], edge_index=d['edge_index'], pos=d['pos'], y=d['y']) for d in data_dict_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline error: 0.0058123222552239895\n"
     ]
    }
   ],
   "source": [
    "# # Apply normalization to your dataset\n",
    "dataset_normalized = gio.normalize_dataset(datalist)\n",
    "\n",
    "baseline_error = gio.compute_baseline_error(datalist)\n",
    "print(f'Baseline error: {baseline_error}')\n",
    "\n",
    "# Apply the function to the dataset\n",
    "dataset_updated = gio.replace_x_with_normalized_x(dataset_normalized)\n",
    "\n",
    "# Apply the function to the dataset\n",
    "dataset_updated = gio.cut_dimensions(dataset_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset length: 101\n"
     ]
    }
   ],
   "source": [
    "test_dl = gio.create_dataloader(dataset=dataset_updated, is_train=True, batch_size=16, train_ratio=0, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33menatterer\u001b[0m (\u001b[33mtum-traffic-engineering\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33menatterer\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "wandb: ERROR Error while calling W&B API: entity your_entity_name not found during upsertBucket (<Response [404]>)\n"
     ]
    },
    {
     "ename": "CommError",
     "evalue": "It appears that you do not have permission to access the requested resource. Please reach out to the project owner to grant you access. If you have the correct permissions, verify that there are no issues with your networking setup.(Error 404: Not Found)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCommError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m wandb\u001b[39m.\u001b[39mlogin()\n\u001b[0;32m----> 2\u001b[0m wandb\u001b[39m.\u001b[39;49minit(project\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39myour_project_name\u001b[39;49m\u001b[39m\"\u001b[39;49m, entity\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39myour_entity_name\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Load the model from the pickle file\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1195\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     logger\u001b[39m.\u001b[39mexception(\u001b[39m\"\u001b[39m\u001b[39merror in wandb.init()\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39me)\n\u001b[1;32m   1193\u001b[0m \u001b[39m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[39m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[0;32m-> 1195\u001b[0m wandb\u001b[39m.\u001b[39;49m_sentry\u001b[39m.\u001b[39;49mreraise(e)\n\u001b[1;32m   1196\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m()\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.10/site-packages/wandb/analytics/sentry.py:155\u001b[0m, in \u001b[0;36mSentry.reraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexception(exc)\n\u001b[1;32m    153\u001b[0m \u001b[39m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39m# but hopefully it's not too bad\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(sys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:1181\u001b[0m, in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, fork_from, resume_from, settings)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     wi \u001b[39m=\u001b[39m _WandbInit()\n\u001b[1;32m   1180\u001b[0m     wi\u001b[39m.\u001b[39msetup(kwargs)\n\u001b[0;32m-> 1181\u001b[0m     \u001b[39mreturn\u001b[39;00m wi\u001b[39m.\u001b[39;49minit()\n\u001b[1;32m   1183\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1184\u001b[0m     \u001b[39mif\u001b[39;00m logger \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_env/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:785\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    783\u001b[0m         backend\u001b[39m.\u001b[39mcleanup()\n\u001b[1;32m    784\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mteardown()\n\u001b[0;32m--> 785\u001b[0m     \u001b[39mraise\u001b[39;00m error\n\u001b[1;32m    787\u001b[0m \u001b[39massert\u001b[39;00m run_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m  \u001b[39m# for mypy\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m run_result\u001b[39m.\u001b[39mHasField(\u001b[39m\"\u001b[39m\u001b[39mrun\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "\u001b[0;31mCommError\u001b[0m: It appears that you do not have permission to access the requested resource. Please reach out to the project owner to grant you access. If you have the correct permissions, verify that there are no issues with your networking setup.(Error 404: Not Found)"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb.init(project=\"test_with_1pct_data\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model from the pickle file\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    checkpoint = pickle.load(f)\n",
    "\n",
    "state_dict = checkpoint['state_dict']\n",
    "config = checkpoint['config']\n",
    "\n",
    "# Recreate the model using the saved configuration\n",
    "loaded_gnn_instance = MyGnn(\n",
    "    in_channels=config['in_channels'],\n",
    "    out_channels=config['out_channels'],\n",
    "    hidden_size=config['hidden_size'],\n",
    "    gat_layers=config['gat_layers'],\n",
    "    gcn_layers=config['gcn_layers'],\n",
    "    output_layer=config['output_layer']\n",
    ")\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "loaded_model = loaded_gnn_instance.to(device)\n",
    "loaded_model.load_state_dict(state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model\n",
    "def evaluate(model, test_dl, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for batch in test_dl:\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = torch.nn.MSELoss()(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "    avg_test_loss = test_loss / len(test_dl)\n",
    "    \n",
    "    # Log the test loss to Wandb\n",
    "    wandb.log({\"test_loss\": avg_test_loss})\n",
    "\n",
    "# Evaluate the loaded model\n",
    "test_loss = evaluate(loaded_model, test_dl, device)\n",
    "print(f'Test Loss: {test_loss}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paris_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
